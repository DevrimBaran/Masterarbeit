\chapter{Related Work}

The early foundations regarding wait-freedom was done indirectly by Leslie Lamport in 1977 and 1983 \cite{Lamport1977ConcurrentReading,Lamport1983SPSCCircularBuffer}. While these works did not directly address or formally define wait-freedom, they laid the groundwork for lock-freedom, which Maurice Herlihy later extended to wait-freedom \cite{herlihy1991wait}. \\
In 1977, Lamport he showed how one writer and multiple readers can share data without the need of locks, eliminating writer delays due to reader interference. It works by using atomic byte-wise read and write operations. The writer will atomically write a value to a memory location from right to left, while the reader uses the same memory location to read the value from left to right. So even if the writer is still in the process of writing, the reader will not block the writer while reading (and vice versa) and still see a correct snapshot of the data. To prevent a reader process from saving inconsistent data (for example while the writer process writes), the writer process will tag each update by incrementing a start counter before an end counter after writing. Readers take the start counter, read the data, and compare if the start counter matches the end counter. If they do not match it means the data is inconsistent and the reader will retry reading the data until it gets a consistent value. To understand this better imagine a date with the format DD/MM/YYYY, where every digit is written as one byte from right to left and read from left to write. This solves the problem of contention between readers and a single writer. If multiple writers are involved, the writers still have to be mutually exclusive, which means that they have to use locks to block each other to prevent inconsistency (So with multiple writers this algorithm cannot be lock-free anymore). \cite{Lamport1977ConcurrentReading} 

In 1983 Leslie Lamport then gave a formal method to write and prove the correctness of any concurrent module in a simple and modular way independent of the used data structure, which he refers to as modules. These modules consist of three components: 
\begin{itemize}
   \item state functions, which are abstract variables describing the module's state
   \item initial conditions, which are predicates on the state functions
   \item properties, which are a mix of safety and liveness requirements
\end{itemize}
Safety requirements define what must never happen (e.g., a queue must never drop an element), while liveness requirements define what must eventually happen (e.g., a non-empty queue must eventually allow dequeueing). He also defines the usage of action sets and environment constraints, which seperate the module action from the environments (e.g., the program where the data structure runs in). For example, a \ac{FIFO} queue specification would include:
\begin{itemize}
   \item State Functions: queue contents, operation parameters, return values
   \item Initial Conditions: queue starts empty
   \item Safety Condition: queue maintains \ac{FIFO} ordering
   \item Liveness Condition: dequeue operation will eventually return a value
\end{itemize}
This systematic methodology to prove the correctness of concurrent data structures laid the necessary groundwork for later developments too. \cite{Lamport1983SPSCCircularBuffer}

Building on methodologys to prove concurrent data structure correctness Herlihy and Wing provided linearizability as a correctness condition for concurrent objects, which is a guarantee that every operation performed appears to take effect instantaneously at some point between the call and return of the operation \cite{HerlihyLinearizability}. This correctness condition was then used to formalize wait-freedom in 1991 \cite{herlihy1991wait}. In the latter work Herlihy proved that any sequential data structure can be transformed into a wait-free concurrent data structure \cite{herlihy1991wait}. A wait-free data structure must satisfy following three constraints: 
\begin{itemize}
   \item Linearizability: operations take effect instantaneously at some point between the call and return of the operation
   \item Bounded steps: operations end in a finite number of steps
   \item Independence: operations finish regardless of other processes' exection (for later understrandin: a process waiting for another process for a maximum number of time and then returning an error if that time is exceeded would still be considered wait-free, since it will finish regardless of the other process)
\end{itemize}
From this perspective the algorithm provided by Lamport in 1977 is already wait-free, even though the term was not yet defined at that time. \cite{herlihy1991wait,HerlihyLinearizability}

Herlihys universal construction and principles (or work that builds upon his work) appear conceptually throughout all of these wait-free algorithms \cite{Kogan2011WaitFreeQueues,FeldmanDechev2015WaitFreeRingBuffer,kogan2012methodology,FeldmanDechevV2,FeldmanDechevV3,RamalheteQueue,wCQWaitFreeQueue,Verma2013Scalable,FastFetchAndAddWaitFreeQueue,WangCacheCoherent,adampsc,jiffy,JayantiLog,Drescher2015GuardedSections,Mate√≠spmc,torquati2010singleproducersingleconsumerqueuessharedcache,Aldinucci2012EfficientSync,Wang2013BQueue,MaffioneCacheAware,ffq}. \cite{herlihy1991wait}

Kogan and Petrank later invented a method called fast-path slow-path, were first usually a lock-free method (the fast-path) is used to try to complete an operation, since lock-free algorithms are usually faster than wait-free algorithms. These lock-free paths are bounded by a maximum number of steps and if the operation does not complete within that bound the algorithm tries to complete the operation using a wait-free method (the slow-path). So in cases where the fast-path succeeds often without switching to the slow path, the algorithm in general completes in a shorter time then a pure wait-free algorithm. This method is used by two algorithms with one of them having a great performance advantage, which we will look into later in this thesis \cite{wCQWaitFreeQueue,FastFetchAndAddWaitFreeQueue}. \cite{kogan2012methodology}