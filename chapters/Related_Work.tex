\chapter{Related Work}

The early foundations regarding wait-freedom was done indirectly by Leslie Lamport in 1977 and 1983 \cite{Lamport1977ConcurrentReading,Lamport1983SPSCCircularBuffer}. Indirectly, because this laid the foundation for lock-freedom, which was later then extended to wait-freedom by Maurice Herlihy \cite{herlihy1991wait}. In 1977 he showed how one writer and multiple readers can share data without the need of locks, eliminating writer delays due to reader interference. It works by using atomic read and write operations byte wise. The writer will atomically write a value to a memory location from right to left, while the reader uses the same memory location to read the value from left to right. So even if the writer is still in the process of writing, the reader will not block the writer while reading (and vice versa), yet always sees a correct snapshot of the data. To prevent a reader process from saving inconsistent data (for example while the writer process writes), the writer process will tag each update by incrementing a start counter before an end counter after writing. Readers take the start counter, read the data, and compare if the start counter matches the end counter. If they do not match it means the data is inconsistent and the reader will retry reading the data until it gets a consistent value. To understand this better imagine a date with the format DD/MM/YYYY, where every digit is saved as one byte from right to left and read from left to write. This solves the problem of contention between readers and a single writer. If multiple writers are involved, the writers still have to be mutually exclusive, which means that they have to use locks to block each other to prevent inconsistency. \cite{Lamport1977ConcurrentReading} 

In 1983 Leslie Lamport then gave a way to write and prove the correctness of any concurrent module in a simple and modular way independent of the used data structure, which he refers to as modules. He defined safety and liveness conditions, where safety means that for instance a queue drops or reorders items and liveness means that that queue when its nonempty an item eventually is dequeued. A module will get state functions, which are the abstract variables needed to define that module, initial conditions, which are simple predicates on those state functions and properties which are a mix of safety and liveness clauses. He also defines the usage of action sets and environment constraints, which seperate the module action from the environments (the program where the data structure runs in). An example would be an \ac{FIFO} queue, where the queue, input variable, return variable would be the state functions and the enqueue and dequeue operations would be the actions. The environment constraints would be the preconditions for the enqueue and dequeue operations, which are that the queue is not full or empty respectively. As safety condition one could define that the enqueue operation only adds when control is inside and as a liveness condition that the dequeue operation will eventually return a value. This concept is important, because it allows to just re-speciffy the data structures interface instead of the whole environment it runs in. Also he proves these concepts, so there is a uniform recipe for oing from spec to code to proof for any concurrent data structure. \cite{Lamport1983SPSCCircularBuffer}

These two insights are also the basis for the wait-free data structure implementation by Maurice Herlihy and his proof that the atomic operations of Lamport are linearizable, which is a correctness condition for concurrent data structures that guarantees every operation performed appears to take effect instantaneously at some point between the call and return of the operation \cite{herlihy1991wait,HerlihyLinearizability}. So while Lamport gave a way how to prove the safety and liveness of a data structure \cite{Lamport1983SPSCCircularBuffer}, Herlihy and Wing specified which safety and liveness conditions are necessary to prove linearizability \cite{HerlihyLinearizability}. 

Herlihy than extended Lamports work to wait-free data structures by using the atomic read and write operations of Lamport and the linearizability proof. He showed that every sequentual data structure can be made wait-free, which is the basis for wait-free algorithms available. His work (or work that builds upon his work) shows up conceptually in all of the wait-free algorithms \cite{naderibeni2023waitfreequeuepolylogarithmicstep,kogan2012methodology,wCQWaitFreeQueue,WaitFreeQueueWithWaitFreeMemoryReclamation,Kogan2011WaitFreeQueues,FastFetchAndAddWaitFreeQueue}. \cite{herlihy1991wait}