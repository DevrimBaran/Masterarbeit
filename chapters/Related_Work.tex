\chapter{Related Work}\label{ch:related-work}

The early foundations regarding wait-freedom were done indirectly by Leslie Lamport in 1983 \cite{Lamport1983SPSCCircularBuffer}. While his work did not directly address or formally define wait-freedom or lock-freedom, they laid the groundwork for Maurice Herlihy to define wait-freedom \cite{herlihy1991wait}. \\
In 1983, Leslie Lamport gave a formal method to write and prove the correctness of any concurrent module in a simple and modular way independent of the used data structure, which he refers to as modules. These modules consist of three components: 
\begin{itemize}
   \item state functions, which are abstract variables describing the module's state.
   \item initial conditions, which are predicates on the state functions.
   \item properties, which are a mix of safety and liveness requirements.
\end{itemize}
Safety requirements define what must never happen (e.g., a queue must never drop an element), while liveness requirements define what must eventually happen (e.g., a non-empty queue must eventually allow dequeueing). He also defines the usage of action sets and environment constraints, which separate the module action from the environments (e.g., the program where the data structure runs in). For example, a \ac{FIFO} queue specification would include:
\begin{itemize}
   \item State Functions: queue contents, operation parameters, return values.
   \item Initial Conditions: queue starts empty.
   \item Safety Condition: queue maintains \ac{FIFO} ordering.
   \item Liveness Condition: dequeue operation will eventually return a value.
\end{itemize}
This systematic methodology to prove the correctness of concurrent data structures laid the necessary groundwork for later developments. \cite{Lamport1983SPSCCircularBuffer}

Building on methodologies to prove concurrent data structure correctness, Herlihy and Wing provided linearisability as a correctness condition for concurrent objects, which is a guarantee that every operation performed appears to take effect instantaneously at some point between the call and return of the operation \cite{HerlihyLinearizability}. This correctness condition was then used to formalise wait-freedom in 1991 \cite{herlihy1991wait}. In the latter work, Herlihy proved that any sequential data structure can be transformed into a wait-free concurrent data structure \cite{herlihy1991wait}. A wait-free data structure must satisfy the following three constraints: 
\begin{itemize}
   \item Linearisability: operations take effect instantaneously at some point between the call and return of the operation.
   \item Bounded steps: operations end in a finite number of steps.
   \item Independence: operations finish regardless of other processes' execution (for later understanding: a process waiting for another process for a maximum number of time and then returning an error if that time is exceeded would still be considered wait-free, since it will finish regardless of the other process).
\end{itemize}
\cite{herlihy1991wait,HerlihyLinearizability}

Herlihy's universal construction and principles (or work that builds upon his work) appear conceptually throughout all of these wait-free algorithms \cite{Kogan2011WaitFreeQueues,FeldmanDechev2015WaitFreeRingBuffer,kogan2012methodology,FeldmanDechevV2,FeldmanDechevV3,RamalheteQueue,wCQWaitFreeQueue,Verma2013Scalable,FastFetchAndAddWaitFreeQueue,WangCacheCoherent,adampsc,jiffy,JayantiLog,Drescher2015GuardedSections,Mate√≠spmc,torquati2010singleproducersingleconsumerqueuessharedcache,Aldinucci2012EfficientSync,Wang2013BQueue,MaffioneCacheAware,ffq}. \cite{herlihy1991wait}

Additionally in 1983, Lamport presented a concurrent lock-free \ac{FIFO} queue implementation using a circular buffer. It works by using an array Q with two pointers HEAD and TAIL, where elements are added at TAIL and removed at HEAD. The producer first checks if the queue is full by testing if TAIL - HEAD equals the queue capacity m, and if so, it busy-waits. When space is available, it transfers the element bit-by-bit using into Q at position TAIL modulo m, then atomically increments TAIL to commit the addition. Similarly, the consumer checks if the queue is empty by testing if TAIL - HEAD equals 0, and if so, it busy-waits. When an element is available, it extracts the element bit-by-bit from Q at position HEAD modulo m using shift operations, then atomically increments HEAD to commit the removal. This means that while data is being shifted, other operations can observe partial states, but the queue still maintains its \ac{FIFO} properties correctly because an element is only considered "in" the queue after TAIL is incremented and only considered "removed" after HEAD is incremented. Eventhough busy-waits are used, the queue will fill eventually and be dequeued eventually, so the producer and also the consumer will finish in a finite number of steps making the Lamport queue wait-free Eventhough that term was not even defined then. \cite{Lamport1983SPSCCircularBuffer} 

The lamport queue is the basis of many more later invented wait-free queues discussed in \cref{ch:choosing-the-optimal-wait-free-data-structure}.

Kogan and Petrank later invented a method called fast-path slow-path, where first usually a lock-free method (the fast-path) is used to try to complete an operation, since lock-free algorithms are usually faster than wait-free algorithms. These lock-free paths are bounded by a maximum number of steps and if the operation does not complete within that bound, the algorithm tries to complete the operation using a wait-free method (the slow-path). So in cases where the fast-path succeeds often without switching to the slow path, the algorithm in general completes in a shorter time than a pure wait-free algorithm. This method is used by two algorithms with one of them having a great performance advantage, which will be demonstrated later in this thesis in \cref{ch:choosing-the-optimal-wait-free-data-structure}. \cite{kogan2012methodology}