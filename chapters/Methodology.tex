\chapter{Methodology}\label{ch:methodology}
To achieve the goal that is defined in \cref{sec:objective}, first all wait-free data structures that could be used for \ac{IPC} through shared memory in \ac{HRTS} need to be identified. To find all existing wait-free data structures a method was used that is more known from mapping studies or literature reviews. Multiple python scripts were implemented to do this \cite{githubMA}. A web scraper script was written to scrape over Google Scholar with the following queries:
\begin{itemize}
   \item \enquote{wait-free queue}
   \item \enquote{wait-free} (\enquote{mpmc} OR \enquote{multi-producer multi-consumer} OR \enquote{multi-writer multi-reader} OR \enquote{many-to-many}) \enquote{queue}
   \item \enquote{wait-free} (\enquote{mpsc} OR \enquote{multi-producer single-consumer} OR \enquote{single-writer multi-reader} OR \enquote{many-to-one}) \enquote{queue}
   \item \enquote{wait-free} (\enquote{spsc} OR \enquote{single-producer single-consumer} OR \enquote{single-writer single-reader} OR \enquote{one-to-one}) \enquote{queue}
   \item \enquote{wait-free} (\enquote{spmc} OR \enquote{single-producer multi-consumer} OR \enquote{multi-writer single-reader} OR \enquote{one-to-many}) \enquote{queue}
\end{itemize}
In Google Scholar a whitespace is considered as an AND. The rest is interpreted as read. With this approach I got a list of 1324 papers. The papers were then written into a CSV file split into query, rank (number of paper), title, year, authors, venue, citations, abstract snippet, full\_abstract and url with \enquote{;} as a delimiter. To extract all of this the information in Google Scholar was extracted and then for the full abstract the scraper went onto the source url and extracted the abstract there. If the url was a direct pdf link, a pdf reader was used to find the abstract. If an abstract was not extractable (some source site which was not considered or other problems), \enquote{ABSTRACT\_NOT\_FOUND} was written instead or if the paper was accessible the full paper was written instead into that cell. Because a lot of scientific web pages will put captchas if continuous requests are made undetected\_chromedriver was imported and used as the web driver. It is an enhanced version of chromedriver which bypasses anti-bot detections. After that I also implemented a regex analyser to analyse the abstracts of the found papers on the words \enquote{lock-free}, \enquote{wait-free}, and \enquote{obstruction-free} and also again without a hyphen in between these words. If these keywords were not found in the abstract or the abstract contained the flag \enquote{ABSTRACT\_NOT\_FOUND} the paper was removed from the CSV. This left 475 papers that contained at least one of these words in their abstract. After that the duplicates were removed by the algorithm, which left 325 papers. The duplicates were removed by checking the url of the paper. Now what was left had to be manually analysed to see if the paper was relevant for the topic. Since LibreOffice and Microsoft Excel have a limit of 32,767 characters per cell an abstract splitter had to be built to split the abstract into multiple cells. Analysing was done by reading the paper and checking if the paper was developing a wait-free \ac{FIFO} queue. While doing that also backward and forward search was done to even find more papers. Papers were only included, if and only if: 
\begin{itemize}
   \item All three of Herlihy's constraints were met, that we listed in \cref{ch:related-work}.
   \item The algorithms could be implemented on x86 architecture using Rust, since this is the architecture and programming language that is used and available for this work.
   \item The algorithm runs in an acceptable time to even benchmark it for an \ac{IPC} via a shared memory use-case.
\end{itemize}
In the end 17 papers were included from the 325 papers and 3 more papers were included by backward and forward search of the papers that were included. The papers were then split into 4 contention categories:
\begin{itemize}
   \item \ac{MPMC} queues \cite{Kogan2011WaitFreeQueues,FeldmanDechevV2,RamalheteQueue,Verma2013Scalable,FastFetchAndAddWaitFreeQueue,wCQWaitFreeQueue}
   \item \ac{MPSC} queues \cite{WangCacheCoherent,jiffy,JayantiLog,Drescher2015GuardedSections}
   \item \ac{SPMC} queues \cite{Mate√≠spmc}
   \item \ac{SPSC} queues \cite{Lamport1983SPSCCircularBuffer,torquati2010singleproducersingleconsumerqueuessharedcache,Wang2013BQueue,MaffioneCacheAware,ffq}
\end{itemize}
Now the wait-free \ac{FIFO} queues had to be compared performance wise. Some papers were just improvements of other papers so only the improved version was used for the comparison. Some other papers showed multiple ways of implementing a wait-free \ac{FIFO} data structure. In the end 6 \ac{MPMC} queues, 4 \ac{MPSC} queues, 1 \ac{SPMC} queue and 11 \ac{SPSC} queues were included into this work.