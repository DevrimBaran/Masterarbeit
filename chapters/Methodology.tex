\chapter{Methodology}\label{ch:methodology}
To achieve the goal that is defined in \cref{sec:objective}, first all wait-free data structures that could be used for \ac{IPC} through shared memory in \ac{HRTS} need to be found. To do this a method was used that is more known from mapping studys or literature reviews. Multiple python scripts were implemented to do this \cite{githubMA}. A web scraper script was written to scrape over google scholar with the following querys:
\begin{itemize}
   \item \enquote{wait-free queue}
   \item \enquote{wait-free} (\enquote{mpmc} OR \enquote{multi-producer multi-consumer} OR \enquote{multi-writer multi-reader} OR \enquote{many-to-many}) \enquote{queue}
   \item \enquote{wait-free} (\enquote{mpsc} OR \enquote{multi-producer single-consumer} OR \enquote{single-writer multi-reader} OR \enquote{many-to-one}) \enquote{queue}
   \item \enquote{wait-free} (\enquote{spsc} OR \enquote{single-producer single-consumer} OR \enquote{single-writer single-reader} OR \enquote{one-to-one}) \enquote{queue}
   \item \enquote{wait-free} (\enquote{spmc} OR \enquote{single-producer multi-consumer} OR \enquote{multi-writer single-reader} OR \enquote{one-to-many}) \enquote{queue}
\end{itemize}
In google scholar a whitespace is considered as an AND. The rest is interpreted as read. With this approach i got a list of 1324 papers. The papers were then written into a csv file split into query, rank (number of paper), title, year, authors, venue, citations, abstract snippet, full\_abstract and url with \enquote{;} as an delimiter. To extract all of this the information in google scholar was extracted and then for the full abstract the scraper went onto the source url and extracted the abstract there. If the url was a direct pdf link, a pdf reader was used to find the abstract. If an abstract was not extractable (some source site which was not considered or other problems), \enquote{ABSTRACT\_NOT\_FOUND} was written instead or if the paper was accessible the full paper was written instead into that cell. Because a lot of scientific web pages will put ceptchas if continuously requests are made undetected\_chromedriver was importet and used as the web driver. It is an enhanced version of chromedriver which bypasses anti bot detections. After that I also implemented a regex analyzer to analyze the abstracts of the found papers on the words \enquote{lock\-free}, \enquote{wait\-free}, and \enquote{obstruction\-free} and also again without a hyphon inbetween these words. If these keywords were not found in the abstract or the abstract contained the flag \enquote{ABSTRACT\_NOT\_FOUND} the paper was removed from the csv. This left 475 papers that contained at least one of these words in their abstract. After that the duplicates were removed by the algorithm, which left 325 papers. The duplicates were removed by checking the url of the paper. Now what was left had to be manually analyzed to see if the paper was relevant for the topic. Since libre office and microsoft excel has a limit of 32.767 charachters per cell a abstract splitter had to be build to split the abstract into multiple cells. Analyzing was done by reading the paper and checking if the paper was developing an wait-free fifo queue. While dooing that also backward and forward search was done to even find more papers. Papers where only included, if and only if: 
\begin{itemize}
   \item All three of Herlihys contraints were met, that we listed in \cref{ch:related-work}.
   \item The algorithms could be implemented on x86 architecture using rust, since this is the architecture and programming languge that is used in the implementation of this thesis. The reason x86 is used, is because that is the architecture available and it is also the architecture that is used in most modern computers. For example Khanchandani and Wattenhofer \cite{halfincrementhalfmax} created a queue using theoretical atomic primitives called half-increment and half-max. These primitives do not exist on real hardware. The same goes for Bédin et al. \cite{memorytomemory} who created a queue using theoretical atomic primitives called memory-to-memory.
   \item The algorithm runs in an acceptable time complexity to even benchmark it. For example the algorithm by Johnen et al. \cite{johnen_et_al:LIPIcs.OPODIS.2022.4} and Naderibeni and Ruppert \cite{polylog} were running so slow that they were not been considered to be benchmarked, since they take too long to run and wait for results. Both use a really complex to implement tree structure that is creating an extreme overhead for just producing and consuming items. 
\end{itemize}
In the end 17 papers were included from the 325 papers and 3 more paper were included by backward and forward search of the papers that were included. The papers were then split into 4 contention categories:
\begin{itemize}
   \item \ac{MPMC} queues \cite{Kogan2011WaitFreeQueues,FeldmanDechev2015WaitFreeRingBuffer,kogan2012methodology,FeldmanDechevV2,FeldmanDechevV3,RamalheteQueue,wCQWaitFreeQueue,Verma2013Scalable,FastFetchAndAddWaitFreeQueue}
   \item \ac{MPSC} queues \cite{WangCacheCoherent,adampsc,jiffy,JayantiLog,Drescher2015GuardedSections}
   \item \ac{SPMC} queues \cite{Mateíspmc}
   \item \ac{SPSC} queues \cite{Lamport1983SPSCCircularBuffer,torquati2010singleproducersingleconsumerqueuessharedcache,Aldinucci2012EfficientSync,Wang2013BQueue,MaffioneCacheAware,ffq}
\end{itemize}
Now the wait-free fifo queues had to be compared performance wise. Some papers were just improvements of other papers so only the improved version was used for the comparison. Some other papers showed multiple ways of implementing a wait-free fifo datastructure. In the end 6 \ac{MPMC} queues, 4 \ac{MPSC} queues, 1 \ac{SPMC} queue and 11 \ac{SPSC} queues were included into this work.