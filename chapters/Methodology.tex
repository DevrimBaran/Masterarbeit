\chapter{Methodology}\label{ch:methodology}
To achieve the goal that is defined in \cref{sec:objective}, first all wait-free data structures that could be used for \ac{IPC} through shared memory in \ac{HRTS} need to be found. To find all existing wait-free data structures, a method was used that is more known from mapping studies or literature reviews. Multiple python scripts were implemented to do this as seen in the accompanying GitHub repository \cite{githubMA}. A web scraper script was written to scrape over Google Scholar with the following queries:
\begin{itemize}
   \item \enquote{wait-free queue}
   \item \enquote{wait-free} (\enquote{mpmc} OR \enquote{multi-producer multi-consumer} OR \enquote{multi-writer multi-reader} OR \enquote{many-to-many}) \enquote{queue}
   \item \enquote{wait-free} (\enquote{mpsc} OR \enquote{multi-producer single-consumer} OR \enquote{single-writer multi-reader} OR \enquote{many-to-one}) \enquote{queue}
   \item \enquote{wait-free} (\enquote{spsc} OR \enquote{single-producer single-consumer} OR \enquote{single-writer single-reader} OR \enquote{one-to-one}) \enquote{queue}
   \item \enquote{wait-free} (\enquote{spmc} OR \enquote{single-producer multi-consumer} OR \enquote{multi-writer single-reader} OR \enquote{one-to-many}) \enquote{queue}
\end{itemize}
In Google Scholar a whitespace is considered as an AND. The rest is interpreted as read. With this approach a list of 1324 papers was found. The papers were then written into a CSV file split into query, rank (number of paper), title, year, authors, venue, citations, abstract snippet, full\_abstract and url with \enquote{;} as a delimiter. To extract all of this, the information in Google Scholar was extracted and then for the full abstract the scraper went onto the source url and extracted the abstract there. If the url was a direct pdf link, a pdf reader was used to find the abstract. If an abstract was not extractable (some source site which was not considered or other problems), \enquote{ABSTRACT\_NOT\_FOUND} was written instead or if the paper was accessible, the full paper was written instead into that cell. Because a lot of scientific web pages will put captchas if continuous requests are made, undetected\_chromedriver was imported and used as the web driver. It is an enhanced version of chromedriver which bypasses anti-bot detections. After that a regex analyser was implemented to analyse the abstracts of the found papers on the words \enquote{lock-free}, \enquote{wait-free}, and \enquote{obstruction-free} and also again without a hyphen in between these words. If these keywords were not found in the abstract, the paper was removed from the CSV. The abstracts of paper with the tag \enquote{ABSTRACT\_NOT\_FOUND} hat to be analyzed manually. This left 475 papers that contained at least one of these words in their abstract. After that the duplicates were removed by the algorithm, which left 325 papers. The duplicates were removed by checking the url of the paper. Now what was left had to be manually analysed to see if the paper was relevant for the topic. Since LibreOffice and Microsoft Excel have a limit of 32,767 characters per cell, an abstract splitter had to be built to split the abstract into multiple cells. Analysing was done by reading the paper and checking if the paper was presenting a wait-free \ac{FIFO} queue. The reason why only \ac{FIFO} queues were considered will be explained in the next chapter. While doing that, also backward and forward search was done to find more papers. Only papers were considered, if the queue described in the paper met the following criteria: 
\begin{itemize}
   \item All three of Herlihy's constraints were met, that we listed in \cref{ch:related-work}.
   \item The algorithms could be implemented on x86 architecture using Rust, since this is the architecture and programming language that is used and available for this work.
   \item The algorithm runs in an acceptable time to even benchmark it for an \ac{IPC} via a shared memory use-case.
\end{itemize}
In the end, 17 queeus were found through web scraping and 3 queues were found by backward and forward search of the papers that were included. The queues were then split into 4 contention categories:
\begin{itemize}
   \item \ac{MPMC} queues with 6 queues \cite{Kogan2011WaitFreeQueues,FeldmanDechevV2,RamalheteQueue,Verma2013Scalable,FastFetchAndAddWaitFreeQueue,wCQWaitFreeQueue}
   \item \ac{MPSC} queues with 4 queues \cite{WangCacheCoherent,jiffy,JayantiLog,Drescher2015GuardedSections}
   \item \ac{SPMC} queues with 1 queue \cite{Mate√≠spmc}
   \item \ac{SPSC} queues with 11 queues \cite{Lamport1983SPSCCircularBuffer,torquati2010singleproducersingleconsumerqueuessharedcache,Wang2013BQueue,MaffioneCacheAware,ffq,JayantiLog}
\end{itemize}
Some papers were just improvements of other papers, so only the improved version was used for the comparison. Some other papers showed multiple ways of implementing a wait-free \ac{FIFO} data structure.