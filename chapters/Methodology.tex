\chapter{Methodology}\label{ch:methodology}
To achieve the goal defined in \cref{sec:objective}, first, all wait-free data structures that can be used for \ac{IPC} through shared memory in \ac{HRTS} need to be identified. To identify all existing wait-free data structures, a method was employed that is more commonly used in mapping studies or literature reviews. Multiple Python scripts were implemented to do this, as seen in the accompanying GitHub repository \cite{githubMA}. A web scraper script was written to scrape over Google Scholar with the following queries:
\begin{itemize}
   \item \enquote{wait-free queue}
   \item \enquote{wait-free} (\enquote{mpmc} OR \enquote{multi-producer multi-consumer} OR \enquote{multi-writer multi-reader} OR \enquote{many-to-many}) \enquote{queue}
   \item \enquote{wait-free} (\enquote{mpsc} OR \enquote{multi-producer single-consumer} OR \enquote{single-writer multi-reader} OR \enquote{many-to-one}) \enquote{queue}
   \item \enquote{wait-free} (\enquote{spsc} OR \enquote{single-producer single-consumer} OR \enquote{single-writer single-reader} OR \enquote{one-to-one}) \enquote{queue}
   \item \enquote{wait-free} (\enquote{spmc} OR \enquote{single-producer multi-consumer} OR \enquote{multi-writer single-reader} OR \enquote{one-to-many}) \enquote{queue}
\end{itemize}
In Google Scholar, a whitespace is considered as an AND. The rest is interpreted as read. With this approach, a list of 1324 papers was found. The papers were then recorded in a CSV file split into query, rank (number of paper), title, year, authors, venue, citations, abstract snippet, full\_abstract and url with \enquote{;} as a delimiter. To extract all of this, the information in Google Scholar was extracted, and then, for the full abstract, the scraper went to the source URL and extracted the abstract there. If the URL was a direct PDF link, a PDF reader was used to find the abstract. If an abstract was not extractable (some source site which was not considered or other problems), \enquote{ABSTRACT\_NOT\_FOUND} was written instead, or if the paper was inaccessible, the whole paper was written instead into that cell.
Because a lot of scientific web pages will put captchas if continuous requests are made, \enquote{undetected\_chromedriver} was imported and used as the web driver. It is an enhanced version of ChromeDriver which bypasses anti-bot detections. After that, a regex analyser was implemented to analyse the abstracts of the found papers on the words \enquote{lock-free}, \enquote{wait-free}, and \enquote{obstruction-free} and also again without a hyphen in between these words. If these keywords were not found in the abstract, the paper was removed from the CSV. The abstracts of papers with the tag \enquote{ABSTRACT\_NOT\_FOUND} had to be analysed manually. This left 475 papers that contained at least one of these words in their abstract. After that, the duplicates were removed by the algorithm, resulting in 325 remaining papers. The duplicates were removed by checking the URL of the paper. Now what was left had to be manually analysed to see if the paper was relevant to the topic. Since LibreOffice and Microsoft Excel have a limit of 32,767 characters per cell, an abstract splitter was built to split the abstract into multiple cells. Analysis was conducted by reading the paper and verifying whether it presented a wait-free \ac{FIFO} queue. The reason why only \ac{FIFO} queues were considered will be explained in the next chapter. While doing that, a backwards and forward search was also done to find more papers. Only papers were considered if the queue described in the paper met the following criteria: 
\begin{itemize}
   \item All three of Herlihy's constraints were met, which are listed in \cref{ch:related-work}.
   \item The algorithms can be implemented on the x86 architecture using Rust, as this is the architecture and programming language used and available for this work.
   \item The algorithm runs in an acceptable time to even benchmark it for an \ac{IPC} via a shared memory use-case.
\end{itemize}
In the end, 17 queues were identified through web scraping, and 3 queues were discovered through a backwards and forward search of the included papers. The queues were then split into 4 contention categories:
\begin{itemize}
   \item \ac{MPMC} queues with 6 queues \cite{Kogan2011WaitFreeQueues,FeldmanDechevV2,RamalheteQueue,Verma2013Scalable,FastFetchAndAddWaitFreeQueue,wCQWaitFreeQueue}
   \item \ac{MPSC} queues with 4 queues \cite{WangCacheCoherent,jiffy,JayantiLog,Drescher2015GuardedSections}
   \item \ac{SPMC} queues with 1 queue \cite{Mate√≠spmc}
   \item \ac{SPSC} queues with 11 queues \cite{Lamport1983SPSCCircularBuffer,torquati2010singleproducersingleconsumerqueuessharedcache,Wang2013BQueue,MaffioneCacheAware,ffq,JayantiLog}
\end{itemize}
Some papers were just improvements of other papers, so only the improved version was used for the comparison. Some other papers showed multiple ways of implementing a wait-free \ac{FIFO} data structure.
