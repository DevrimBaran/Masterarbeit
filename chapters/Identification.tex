\chapter{Choosing the Optimal Wait-Free Data Structure and algorithm}\label{ch:choosing-the-optimal-wait-free-data-structure}

\section{Optimal Wait-Free Data Structure}\label{sec:optimal-wait-free-data-structure}

An important question is what data structure to use for the implementation of a wait-free synchronisation technique for \ac{IPC}. M. Herlihy showed that every data structure can be made wait-free \cite{herlihy1991wait}. So it is important to choose the optimal data structure for our use. Considering that the reason of this work is to optimize modern manufacturing and automation, some form of correct data flow order is as well necessary for correct work flow for instance in an modern manufacturing line or more critical in a driverless car. From that point on we can think of an already natural fit like \ac{FIFO} queues. Natural because in such queues a producer process can enqueue messages and the consumer process can dequeue messages sequentially. This models real-world data flows (sensor readings, commands, network packets), which are inherently sequential. Consequently with such queues the order of the data flow is preversed without the need of implementing additional functionalities. In contrast, data structures like stacks, sets, or maps do not maintain this kind of arrival order and moreover add semantics like \ac{LIFO} order or key-value pairs, which are in most cases not desired or even unnecessary. This would bring in the need of additional functions to just get rid of undesired side effects. Furthermore in a queue we only have two operations, an enqueue and an dequeue operation. All the other data structures introduce more operations and therefore more complexity and therefore more performance overhead. The less operations we have, the less complex the implementation will be. Because of these advantages and also because of the fact that in most publications in the wait-free domain queues are beeing used, limiting this thesis to queues only is reasonable. \cite{jiffy}

\section{Wait-Free Algorithms}\label{sec:wait-free-alg}
As we know now on which data structure we should focus an important question is which algorithms to use. In \cref{ch:methodology} we defined 4 different contention categories. Which kind of algorithm going to be used will be decided contention based. Since all of them have different complexity in runtime, it is important to choose the right contention category for the right use case to save resources and have faster execution times to meet the timing constraints of \ac{HRTS}. In modern manufacturing and automation devices are used which can run multiple applications on a single device. This could mean that every application running on one device could be a producer and a consumer to each other (\ac{MPMC}) and also maybe some single application of all applications running on one device produces data for just a single other consuming application (\ac{SPSC}). And maybe some single application is a producer for multiple consuming applications (\ac{SPMC}) and multiple applications are producers of a single consuming application (\ac{MPSC}). So it can be that all cases can occur in just one device. This means that we have to consider all the different cases of contention. In the following sections we will discuss the different cases and their algorithms. We will also implement them and test their performance (how fast an algorithm can produce and consume items concurrently). In the following we will first implement all algorithms for each category and then benchmark them. Then after that from each category the best algorithm will be chosen and then benchmarked with each other to see, if really 4 different algorithms for each category is necessary. The reason for that is, that for instance the best performed \ac{MPMC} algorithm could outperform all other algorithms even for their contention category, since a \ac{MPMC} approach can cover all contention cases. The goal with this approach is to have as little overhead as possible, since an algorithm explicitly implemented for a \ac{MPMC} use case could have extreme overhead for a \ac{SPSC} case.

\subsection{Single Producer and Single Consumer}\label{subsec:single-producer-and-single-consumer}
This is the most simple form of \ac{IPC}. In \ac{SPSC} there is nearly no contention from other processes, because we only have one producer and one consumer. >The only contention we have here is between the consumer and producer. In this subsection we will focus on benchmarking all the different \ac{SPSC} approaches we found so that we can select the optimal approach for the \ac{SPSC} use case. The following list shows the algorithms that are used for the \ac{SPSC} contention case:
\begin{itemize}
   \item Lamports Circular Bufffer Queue \cite{Lamport1983SPSCCircularBuffer,MaffioneCacheAware}
   \item \ac{BLQ}, Batched Lamport Queue, which is a Lamport Queue with batching to reduce cache conflicts \cite{MaffioneCacheAware}
   \item \ac{LLQ}, Lamport Queue reducing cache conflicts by lazily loading control indices \cite{MaffioneCacheAware}
   \item \ac{FFQ}, reducing cache misses by embedding synchronization within queue slots \cite{ffq}
   \item \ac{IFFQ}, Additionally on FastForward producer and consumer works rarely on same cache line \cite{MaffioneCacheAware}
   \item Cache Batching (In this example done as \ac{BIFFQ}), use temporary buffer to accumulate items before burst write to shared queue \cite{MaffioneCacheAware}
   \item B-Queue, Batching with backtracking for more efficient cache usage \cite{Wang2013BQueue}
   \item \ac{uSPSC}, pool of \ac{SPSC} lamport buffer rings that can grow and shrink on demand \cite{torquati2010singleproducersingleconsumerqueuessharedcache,Aldinucci2012EfficientSync}
   \item \ac{dSPSC}, using dynamically grwoing linked list of nodes \cite{torquati2010singleproducersingleconsumerqueuessharedcache,Aldinucci2012EfficientSync}
   \item \ac{mSPSC}, is also cache batching but with lamport queue \cite{torquati2010singleproducersingleconsumerqueuessharedcache}.
\end{itemize}
\todo{put benchmark file here}
\todo{put benchmark results here}

\subsection{Multiple Producer and Single Consumer}\label{subsec:multiple-producer-and-single-consumer}
This is a bit more complex to implement than the \ac{SPSC} case. The reason for that is, that now we have multiple producers which can enqueue items at the same time while the consumer can dequeue items. Here we have to be a bit more careful with the implementation. Also here multiple approaches are available from different papers. These are the algorithms that get benchmarked for this case to choose the optimal one: 
\begin{itemize}
   \item 
\end{itemize}

\subsection{Single Producer and Multiple Consumer}\label{subsec:single-producer-and-multiple-consumer}
Since there is only one specific algorithm we found for this contention category, we will not do a benchmark for this category. The algorithm is the following:
\todo{Put all the algorithms in here}
\begin{itemize}
   \item 
\end{itemize}
This algorithm will be implemented and benchmarked against the best performed algorithms of the other contention categories directly.


\subsection{Multiple Producer and Multiple Consumer}\label{subsec:multiple-producer-and-multiple-consumer}
Finally we can look into the \ac{MPMC} case. These are the algorithms that get benchmarked:
\begin{itemize}
   \item Linked-List queue with peer helping via shared descriptor and Phase-Based priority helping \cite{Kogan2011WaitFreeQueues,kogan2012methodology}
   \item Linked-List queue with peer helping via shared descriptor and Turn-Based helping with \cite{RamalheteQueue}
   \item Array/Ring-Buffer queue with \ac{FAA}-Distributed indexing and slot level coordination via direct CAS on slot with helping for unfilled reservations \cite{FastFetchAndAddWaitFreeQueue}
   \item Array/Ring-Buffer queue with \ac{FAA}-Distributed indexing and slot level coordination sequence ID (seqid) matching and bitmasked slot state management \cite{FeldmanDechev2015WaitFreeRingBuffer,FeldmanDechevV2,FeldmanDechevV3}
   \item Segment array queue with \ac{FAA} Indexing and collaborative slot management \cite{wCQWaitFreeQueue}
   \item Dedicated helper thread mediated queue \cite{Verma2013Scalable}
\end{itemize}
