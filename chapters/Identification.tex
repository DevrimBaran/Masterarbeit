\chapter{Analyzing existing Wait-Free Data Structures and Algorithms}\label{ch:choosing-the-optimal-wait-free-data-structure}

\section{Optimal Wait-Free Data Structure}\label{sec:optimal-wait-free-data-structure}

An important question is what data structure to use for the implementation of a wait-free synchronisation technique for \ac{IPC}. M. Herlihy showed that every sequential data structure can be made wait-free \cite{herlihy1991wait}. So it is important to choose the optimal data structure for our use. Considering that the reason of this work is to optimize modern manufacturing and automation, some form of correct data flow order is as well necessary for correct work flow for instance in an modern manufacturing line or more critical in a driverless car. Hence an already natural fit like \ac{FIFO} queues. Natural because in such queues a producer process can enqueue messages and the consumer process can dequeue messages sequentially. This models real-world data flows (sensor readings, commands, network packets), which are inherently sequential. Consequently with such queues the order of the data flow is preversed without the need of implementing additional functionalities. In contrast, data structures like stacks, sets, or maps do not maintain this kind of arrival order and moreover add semantics like \ac{LIFO} order or key-value pairs, which are in most cases not desired or even unnecessary. This would bring in the need of additional functions to just get rid of undesired side effects. Furthermore in a queue only two operations exist, an enqueue and an dequeue operation. All the other data structures introduce more operations and therefore more complexity and therefore more performance overhead. The less operations exist, the less complex the implementation will be. Because of these advantages and also because of the fact that in most publications in the wait-free domain queues are beeing used, limiting this thesis to queues only is reasonable. \cite{jiffy}

\section{Wait-Free Algorithms}\label{sec:wait-free-alg}
With the appropriate data structure established, an important consideration is the selection of suitable algorithms. In \cref{ch:methodology} 4 different contention categories are defined. Which kind of algorithm going to be used will be decided contention based. Since all of them have different complexity in runtime, it is important to choose the right contention category for the right use case to save resources and have faster execution times to meet the timing constraints of \ac{HRTS}. In modern manufacturing and automation devices are used which can run multiple applications on a single device. This could mean that every application running on one device could be a producer and a consumer to each other (\ac{MPMC}) and also maybe some single application of all applications running on one device produces data for just a single other consuming application (\ac{SPSC}). And maybe some single application is a producer for multiple consuming applications (\ac{SPMC}) and multiple applications are producers of a single consuming application (\ac{MPSC}). So it can be that all cases can occur in just one device. This means that all the different cases of contention have to be considered. In the following the different cases and their algorithms will be discussed. Moreover they will be implemented and their performance tested via a benchmarked (how fast an algorithm can produce and consume items concurrently). Subsequently from each category the best algorithm will be chosen and their performance will be compared with each other to Identify, if 4 different categories are necessary. The reason for that is, that for instance the best performed \ac{MPMC} algorithm could outperform all other algorithms even for their contention category, since a \ac{MPMC} approach can cover all contention cases. The goal with this approach is to have as little overhead as possible, since an algorithm explicitly implemented for a \ac{MPMC} use case could have extreme overhead for a \ac{SPSC} case. The implementation will be discussed in \cref{ch:implementation} and the results of the performance readings will be discussed in \cref{ch:results}. The following subsections will give an overview of the different contention categories and their algorithms found. The subsections will also shortly describe how the enqueue and dequeue in these algorithms work. Given that all algorithms found are about inter-thread communication and not \ac{IPC}, the following explanation of the specific algorithms will include the terminology of threads to be precise about the papers. In \cref{ch:implementation} it will be elaborated how these thread based algorithms are adapted to \ac{IPC} in Rust. Other minor Rust-specific deviations from the following algorithms (different types required by Rust's safety model, additional memory fences, etc.) can be seen in the GitHub repository accompanying this thesis \cite{githubMA}. These are not detailed here as such explanations would provide limited value to the topic of this work, and while a comprehensive analysis would be relevant, it would require more extensive exploration than is feasible within the scope of this work.

\subsection{\acf{SPSC}}\label{subsec:single-producer-and-single-consumer}
This is the most simple form of \ac{IPC}. In \ac{SPSC} there is nearly no contention from other processes, because only one producer and one consumer is working. The only contention is between the consumer and producer. In this subsection the focus will lie on benchmarking all the different \ac{SPSC} approaches found so that the optimal algorithm for the \ac{SPSC} use case can be selected. The following list shows the algorithms that are used for the \ac{SPSC} contention case. Since \ac{BLQ}, \ac{LLQ}, \ac{BIFFQ} and \ac{IFFQ} are from the same paper, explanations and variables are shared between these algorithms to avoid redundancy:
\subsubsection{Lamport's Circular Buffer Queue}
Uses a circular array with two shared indices for synchronization, based on the algorithm originally proposed by Leslie Lamport in 1983 \cite{Lamport1983SPSCCircularBuffer} and shown here in \cref{alg:lamport-queue} following Maffione et al.'s version \cite{MaffioneCacheAware}. The producer first checks if the queue is full (reached capacity N) in line 2, which requires reading the consumer's \texttt{read} index. If the queue is not full, it writes the input data to the slot at position \texttt{write \& mask} in line 5, where the bitwise AND operation wraps the index around when it reaches the array end. The producer then increments \texttt{write} to signal that the written data is available in line 7. The consumer mirrors this behavior by checking if the queue is empty in line 12, which requires reading the producer's \texttt{write} index. If the queue is not empty, the consumer reads data from the slot at position \texttt{read \& mask} in line 16, using the same modulo arithmetic through bitwise AND, and incrementing its \texttt{read} index to signal that the slot is available in line 17. This wraparound behavior creates the circular buffer structure, allowing the fixed-size array to be reused continuously. This design requires the queue size to be a power of two for the mask operation to work correctly. Each operation requires accessing both shared indices plus the data slot, causing up to three cache misses per item when the queue moves between nearly empty and nearly full states. According to Drepper, cache misses occur when requested data is not in the local CPU core's cache and must be fetched from another core's cache, with the cache coherence protocol ensuring memory consistency across all cores \cite{drepper2007every}. Drepper showed that performance can degrade by 390\%, 734\%, and 1,147\% for 2, 3, and 4 threads respectively. This happens because cache lines, the 64-byte blocks (on x86 architectures) that move between CPU caches, ping-pong between the producer's and consumer's cores as they take turns accessing the same memory locations \cite{drepper2007every}.

\begin{algorithm}[!ht]
   \centering
   \captionsetup{justification=centering}
   \caption{Lamports Queue \cite{MaffioneCacheAware}}
   \label{alg:lamport-queue}
   \scriptsize
   \begin{algorithmic}[1]
       \Function{lq\_enqueue}{$q$, $e$}
           \If{$q.write - q.read = N$} \Comment{Check if full}
               \State \Return $-1$ \Comment{No space}
           \EndIf
           \State $q.slots[q.write \land q.mask] \gets e$
           \State \texttt{store\_release\_barrier()}
           \State $q.write \gets q.write + 1$
           \State \Return $0$
       \EndFunction
       
       \State
       
       \Function{lq\_dequeue}{$q$}
           \If{$q.read = q.write$} \Comment{Check if empty}
               \State \Return \texttt{NULL\_ELEM} \Comment{Queue empty}
           \EndIf
           \State \texttt{load\_acquire\_barrier()}
           \State $e \gets q.slots[q.read \land q.mask]$
           \State $q.read \gets q.read + 1$
           \State \Return $e$
       \EndFunction
   \end{algorithmic}
   \cite{MaffioneCacheAware}
\end{algorithm}

\subsubsection{\acl{LLQ}}
Reduces the described cache misses of Lamports queue by postponing index reads until necessary, as shown in \cref{alg:llq}. Additionally to Lamports original enqueue function the producer maintains a local \texttt{read\_shadow} copy and only updates it when running out of known free slots (Lines 2 - 6). Similarly, the consumer uses \texttt{write\_shadow} to avoid repeatedly checking for new items. Additionally, \ac{LLQ} keeps $K$ slots (where $K$ is slots per cache line) permanently empty, preventing producer and consumer from touching the same cache line when the queue is full. This works well when one thread is faster than the other, reducing worst-case misses from 3 to about 2 per item. \cite{MaffioneCacheAware}

\begin{algorithm}[!ht]
   \centering
   \captionsetup{justification=centering}
   \caption{\acl{LLQ} Operations \cite{MaffioneCacheAware}}
   \label{alg:llq}
   \scriptsize
   \begin{algorithmic}[1]
       \Function{llq\_enqueue}{$q$, $e$}
           \If{$q.write - q.read\_shadow = N - K$} \Comment{Lazy load check}
               \State $q.read\_shadow \gets q.read$ \Comment{Update shadow}
               \If{$q.write - q.read\_shadow = N - K$}
                   \State \Return $-1$ \Comment{No space}
               \EndIf
           \EndIf
           \State $q.slots[q.write \land q.mask] \gets e$
           \State \texttt{store\_release\_barrier()}
           \State $q.write \gets q.write + 1$
           \State \Return $0$
       \EndFunction
       
       \State
       
       \Function{llq\_dequeue}{$q$}
           \If{$q.read = q.write\_shadow$} \Comment{Lazy load check}
               \State $q.write\_shadow \gets q.write$ \Comment{Update shadow}
               \If{$q.read = q.write\_shadow$}
                   \State \Return \texttt{NULL\_ELEM}
               \EndIf
           \EndIf
           \State \texttt{load\_acquire\_barrier()}
           \State $e \gets q.slots[q.read \land q.mask]$
           \State $q.read \gets q.read + 1$
           \State \Return $e$
       \EndFunction
   \end{algorithmic}
\end{algorithm}

\subsubsection{\acl{BLQ}}
Extends \ac{LLQ} with explicit batching to further reduce synchronization costs, as detailed in \cref{alg:blq}. The producer accumulates items using private \texttt{write\_priv} in line 11, filling slots without updating the shared \texttt{write} index. Only the function \texttt{blq\_enqueue\_publish} in lines 15 - 18 makes the batch visible by advancing \texttt{write} in line 17. The consumer works symmetrically, using \texttt{read\_priv} in line 32 for local progress before updating \texttt{read} in line 40. With typical batch sizes like $B = 32$, synchronization overhead is amortized across operations, reducing cache misses. However, the application using this queue design must explicitly call the publish functions even with partial batches to avoid unbounded latency, because items remain invisible to the consumer until published. This design particularly benefits applications that naturally process data in batches, such as network packet processing, where batch boundaries are well-defined. \cite{MaffioneCacheAware}

\begin{algorithm}[!ht]
   \centering
   \captionsetup{justification=centering}
   \caption{\acl{BLQ} Operations \cite{MaffioneCacheAware}}
   \label{alg:blq}
   \scriptsize
   \begin{algorithmic}[1]
       \Function{blq\_enqueue\_space}{$q$, $needed$}
           \State $space \gets N - K - (q.write\_priv - q.read\_shadow)$
           \If{$space < needed$}
               \State $q.read\_shadow \gets q.read$ \Comment{Update shadow}
               \State $space \gets N - K - (q.write\_priv - q.read\_shadow)$
           \EndIf
           \State \Return $space$
       \EndFunction
       
       \State
       
       \Function{blq\_enqueue\_local}{$q$, $e$}
           \State $q.slots[q.write\_priv \land q.mask] \gets e$
           \State $q.write\_priv \gets q.write\_priv + 1$
       \EndFunction
       
       \State
       
       \Function{blq\_enqueue\_publish}{$q$}
           \State \texttt{store\_release\_barrier()}
           \State $q.write \gets q.write\_priv$
       \EndFunction
       
       \State
       
       \Function{blq\_dequeue\_space}{$q$}
           \State $available \gets q.write\_shadow - q.read\_priv$
           \If{$available = 0$}
               \State $q.write\_shadow \gets q.write$ \Comment{Update shadow}
               \State $available \gets q.write\_shadow - q.read\_priv$
           \EndIf
           \State \Return $available$
       \EndFunction
       
       \State
       
       \Function{blq\_dequeue\_local}{$q$}
           \State \texttt{load\_acquire\_barrier()}
           \State $e \gets q.slots[q.read\_priv \land q.mask]$
           \State $q.read\_priv \gets q.read\_priv + 1$
           \State \Return $e$
       \EndFunction
       
       \State
       
       \Function{blq\_dequeue\_publish}{$q$}
           \State $q.read \gets q.read\_priv$
       \EndFunction
   \end{algorithmic}
\end{algorithm}

\subsubsection{\acl{FFQ}}
Synchronization by embedding control information directly within the data slots, eliminating separate shared indices shown in \cref{alg:ffq}. Unlike Lamport's queue which requires checking both \texttt{head} and \texttt{tail} indices, \acsp{FFQ} producer simply examines if the next slot contains \texttt{NULL} before writing. When the slot is empty (\texttt{NULL}), the producer writes the data directly and advances its private \texttt{head} index. The consumer follows a similar pattern by reading from the current slot position, and if data is present (non-\texttt{NULL}), it retrieves the value, writes \texttt{NULL} to mark the slot empty, and advances its private \texttt{tail} index. This design couples control and data, reducing shared memory accesses from three (\texttt{head}, \texttt{tail}, \texttt{buffer}) to just one (\texttt{buffer} slot). Each thread maintains its own private index that never needs synchronization. The producer tracks where to write next through \texttt{head}, while the consumer tracks where to read next through \texttt{tail}. The \texttt{NULL} value serves dual purpose as both an empty indicator and the synchronization mechanism. While this approach reduces memory barriers and cache misses significantly, it still has the ping-pong effect when the queue has few elements, causing the producer and consumer to operate on the same cache line. \cite{ffq} 

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{\acl{FFQ} Operations \cite{ffq}}
    \label{alg:ffq}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{ffq\_enqueue}{$q$, $data$}
            \If{$q.buffer[q.head] \neq \texttt{NULL}$}
                \State \Return \texttt{EWOULDBLOCK}
            \EndIf
            \State $q.buffer[q.head] \gets data$
            \State $q.head \gets \text{NEXT}(q.head)$
            \State \Return $0$
        \EndFunction
        
        \State
        
        \Function{ffq\_dequeue}{$q$}
            \State $data \gets q.buffer[q.tail]$
            \If{$data = \texttt{NULL}$}
                \State \Return \texttt{EWOULDBLOCK}
            \EndIf
            \State $q.buffer[q.tail] \gets \texttt{NULL}$
            \State $q.tail \gets \text{NEXT}(q.tail)$
            \State \Return $data$
        \EndFunction
    \end{algorithmic}
 \end{algorithm}

\subsubsection{\acl{IFFQ}}
Prevents cache conflicts through spatial separation using a look-ahead mechanism shown in \cref{alg:iffq}. The producer checks if a slot $H$ positions ahead (4 cache lines ahead) is empty before proceeding in line 4, ensuring it works far ahead of the consumer. This check happens only once every $H$ items when \texttt{write} reaches \texttt{limit} in line 2. The consumer delays clearing slots through the function \texttt{iffq\_dequeue\_publish} seen in lines 23-28, maintaining separation between producer and consumer regions. With $2H$ permanently unused slots as a buffer zone, producer and consumer operate on different cache lines, reducing cache misses even more. \cite{MaffioneCacheAware}

\begin{algorithm}[!ht]
   \centering
   \captionsetup{justification=centering}
   \caption{\acl{IFFQ} Operations \cite{MaffioneCacheAware}}
   \label{alg:iffq}
   \scriptsize
   \begin{algorithmic}[1]
       \Function{iffq\_enqueue}{$q$, $e$}
           \If{$q.write = q.limit$} \Comment{Check limit}
               \State $next\_limit \gets q.limit + H$
               \If{$q.slots[next\_limit \land q.mask] \neq \texttt{NULL\_ELEM}$}
                   \State \Return $-1$ \Comment{No space}
               \EndIf
               \State $q.limit \gets next\_limit$ \Comment{Free partition}
           \EndIf
           \State $q.slots[q.write \land q.mask] \gets e$
           \State $q.write \gets q.write + 1$
           \State \Return $0$
       \EndFunction
       
       \State
       
       \Function{iffq\_dequeue\_local}{$q$}
           \State $e \gets q.slots[q.read \land q.mask]$
           \If{$e = \texttt{NULL\_ELEM}$}
               \State \Return \texttt{NULL\_ELEM}
           \EndIf
           \State $q.read \gets q.read + 1$
           \State \Return $e$
       \EndFunction
       
       \State
       
       \Function{iffq\_dequeue\_publish}{$q$}
           \While{$q.clear \neq$ next\_clear$(q.read)$}
               \State $q.slots[q.clear \land q.mask] \gets \texttt{NULL\_ELEM}$
               \State $q.clear \gets q.clear + 1$
           \EndWhile
       \EndFunction
   \end{algorithmic}
\end{algorithm}

\subsubsection{\acl{BIFFQ}}
Addresses \acsp{IFFQ} weakness when the queue is nearly empty by adding producer-side buffering, as shown in \cref{alg:biffq}. Items first accumulate in a thread-local buffer seen in line 10, then the function \texttt{biffq\_enqueue\_publish} beginning at line 15 writes them to the queue in a rapid burst in lines 15 - 18. Also like in \ac{BLQ} the application using this queue must call this function explicitly to avoid deadlocks. This behavior creates an intended race condition, which is beneficial if all writes complete before the consumer notices. The cache line stays with the producer to avoid ping-pong effects. The consumer side remains unchanged from \ac{IFFQ}. While theoretical worst-case behavior is similar to \ac{IFFQ}, practical measurements show significant improvement when the queue operates near empty, making \ac{BIFFQ} effective across all operating conditions. \cite{MaffioneCacheAware}

\begin{algorithm}[!ht]
   \centering
   \captionsetup{justification=centering}
   \caption{\acl{BIFFQ} Operations \cite{MaffioneCacheAware}}
   \label{alg:biffq}
   \scriptsize
   \begin{algorithmic}[1]
       \Function{biffq\_wspace}{$q$, $needed$}
           \State $space \gets q.limit - q.write$
           \If{$space < needed$}
               \State \Return $space$ \Comment{Force limit update}
           \EndIf
           \State \Return $space$
       \EndFunction
       
       \State
       
       \Function{biffq\_enqueue\_local}{$q$, $e$}
           \State $q.buf[q.buffered] \gets e$ \Comment{Store in buffer}
           \State $q.buffered \gets q.buffered + 1$
       \EndFunction
       
       \State
       
       \Function{biffq\_enqueue\_publish}{$q$}
           \For{$i \gets 0$ \textbf{to} $q.buffered - 1$}
               \State $q.slots[q.write \land q.mask] \gets q.buf[i]$ \Comment{Fast burst}
               \State $q.write \gets q.write + 1$
           \EndFor
           \State $q.buffered \gets 0$
           \State $q.limit \gets q.write + H$ \Comment{Update limit}
       \EndFunction
   \end{algorithmic}
\end{algorithm}

\subsubsection{B-Queue}
Addresses the deadlock issues inherent in batching approaches through a self-adaptive backtracking mechanism that dynamically adjusts to production rates shown in \cref{alg:bqueue}. The producer maintains local \texttt{head} and \texttt{batch\_head} pointers, probing \texttt{BATCH\_SIZE} positions ahead when needed in line 3. The consumer's adaptive backtracking algorithm (lines 27 - 41) maintains a \texttt{batch\_history} variable that records successful batch sizes from previous operations. When searching for data, it starts from this historical value rather than always beginning at \texttt{BATCH\_SIZE}, significantly reducing latency when the producer operates slowly. If in line 29 - 31 the recorded size is below \texttt{BATCH\_MAX}, the algorithm optimistically increments \texttt{batch\_size} by \texttt{INCREMENT} (typically one cache line) to probe for higher throughput when the producer accelerates. The binary search then proceeds from this adaptive starting point, halving the batch size until finding available data or reaching zero. In the dequeue function, the consumer uses this computed value to update \texttt{batch\_tail} in line 15. This eliminates the need for manual parameter adjustment or manual calling of a publish function while maintaining cache line separation and preventing deadlocks. \cite{Wang2013BQueue}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{B-Queue with Self-Adaptive Backtracking\cite{Wang2013BQueue}}
    \label{alg:bqueue}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{bqueue\_enqueue}{$q$, $e$}
            \If{$q.head = q.batch\_head$} \Comment{No empty slots}
                \If{$q.buffer[(q.head + \text{BATCH\_SIZE}) \bmod q.size] \neq \texttt{NULL}$}
                    \State \Return $-1$ \Comment{Queue full}
                \EndIf
                \State $q.batch\_head \gets q.head + \text{BATCH\_SIZE}$
            \EndIf
            \State $q.buffer[q.head \bmod q.size] \gets e$
            \State $q.head \gets q.head + 1$
            \State \Return $0$
        \EndFunction
        
        \State
        
        \Function{bqueue\_dequeue}{$q$}
            \If{$q.tail = q.batch\_tail$} \Comment{No filled slots}
                \State $batch\_tail \gets$ \Call{adaptive\_backtrack}{$q$}
                \If{$batch\_tail = -1$}
                    \State \Return \texttt{NULL}
                \EndIf
                \State $q.batch\_tail \gets batch\_tail$
            \EndIf
            \State $e \gets q.buffer[q.tail \bmod q.size]$
            \State $q.buffer[q.tail \bmod q.size] \gets \texttt{NULL}$
            \State $q.tail \gets q.tail + 1$
            \State \Return $e$
        \EndFunction
        
        \State
        
        \Function{adaptive\_backtrack}{$q$}
            \State $batch\_size \gets q.batch\_history$ \Comment{Start from historical value}
            \If{$batch\_size < \text{BATCH\_MAX}$}
                \State $batch\_size \gets batch\_size + \text{INCREMENT}$ \Comment{Try larger batch}
            \EndIf
            \While{$batch\_size > 0$}
                \State $batch\_tail \gets q.tail + batch\_size$
                \If{$q.buffer[(batch\_tail - 1) \bmod q.size] \neq \texttt{NULL}$}
                    \State $q.batch\_history \gets batch\_size$ \Comment{Remember successful size}
                    \State \Return $batch\_tail$
                \EndIf
                \State $batch\_size \gets batch\_size / 2$ \Comment{Binary search}
            \EndWhile
            \State \Return $-1$
        \EndFunction
    \end{algorithmic}
\end{algorithm}




\subsubsection{\acl{dSPSC}}
A dynamically space allocating queue using a linked list with node caching to reduce memory allocation overhead, as shown in \cref{alg:dspsc-detailed}. Unlike bounded circular buffers like the Lamport Queue, dSPSC dynamically allocates nodes as needed, making it suitable for scenarios where queue size cannot be predetermined. The implementation maintains a dummy head node \texttt{head} to ensure producer and consumer always operate on different nodes to prevent cache line conflicts. The \texttt{SPSC\_Buffer} (line 4, which is just a Lamport Queue) serves as a node cache to recycle deallocated nodes to minimize malloc or free calls. When pushing, the producer first checks the cache for a recycled node in line 8, falling back to malloc only when the cache is empty in line 11. After setting the data and next pointer, a memory barrier ensures correct ordering before linking the new node into the list in lines 20-22. The consumer checks for available data by testing if the dummy head points to a data node in line 28. Upon a successful pop, the consumer advances the head pointer so the data node becomes the new dummy and then attempts to cache the old dummy for reuse in lines 30-33. While node caching improves performance, reading and referencing the pointer so often causes memory accesses spread over multiple cache lines. As shown earlier this leads to cache misses. \cite{torquati2010singleproducersingleconsumerqueuessharedcache}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{Dynamic List-based SPSC Queue (dSPSC) Operations \cite{torquati2010singleproducersingleconsumerqueuessharedcache}}
    \label{alg:dspsc-detailed}
    \scriptsize
    \begin{algorithmic}[1]
        \State \textbf{struct} Node \{ void* data; Node* next; \}
        \State Node* head; \Comment{Points to dummy node}
        \State Node* tail; \Comment{Points to last data node}
        \State SPSC\_Buffer cache; \Comment{Bounded cache for node recycling}
        
        \State
        
        \Function{allocnode}{}
            \State Node* $n \gets$ NULL
            \If{cache.pop(\&$n$)} \Comment{Try cache first}
                \State \Return $n$
            \EndIf
            \State $n \gets$ (Node*)malloc(sizeof(Node))
            \State \Return $n$
        \EndFunction
        
        \State
        
        \Function{push}{void* data}
            \State Node* $n \gets$ allocnode() \Comment{Get node from cache or malloc}
            \State $n$->data $\gets$ data
            \State $n$->next $\gets$ NULL
            \State WMB() \Comment{Write Memory Barrier}
            \State tail->next $\gets$ $n$ \Comment{Link new node}
            \State tail $\gets$ $n$ \Comment{Update tail pointer}
            \State \Return true
        \EndFunction
        
        \State
        
        \Function{pop}{void** data}
            \If{head->next $\neq$ NULL} \Comment{Check if data available}
                \State Node* $n \gets$ head \Comment{Save current dummy}
                \State *data $\gets$ (head->next)->data \Comment{Extract data}
                \State head $\gets$ head->next \Comment{Advance to next node}
                \If{!cache.push($n$)} \Comment{Try to recycle old dummy}
                    \State free($n$) \Comment{Free if cache full}
                \EndIf
                \State \Return true
            \EndIf
            \State \Return false \Comment{Queue empty}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{\acl{uSPSC}}
An unbounded queue that links multiple Lamport Queues to combine the cache efficiency of Lamports circular buffer queues with unlimited capacity, as shown in \cref{alg:uspsc}. Unlike dSPSC which uses scattered linked list nodes, uSPSC maintains spatial locality by keeping data in contiguous circular buffers while only linking the buffers themselves. The implementation uses two pointers \texttt{buf\_w} pointing to the producer's current write buffer and \texttt{buf\_r} pointing to the consumer's current read buffer. When pushing, the producer checks if the current buffer is full in line 2, and if so, requests a new buffer from the pool via \texttt{next\_w()} in line 3 before writing the data to \texttt{buf\_w}. The consumer first checks if its current buffer is empty in line 10. If empty, it determines whether the queue is truly empty by comparing read and write buffer pointers in line 11. If they point to the same buffer, no more data exists. Otherwise, after rechecking emptiness to prevent race conditions in line 14, the consumer obtains the next buffer via \texttt{next\_r()} and releases the empty buffer back to the pool for recycling in lines 15-17. This double-check prevents data loss when the producer writes to the current buffer between the initial emptiness check and the buffer comparison. By reusing entire buffers rather than individual nodes, uSPSC matches bounded \ac{SPSC} queues cache behavior while providing unbounded capacity. \cite{Aldinucci2012EfficientSync}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{\acl{uSPSC} Operations\cite{Aldinucci2012EfficientSync}}
    \label{alg:uspsc}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{uspsc\_push}{$q$, $data$}
            \If{$q.buf\_w.full()$} \Comment{Current buffer full}
                \State $q.buf\_w \gets q.pool.next\_w()$ \Comment{Get new buffer}
            \EndIf
            \State $q.buf\_w.push(data)$
            \State \Return \texttt{true}
        \EndFunction
        
        \State
        
        \Function{uspsc\_pop}{$q$, $data$}
            \If{$q.buf\_r.empty()$}
                \If{$q.buf\_r = q.buf\_w$} \Comment{Same buffer?}
                    \State \Return \texttt{false} \Comment{Queue truly empty}
                \EndIf
                \If{$q.buf\_r.empty()$} \Comment{Recheck after comparison}
                    \State $tmp \gets q.pool.next\_r()$
                    \State $q.pool.release(q.buf\_r)$ \Comment{Recycle buffer}
                    \State $q.buf\_r \gets tmp$
                \EndIf
            \EndIf
            \State \Return $q.buf\_r.pop(data)$
        \EndFunction
    \end{algorithmic}
 \end{algorithm}

\subsubsection{\acl{mSPSC}}
Reduces the ping-pong effect in Lamport's circular buffer by batching multiple elements before insertion, as shown in \cref{alg:mspsc}. Instead of writing elements one by one directly to the shared buffer, \ac{mSPSC} accumulates items in a thread-local array \texttt{batch}. The producer stores incoming data in the batch array in lines 2-3, and when the batch reaches \texttt{BATCH\_SIZE} in line 4, the producer calls \texttt{multipush} to insert all elements at once in line 5. The \texttt{multipush} function first calculates the final write position in line 11 and checks if sufficient space exists in line 12. As seen in lines 15-17 elements are written in reverse order, starting from the furthest position and working backwards. This backward insertion creates distance between the write pointer and where the consumer is reading, ensuring they operate on different cache lines. A write memory barrier in line 18 ensures all batch writes are visible before updating the write pointer in line 19. The batch counter resets in line 20, preparing for the next batch. The \texttt{flush} function in lines 24-29 allows forcing partial batch writes when needed. While adding an extra copy per element from batch to buffer, the improved cache behavior from reduced traffic from the coherence protocol compensates for this overhead. \cite{torquati2010singleproducersingleconsumerqueuessharedcache}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{\acl{mSPSC} Operations\cite{torquati2010singleproducersingleconsumerqueuessharedcache}}
    \label{alg:mspsc}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{mspsc\_push}{$q$, $data$}
            \State $q.batch[q.count] \gets data$
            \State $q.count \gets q.count + 1$
            \If{$q.count = $ \texttt{BATCH\_SIZE}}
                \State \Return \Call{multipush}{$q$, $q.batch$, $q.count$}
            \EndIf
            \State \Return \texttt{true}
        \EndFunction
        
        \State
        
        \Function{multipush}{$q$, $batch$, $len$}
            \State $last \gets q.write + len - 1$ \Comment{Calculate end position}
            \If{$q.slots[last \bmod q.size] \neq$ \texttt{NULL}}
                \State \Return \texttt{false} \Comment{Not enough space}
            \EndIf
            \For{$i \gets len - 1$ \textbf{downto} $0$} \Comment{Reverse order}
                \State $q.slots[(q.write + i) \bmod q.size] \gets batch[i]$
            \EndFor
            \State \textbf{WMB}() \Comment{Ensure all writes visible}
            \State $q.write \gets (last + 1) \bmod q.size$
            \State $q.count \gets 0$ \Comment{Reset batch counter}
            \State \Return \texttt{true}
        \EndFunction
        
        \State
        
        \Function{flush}{$q$}
            \If{$q.count > 0$}
                \State \Return \Call{multipush}{$q$, $q.batch$, $q.count$}
            \EndIf
            \State \Return \texttt{true}
        \EndFunction
    \end{algorithmic}
 \end{algorithm}

\subsubsection{Jayantis \ac{SPSC} Queue}

\subsection{\acf{MPSC}}\label{subsec:multiple-producer-and-single-consumer}
This is a bit more complex to implement than the \ac{SPSC} case. Multiple producers can enqueue items at the same time while a single consumer dequeues items. Also here multiple approaches are available from different papers. These are the algorithms that found for this case: 
\begin{itemize}
   \item Jayanti Queue: Distributes the global queue across n local queues per producer. Producers obtain timestamps via atomic counter reads, insert [element, timestamp] pairs into their local queues, then propagate front elements up a binary tree of minimum values (propagating means updating values level by level from leaves toward the root). The consumer reads the root to find the earliest timestamp, dequeues from that local queue, and propagates changes back up the tree. Internal nodes maintain minimum timestamps of their subtrees, enabling O(log n) operations using \ac{LL/SC} (Since the hardware used for this work was accomplished on x86, \ac{LL/SC} was substituted with \ac{CAS} linked with version numbers to avoid ABA). \cite{JayantiLog}
   \item Drescher Queue: Uses a linked list with dummy node recycling to achieve wait-freedom. Producers atomically swap the tail pointer via \ac{FAS} to append nodes, then link the previous tail to the new node. The consumer advances the head pointer and recycles the dummy node by re-enqueueing it when encountered, ensuring the queue never becomes empty. Instead helping atomic pointer swaps are used. \cite{Drescher2015GuardedSections}
   \item Jiffy Queue: Maintains elements in a linked list of fixed-size arrays to minimize memory usage, but still have good execution times. Producers use \ac{FAA} to atomically claim indices, allocate new buffers via \ac{CAS} if their index exceeds current capacity, then traverse to their target buffer and write elements. When the consumer finds an empty head cell, it scans forward for completed entries, marking them as handled and folding away fully consumed buffers. The second enqueuer in each buffer preemptively allocates the next segment to reduce contention. \cite{jiffy}
   \item DQueue: Buffers enqueue requests in thread-local arrays before batch writing to shared segments. Producers obtain indices via \ac{FAA}, accumulate requests locally, then flush to shared memory when buffers fill, exploiting spatial locality. The consumer helps suspended producers by scanning all local buffers when finding empty cells. Key optimizations include eliminating \ac{CAS} operations to avoid write buffer drains and helping all producers indiscriminately. \cite{WangCacheCoherent}
\end{itemize}

\subsection{\acf{SPMC}}\label{subsec:single-producer-and-multiple-consumer}
This case is trickier to implement, since multiple consumers have to be synchronized to not get any double read or loosing an item. Multiple producers was simpler, since making producers write specific data each is not so hard. In this case every consumer has to be synchronized so that when one item is consumed by a consumer, it cannot be consumed again from another. That is most probably also the reason only one algorithm for this contention category was found. No benchmark will be performed for this category, since there is no other algorithm to compete with in this category. The algorithm is the following:
\begin{itemize}
   \item David Queue: Uses a 2D array ITEMS where each cell is a Swap object, a 1D array HEAD where each element is an \ac{FAA} object, and a shared ROW register. The producer writes to consecutive cells in the current row of ITEMS and when the producer detects it gets overtaken by a consumer (reads a value indicating the cell was read), it jumps to the next row. Consumers read the active row from ROW, use \ac{FAA} on HEAD[row] to get a unique index, then swap the value, indicating the cell was read, into ITEMS[row, index] to retrieve the element. The algorithm achieves O(1) time complexity with 3-bounded wait-free operations. Row migration ensures linearizability by preventing consumers from accessing future write locations. \cite{Mateíspmc}
\end{itemize}

\subsection{\acf{MPMC}}\label{subsec:multiple-producer-and-multiple-consumer}
Finally a look into the \ac{MPMC} case can be made. Synchronizing \ac{MPMC} is a bit simpler then synchronizing \ac{SPMC}, because here each consumer consumes the items of one other producer instead of sharing the same pool of data that has to be consumed. These are the algorithms that were found:
\begin{itemize}
   \item  Kogan and Petranks queue: Uses a priority-based helping mechanism built on the Michael-Scott lock-free queue. Threads obtain phase numbers by calculating the maximum phase across all threads in a shared state array and adding one. Each thread records its operation details (phase, pending flag, operation type, node reference) in the state array at its thread ID index. During execution, threads traverse the state array and help all operations with phase numbers less than or equal to their own. Producers append nodes to the tail using \ac{CAS} then update the pending flag to false in the state array and then advance the tail pointer. Consumers write their ID into the deqTid field of the head node then update their state entry and then advance head. Empty queues are handled by checking if head equals tail with a null next pointer. The algorithm ensures each operation completes within O(n²) steps through systematic helping, where threads can only bypass each other a bounded number of times. Linearization occurs at the \ac{CAS} operations that modify the queue structure. \cite{Kogan2011WaitFreeQueues}
   \item Turn Queue: Uses a circular turn-based consensus mechanism. Producers publish nodes in \enquote*{enqueuers[tid]} array then the next turn is determined by tail's \enquote*{enqTid}, with all threads helping the first non-null request to the right (modulo the array size), guaranteeing completion within \enquote*{MAX\_THREADS} iterations. Consumers use dual arrays (\enquote*{deqself} and \enquote*{deqhelp}) where \enquote*{deqself[tid]} = \enquote*{deqhelp[tid]} opens a request. Threads assign nodes via \ac{CAS} on \enquote*{node.deqTid} and publish results in \enquote*{deqhelp[tid]} to close requests. Empty queues trigger the \enquote*{giveUp()} to rollback while preserving concurrent assignments. Linearization occurs at tail and head advances. \cite{RamalheteQueue}
   \item YMC queue: Uses an unbounded array (emulated via linked list of segments) with \ac{FAA}-based indexing and fast-path/slow-path design. Producers obtain slots via \ac{FAA} on tail counter and attempt \ac{CAS} insertion. On failure, the producers publish requests (\enquote*{value}, \enquote*{pending}, \enquote*{cell\_id}) and enter slow path. Helpers use Dijkstra's protocol—after marking cells unusable with top or buttom markers and they check for pending enqueue requests. Helpers traverse a peer ring, advancing only after completing requests, guaranteeing progress within (n-1)² failures. Consumers \ac{FAA} the head counter and call \enquote*{help\_enq} to secure values. Slow-path dequeues announce candidate cells with monotonically increasing indices via \ac{CAS} on request state. Linearizability requires enqueued values have indices < T (tail) and dequeued values have indices < H (head), maintained by \enquote*{advance\_end\_for\_linearizability} calls. Failed operations advance the helper ring until all threads become helpers ensuring all threads finish. \cite{FastFetchAndAddWaitFreeQueue}
   \item Feldman-Dechev Queue: Uses a ring buffer with sequence number distribution for contention management. Producers obtain a unique position via \ac{FAA} on the tail counter, receiving a sequence ID (\enquote*{seqid}) that determines their slot (\enquote*{seqid} modulo the capacity). They attempt to replace an \enquote*{EmptyNode} (containing only a \enquote*{seqid}) with a \enquote*{ValueNode} (containing both \enquote*{seqid} and data) via \ac{CAS}, retrying while temporarily pausing and rechecking if the slot's \enquote*{seqid} indicates a delayed operation. Consumers similarly use \ac{FAA} on the head counter and replace \enquote*{ValueNodes} with \enquote*{EmptyNodes} containing \enquote*{seqid} + \enquote*{capacity}. Both operations employ bitmarking to handle out-of-order completions meaning when encountering a delayed element, threads mark it with a delay bit to signal correction is needed. After exceeding \enquote*{MAX\_FAILS} attempts, operations switch to a slow path using an announcement table where threads publish operation descriptors. All threads periodically check this table (every \enquote*{CHECK\_DELAY} operations) and help announced operations, guaranteeing completion within \enquote*{MAX\_FAILS} + \enquote*{NUM\_THREADS²} steps. The sequence numbers additionally prevent ABA problems and enable progress despite random delays. \cite{FeldmanDechev2015WaitFreeRingBuffer,FeldmanDechevV2,FeldmanDechevV3}
   \item wCQ: Uses a circular array with fast and slow paths. Producers claim slots via \ac{FAA} on a tail counter, then attempt \ac{CAS} to insert if the slot's cycle number indicates availability. After exceeding a patience threshold, producers record their operation in per-thread descriptors and enter a slow path where helpers execute the same operation on behalf of a stuck thread. The slow path replaces uses a modified non-atomic \ac{FAA}, which is a distributed protocol that coordinates helpers through \enquote*{INC} (increment pending) and \enquote*{FIN} (finalized) bits in thread-local counters, ensuring all helpers converge to the same counter value and exactly one successfully increments the global counter. Consumers similarly use \ac{FAA} on the head counter and mark consumed entries with a special bottom value. Both operations use a two-step protocol meaning enqueuers initially insert with \enquote*{Enq=false}, then set \enquote*{Enq=true} after finalizing their request, allowing concurrent dequeuers to help finalize pending operations. The queue maintains 2n slots for n threads with note fields storing cycle numbers to prevent helpers from incorrectly modifying entries from previous cycles. \cite{wCQWaitFreeQueue}
   \item Verma Queue: Uses an helper that runs on a dedicated core and mediates all queue operations through a shared state array. Producers submit enqueue requests by writing [operation, element]tuples to their designated position in the state array indexed by a thread ID, then spin until the \enquote*{isCompleted} flag is set. The helper continuously traverses the state array in round-robin (a traversal algorithm not deeper explained in this work) order, processing pending requests by appending nodes to the tail of an underlying linked list for enqueues or removing from the head for dequeues. The traversal time is bounded (at most n iterations for n threads) so that every thread finishes in a bounded time, while contention is eliminated by serializing all queue changes through the single helper thread. The helper thread includes volatile variables to ensure cache coherence and padding to avoid false sharing between state array entries. \cite{Verma2013Scalable}
\end{itemize}
