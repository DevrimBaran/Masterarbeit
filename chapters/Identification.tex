\chapter{Analysing existing Wait-Free Data Structures and Algorithms}\label{ch:choosing-the-optimal-wait-free-data-structure}
After finding and identifying existing wait-free data structures and algorithms, they need to be analysed as said in \cref{sec:objective} to meet the primary objective of this work. That is exactly what this chapter will be about.

\section{Optimal Wait-Free Data Structure}\label{sec:optimal-wait-free-data-structure}
Before analysing the algorithms, first it needs to be set what data structure to use for the implementation of wait-free synchronisation for \ac{IPC}. M. Herlihy showed that every sequential data structure can be made wait-free \cite{herlihy1991wait}. So it is important to choose the optimal data structure for our use. Considering that the reason for this work is to optimise modern manufacturing and automation, some form of correct data flow order is also necessary for correct workflow, for instance in a modern manufacturing line or more critically in a driverless car. Hence, an already natural fit like \ac{FIFO} queues. Natural because in such queues a producer process can enqueue messages and the consumer process can dequeue messages sequentially. This models real-world data flows (sensor readings, commands, network packets), which are inherently sequential. Consequently, with such queues the order of the data flow is preserved without the need for implementing additional functionalities. In contrast, data structures like stacks, sets, or maps do not maintain this kind of arrival order and moreover add semantics like \ac{LIFO} order or key-value pairs, which are in most cases not desired or even unnecessary. This would bring in the need for additional functions to just get rid of undesired side effects. Furthermore, in a queue only two operations exist: an enqueue and a dequeue operation. All the other data structures introduce more operations and therefore more complexity and therefore more performance overhead. The fewer operations exist, the less complex the implementation will be. Because of these advantages and also because of the fact that in most publications in the wait-free domain queues are being used, limiting this thesis to queues only is reasonable. \cite{jiffy}

\section{Wait-Free Algorithms}\label{sec:wait-free-alg}
With the appropriate data structure established, an important consideration is the selection of suitable algorithms. In \cref{ch:methodology} 4 different contention categories are defined. Which kind of algorithm is going to be used will be decided contention-based. Since all of them have different complexity in runtime, it is important to choose the right contention category for the right use case to save resources and have faster execution times to meet the timing constraints of \ac{HRTS}. In modern manufacturing and automation, devices are used which can run multiple applications on a single device. This could mean that every application running on one device could be a producer and a consumer to each other (\ac{MPMC}) and also maybe some single application of all applications running on one device produces data for just a single other consuming application (\ac{SPSC}). And maybe some single application is a producer for multiple consuming applications (\ac{SPMC}) and multiple applications are producers of a single consuming application (\ac{MPSC}). So it can be that all cases can occur in just one device. This means that all the different cases of contention have to be considered. In the following, the different cases and their algorithms will be discussed. Moreover, they will be implemented and their performance tested via a benchmark (how fast an algorithm can produce and consume items concurrently). Subsequently, from each category the best algorithm will be chosen and their performance will be compared with each other to identify if 4 different categories are necessary. The reason for that is that, for instance, the best-performed \ac{MPMC} algorithm could outperform all other algorithms even for their contention category, since an \ac{MPMC} approach can cover all contention cases. The goal with this approach is to have as little overhead as possible, since an algorithm explicitly implemented for an \ac{MPMC} use case could have extreme overhead for an \ac{SPSC} case. The implementation will be discussed in \cref{ch:implementation} and the results of the performance readings will be discussed in \cref{ch:results}. The following subsections will give an overview of the different contention categories and their algorithms found. The subsections will also shortly describe how the enqueue and dequeue in these algorithms work. Given that all algorithms found are about inter-thread communication and not \ac{IPC}, the following explanation of the specific algorithms will include the terminology of threads to be precise about the papers. In \cref{ch:implementation} it will be elaborated how these thread-based algorithms are adapted to \ac{IPC} in Rust. Other minor Rust-specific deviations from the following algorithms (different types required by Rust's safety model, additional memory fences, etc.) can be seen in the GitHub repository accompanying this thesis \cite{githubMA}. These are not detailed here as such explanations would provide limited value to the topic of this work, and whilst a comprehensive analysis would be relevant, it would require more extensive exploration than is feasible within the scope of this work.

\subsection{\acf{SPSC}}\label{subsec:single-producer-and-single-consumer}
This is the most simple form of \ac{IPC}. In \ac{SPSC} there is nearly no contention from other processes, because only one producer and one consumer is working. The only contention is between the consumer and producer. This leads to the producer and consumer finishing in a bounded number of steps without special synchronisation techniques like helping or atomic primitives. The only concern is that the data the consumer reads is consistent. Different approaches were tested in different papers, that will be seen here. Since \ac{BLQ}, \ac{LLQ}, \ac{BIFFQ} and \ac{IFFQ} are from the same paper, explanations and variables are shared between these algorithms to avoid redundancy:

\subsubsection{Lamport's Circular Buffer Queue}\label{subsubsec:lamport-circular-buffer-queue}
Uses a circular array with two shared indices for synchronisation, based on the algorithm originally proposed by Leslie Lamport in 1983 \cite{Lamport1983SPSCCircularBuffer} and shown here in \cref{alg:lamport-queue} following Maffione et al.'s version \cite{MaffioneCacheAware}. The producer first checks if the queue is full (reached capacity N) in line 2, which requires reading the consumer's \texttt{read} index. If the queue is not full, it writes the input data to the slot at position \texttt{write \& mask} in line 5, where the bitwise AND operation wraps the index around when it reaches the array end. The producer then increments \texttt{write} to signal that the written data is available in line 7. The consumer mirrors this behaviour by checking if the queue is empty in line 12, which requires reading the producer's \texttt{write} index. If the queue is not empty, the consumer reads data from the slot at position \texttt{read \& mask} in line 16, using the same modulo arithmetic through bitwise AND, and incrementing its \texttt{read} index to signal that the slot is available in line 17. This wraparound behaviour creates the circular buffer structure, allowing the fixed-size array to be reused continuously. It can be observed that consumer and producer just carry out a finite number of operations without waiting for any condition from the other process, which leads to them finishing in a finite number of steps. Unfortunately, each operation requires accessing both shared indices plus the data slot, causing up to three cache misses per item when the queue moves between nearly empty and nearly full states. According to U. Drepper \cite{drepper2007every}, cache misses occur when different processes access variables that reside on the same cache line. A cache miss is when the requested data is not in the local CPU core's cache and must be fetched from another core's cache or the main memory, with the cache coherence protocol ensuring memory consistency across all cores. Drepper showed that performance can degrade by 390\%, 734\%, and 1,147\% for 2, 3, and 4 threads respectively. This happens because cache lines, the 64-byte blocks (on x86 architectures) that move between CPU caches, ping-pong between the producer's and consumer's cores as they take turns accessing the same memory locations.

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{Lamport's Queue \cite{MaffioneCacheAware}}
    \label{alg:lamport-queue}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{lq\_enqueue}{$q$, $e$}
            \If{$q.write - q.read = N$} \Comment{Check if full}
                \State \Return $-1$ \Comment{No space}
            \EndIf
            \State $q.slots[q.write \land q.mask] \gets e$
            \State \texttt{store\_release\_barrier()}
            \State $q.write \gets q.write + 1$
            \State \Return $0$
        \EndFunction
        
        \State
        
        \Function{lq\_dequeue}{$q$}
            \If{$q.read = q.write$} \Comment{Check if empty}
                \State \Return \texttt{NULL\_ELEM} \Comment{Queue empty}
            \EndIf
            \State \texttt{load\_acquire\_barrier()}
            \State $e \gets q.slots[q.read \land q.mask]$
            \State $q.read \gets q.read + 1$
            \State \Return $e$
        \EndFunction
    \end{algorithmic}
    \cite{MaffioneCacheAware}
\end{algorithm}

\subsubsection{\acf{LLQ}}
Reduces the described cache misses by postponing index reads until necessary, as shown in \cref{alg:llq}. Additionally to Lamport's original enqueue function, the producer maintains a local \texttt{read\_shadow} copy and only updates it when running out of known free slots in lines 2 to 6. Similarly, the consumer uses \texttt{write\_shadow} to avoid repeatedly checking for new items. Moreover, \ac{LLQ} keeps $K$ slots (where $K$ is slots per cache line) permanently empty, preventing producer and consumer from touching the same cache line when the queue is full. This works well when one thread is faster than the other, reducing worst-case misses from 3 to about 2 per item. This queue only adds a bounded number of additional reads on top of the Lamport queue, still making the design wait-free. \cite{MaffioneCacheAware}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{\ac{LLQ} Operations \cite{MaffioneCacheAware}}
    \label{alg:llq}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{llq\_enqueue}{$q$, $e$}
            \If{$q.write - q.read\_shadow = N - K$} \Comment{Lazy load check}
                \State $q.read\_shadow \gets q.read$ \Comment{Update shadow}
                \If{$q.write - q.read\_shadow = N - K$}
                    \State \Return $-1$ \Comment{No space}
                \EndIf
            \EndIf
            \State $q.slots[q.write \land q.mask] \gets e$
            \State \texttt{store\_release\_barrier()}
            \State $q.write \gets q.write + 1$
            \State \Return $0$
        \EndFunction
        
        \State
        
        \Function{llq\_dequeue}{$q$}
            \If{$q.read = q.write\_shadow$} \Comment{Lazy load check}
                \State $q.write\_shadow \gets q.write$ \Comment{Update shadow}
                \If{$q.read = q.write\_shadow$}
                    \State \Return \texttt{NULL\_ELEM}
                \EndIf
            \EndIf
            \State \texttt{load\_acquire\_barrier()}
            \State $e \gets q.slots[q.read \land q.mask]$
            \State $q.read \gets q.read + 1$
            \State \Return $e$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{\acf{BLQ}}
Extends \ac{LLQ} with explicit batching to further reduce synchronisation costs, as detailed in \cref{alg:blq}. The producer accumulates items using private \texttt{write\_priv} in line 11, filling slots without updating the shared \texttt{write} index. Only the function \texttt{blq\_enqueue\_publish} in lines 15 to 18 makes the batch visible by advancing \texttt{write} in line 17. The consumer works symmetrically, using \texttt{read\_priv} in line 32 for local progress before updating \texttt{read} in line 40. With typical batch sizes like $B = 32$, synchronisation overhead is amortised across operations, reducing cache misses. However, the application using this queue design must explicitly call the publish functions even with partial batches to avoid unbounded latency, because items remain invisible to the consumer until published. This design particularly benefits applications that naturally process data in batches, such as network packet processing, where batch boundaries are well-defined. The added batching logic on top of Lamport's queue only adds a bounded amount of additional writes, not changing the overall bounded step logic of Lamport's queue. \cite{MaffioneCacheAware}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{\ac{BLQ} Operations \cite{MaffioneCacheAware}}
    \label{alg:blq}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{blq\_enqueue\_space}{$q$, $needed$}
            \State $space \gets N - K - (q.write\_priv - q.read\_shadow)$
            \If{$space < needed$}
                \State $q.read\_shadow \gets q.read$ \Comment{Update shadow}
                \State $space \gets N - K - (q.write\_priv - q.read\_shadow)$
            \EndIf
            \State \Return $space$
        \EndFunction
        
        \State
        
        \Function{blq\_enqueue\_local}{$q$, $e$}
            \State $q.slots[q.write\_priv \land q.mask] \gets e$
            \State $q.write\_priv \gets q.write\_priv + 1$
        \EndFunction
        
        \State
        
        \Function{blq\_enqueue\_publish}{$q$}
            \State \texttt{store\_release\_barrier()}
            \State $q.write \gets q.write\_priv$
        \EndFunction
        
        \State
        
        \Function{blq\_dequeue\_space}{$q$}
            \State $available \gets q.write\_shadow - q.read\_priv$
            \If{$available = 0$}
                \State $q.write\_shadow \gets q.write$ \Comment{Update shadow}
                \State $available \gets q.write\_shadow - q.read\_priv$
            \EndIf
            \State \Return $available$
        \EndFunction
        
        \State
        
        \Function{blq\_dequeue\_local}{$q$}
            \State \texttt{load\_acquire\_barrier()}
            \State $e \gets q.slots[q.read\_priv \land q.mask]$
            \State $q.read\_priv \gets q.read\_priv + 1$
            \State \Return $e$
        \EndFunction
        
        \State
        
        \Function{blq\_dequeue\_publish}{$q$}
            \State $q.read \gets q.read\_priv$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{\acf{FFQ}}
Synchronisation by embedding control information directly within the data slots, eliminating separate shared indices shown in \cref{alg:ffq}. Unlike Lamport's queue which requires checking both \texttt{head} and \texttt{tail} indices, \ac{FFQ}'s producer simply examines if the next slot contains \texttt{NULL} in line 2 before writing. When the slot is empty (\texttt{NULL}), the producer writes the data directly and advances its private \texttt{head} index in lines 5 and 6. Otherwise, the queue is full and the operation returns. The consumer follows a similar pattern by reading from the current slot position in line 11 and then checks in line 12 if data is present (non-\texttt{NULL}). If not NULL, it retrieves the value and writes \texttt{NULL} to mark the slot empty in line 15. Then it advances its private \texttt{tail} index in the line after that. This is how the synchronisation happens and why the producer and consumer end in a finite number of steps. Neither consumer nor producer will end up in a retry loop waiting for another. Additionally, the shared memory access was reduced from three (\texttt{head}, \texttt{tail}, \texttt{buffer}) to just one (\texttt{buffer} slot). Each thread maintains its own private index that never needs synchronisation. The producer tracks where to write next through \texttt{head}, whilst the consumer tracks where to read next through \texttt{tail}. The \texttt{NULL} value serves dual purpose as both an empty indicator and the synchronisation mechanism. Whilst this approach reduces memory barriers and cache misses significantly, it still has the ping-pong effect when the queue has few elements, causing the producer and consumer to operate on the same cache line. It can be seen that \ac{FFQ}'s enqueue and dequeue function do complete in exactly 3 steps without any loops or retry mechanisms, leading to a wait-free trait. \cite{ffq} 

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{\ac{FFQ} Operations \cite{ffq}}
    \label{alg:ffq}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{ffq\_enqueue}{$q$, $data$}
            \If{$q.buffer[q.head] \neq \texttt{NULL}$}
                \State \Return \texttt{EWOULDBLOCK}
            \EndIf
            \State $q.buffer[q.head] \gets data$
            \State $q.head \gets \text{NEXT}(q.head)$
            \State \Return $0$
        \EndFunction
        
        \State
        
        \Function{ffq\_dequeue}{$q$}
            \State $data \gets q.buffer[q.tail]$
            \If{$data = \texttt{NULL}$}
                \State \Return \texttt{EWOULDBLOCK}
            \EndIf
            \State $q.buffer[q.tail] \gets \texttt{NULL}$
            \State $q.tail \gets \text{NEXT}(q.tail)$
            \State \Return $data$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{\acf{IFFQ}}
Prevents cache conflicts of \ac{FFQ} through spatial separation using a look-ahead mechanism shown in \cref{alg:iffq}. The producer checks if a slot $H$ positions ahead (4 cache lines ahead) is empty before proceeding in line 4, ensuring it works far ahead of the consumer. This check happens only once every $H$ items when \texttt{write} reaches \texttt{limit} in line 2. The consumer delays clearing slots through the function \texttt{iffq\_dequeue\_publish} seen in lines 23 to 28, maintaining separation between producer and consumer regions. With $2H$ permanently unused slots as a buffer zone, producer and consumer operate on different cache lines, reducing cache misses even more. Wait-freedom is preserved because the look-ahead check adds only one bounded operation every $H$ items on top of \ac{FFQ}, and the delayed clearing in \texttt{iffq\_dequeue\_publish} executes a loop bounded by algorithm parameters, maintaining the original bounded step guarantee of \ac{FFQ}.\cite{MaffioneCacheAware}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{\ac{IFFQ} Operations \cite{MaffioneCacheAware}}
    \label{alg:iffq}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{iffq\_enqueue}{$q$, $e$}
            \If{$q.write = q.limit$} \Comment{Check limit}
                \State $next\_limit \gets q.limit + H$
                \If{$q.slots[next\_limit \land q.mask] \neq \texttt{NULL\_ELEM}$}
                    \State \Return $-1$ \Comment{No space}
                \EndIf
                \State $q.limit \gets next\_limit$ \Comment{Free partition}
            \EndIf
            \State $q.slots[q.write \land q.mask] \gets e$
            \State $q.write \gets q.write + 1$
            \State \Return $0$
        \EndFunction
        
        \State
        
        \Function{iffq\_dequeue\_local}{$q$}
            \State $e \gets q.slots[q.read \land q.mask]$
            \If{$e = \texttt{NULL\_ELEM}$}
                \State \Return \texttt{NULL\_ELEM}
            \EndIf
            \State $q.read \gets q.read + 1$
            \State \Return $e$
        \EndFunction
        
        \State
        
        \Function{iffq\_dequeue\_publish}{$q$}
            \While{$q.clear \neq$ next\_clear$(q.read)$}
                \State $q.slots[q.clear \land q.mask] \gets \texttt{NULL\_ELEM}$
                \State $q.clear \gets q.clear + 1$
            \EndWhile
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{\acf{BIFFQ}}
Gets rid of \ac{IFFQ}'s weakness when the queue is nearly empty by adding producer-side buffering, as shown in \cref{alg:biffq}. Items first accumulate in a thread-local buffer seen in line 10, then the function \texttt{biffq\_enqueue\_publish} beginning at line 15 writes them to the queue in a rapid burst in lines 15 to 18. Also like in \ac{BLQ}, the application using this queue must call this function explicitly to avoid deadlocks. This behaviour creates an intended race condition, which is beneficial if all writes complete before the consumer notices. The cache line stays with the producer to avoid ping-pong effects. The consumer side remains unchanged from \ac{IFFQ}. Whilst theoretical worst-case behaviour is similar to \ac{IFFQ}, practical measurements show significant improvement when the queue operates near empty, making \ac{BIFFQ} effective across all operating conditions. Like \ac{BLQ}, the buffering mechanism maintains wait-freedom through bounded local operations and a publish loop limited by buffer size on top of \ac{FFQ}'s bounded steps, ensuring every operation completes within a fixed number of steps regardless of the consumer's behaviour. \cite{MaffioneCacheAware}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{\ac{BIFFQ} Operations \cite{MaffioneCacheAware}}
    \label{alg:biffq}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{biffq\_wspace}{$q$, $needed$}
            \State $space \gets q.limit - q.write$
            \If{$space < needed$}
                \State \Return $space$ \Comment{Force limit update}
            \EndIf
            \State \Return $space$
        \EndFunction
        
        \State
        
        \Function{biffq\_enqueue\_local}{$q$, $e$}
            \State $q.buf[q.buffered] \gets e$ \Comment{Store in buffer}
            \State $q.buffered \gets q.buffered + 1$
        \EndFunction
        
        \State
        
        \Function{biffq\_enqueue\_publish}{$q$}
            \For{$i \gets 0$ \textbf{to} $q.buffered - 1$}
                \State $q.slots[q.write \land q.mask] \gets q.buf[i]$ \Comment{Fast burst}
                \State $q.write \gets q.write + 1$
            \EndFor
            \State $q.buffered \gets 0$
            \State $q.limit \gets q.write + H$ \Comment{Update limit}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{B-Queue}
Enhances the performance of batching approaches through a self-adaptive backtracking mechanism that dynamically adjusts to production rates shown in \cref{alg:bqueue}. The producer maintains local \texttt{head} and \texttt{batch\_head} pointers, probing \texttt{BATCH\_SIZE} positions ahead when needed in line 3. The consumer's adaptive backtracking algorithm from lines 27 to 41 maintains a \texttt{batch\_history} variable that records successful batch sizes from previous operations. When searching for data, it starts from this historical value rather than always beginning at \texttt{BATCH\_SIZE}, significantly reducing latency when the producer operates slowly. If in lines 29 to 31 the recorded size is below \texttt{BATCH\_MAX}, the algorithm optimistically increments \texttt{batch\_size} by \texttt{INCREMENT} (typically one cache line) to probe for higher throughput when the producer accelerates. The binary search then proceeds from this adaptive starting point, halving the batch size until finding available data or reaching zero. In the dequeue function, the consumer uses this computed value to update \texttt{batch\_tail} in line 15. This eliminates the need for manual parameter adjustment or manual calling of a publish function whilst maintaining cache line separation and preventing deadlocks. It can be observed that the producer is just executing a constant number of operations. The consumer's backtracking loop is bounded by at most $\log_2(\texttt{BATCH\_SIZE})$ iterations since \texttt{batch\_size} halves each time as seen in line 40, which guarantees that this loop will end in a finite number of steps. \cite{Wang2013BQueue}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{B-Queue with Self-Adaptive Backtracking \cite{Wang2013BQueue}}
    \label{alg:bqueue}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{bqueue\_enqueue}{$q$, $e$}
            \If{$q.head = q.batch\_head$} \Comment{No empty slots}
                \If{$q.buffer[(q.head + \text{BATCH\_SIZE}) \bmod q.size] \neq \texttt{NULL}$}
                    \State \Return $-1$ \Comment{Queue full}
                \EndIf
                \State $q.batch\_head \gets q.head + \text{BATCH\_SIZE}$
            \EndIf
            \State $q.buffer[q.head \bmod q.size] \gets e$
            \State $q.head \gets q.head + 1$
            \State \Return $0$
        \EndFunction
        
        \State
        
        \Function{bqueue\_dequeue}{$q$}
            \If{$q.tail = q.batch\_tail$} \Comment{No filled slots}
                \State $batch\_tail \gets$ \Call{adaptive\_backtrack}{$q$}
                \If{$batch\_tail = -1$}
                    \State \Return \texttt{NULL}
                \EndIf
                \State $q.batch\_tail \gets batch\_tail$
            \EndIf
            \State $e \gets q.buffer[q.tail \bmod q.size]$
            \State $q.buffer[q.tail \bmod q.size] \gets \texttt{NULL}$
            \State $q.tail \gets q.tail + 1$
            \State \Return $e$
        \EndFunction
        
        \State
        
        \Function{adaptive\_backtrack}{$q$}
            \State $batch\_size \gets q.batch\_history$ \Comment{Start from historical value}
            \If{$batch\_size < \text{BATCH\_MAX}$}
                \State $batch\_size \gets batch\_size + \text{INCREMENT}$ \Comment{Try larger batch}
            \EndIf
            \While{$batch\_size > 0$}
                \State $batch\_tail \gets q.tail + batch\_size$
                \If{$q.buffer[(batch\_tail - 1) \bmod q.size] \neq \texttt{NULL}$}
                    \State $q.batch\_history \gets batch\_size$ \Comment{Remember successful size}
                    \State \Return $batch\_tail$
                \EndIf
                \State $batch\_size \gets batch\_size / 2$ \Comment{Binary search}
            \EndWhile
            \State \Return $-1$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{\acf{dSPSC}}
A dynamically space-allocating queue using a linked list with node caching to reduce memory allocation overhead, as shown in \cref{alg:dspsc-detailed}. Unlike bounded circular buffers like the Lamport Queue, dSPSC dynamically allocates nodes as needed, making it suitable for scenarios where queue size cannot be predetermined. The implementation maintains a dummy head node \texttt{head} to ensure producer and consumer always operate on different nodes to prevent cache line conflicts. The \texttt{SPSC\_Buffer} (line 4, which is just a Lamport Queue) serves as a node cache to recycle deallocated nodes to minimise malloc or free calls. When pushing, the producer first checks the cache for a recycled node in line 8, falling back to malloc only when the cache is empty in line 11. After setting the data and next pointer, a memory barrier ensures correct ordering before linking the new node into the list in lines 20 to 22. The consumer checks for available data by testing if the dummy head points to a data node in line 28. Upon a successful pop, the consumer advances the head pointer so the data node becomes the new dummy and then attempts to cache the old dummy for reuse in lines 30 to 33. As one can see, the consumer and also the producer do not have any kind of loops and just carry out a small set of operations, which leads to the wait-freedom of both. The node caching is for improving performance, because reading and referencing the pointer so often causes memory accesses spread over multiple cache lines. As shown earlier, this leads to cache misses. \cite{torquati2010singleproducersingleconsumerqueuessharedcache}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{\ac{dSPSC} Operations \cite{torquati2010singleproducersingleconsumerqueuessharedcache}}
    \label{alg:dspsc-detailed}
    \scriptsize
    \begin{algorithmic}[1]
        \State \textbf{struct} Node \{ void* data; Node* next; \}
        \State Node* head; \Comment{Points to dummy node}
        \State Node* tail; \Comment{Points to last data node}
        \State SPSC\_Buffer cache; \Comment{Bounded cache for node recycling}
        
        \State
        
        \Function{allocnode}{}
            \State Node* $n \gets$ NULL
            \If{cache.pop(\&$n$)} \Comment{Try cache first}
                \State \Return $n$
            \EndIf
            \State $n \gets$ (Node*)malloc(sizeof(Node))
            \State \Return $n$
        \EndFunction
        
        \State
        
        \Function{push}{void* data}
            \State Node* $n \gets$ allocnode() \Comment{Get node from cache or malloc}
            \State $n$->data $\gets$ data
            \State $n$->next $\gets$ NULL
            \State WMB() \Comment{Write Memory Barrier}
            \State tail->next $\gets$ $n$ \Comment{Link new node}
            \State tail $\gets$ $n$ \Comment{Update tail pointer}
            \State \Return true
        \EndFunction
        
        \State
        
        \Function{pop}{void** data}
            \If{head->next $\neq$ NULL} \Comment{Check if data available}
                \State Node* $n \gets$ head \Comment{Save current dummy}
                \State *data $\gets$ (head->next)->data \Comment{Extract data}
                \State head $\gets$ head->next \Comment{Advance to next node}
                \If{!cache.push($n$)} \Comment{Try to recycle old dummy}
                    \State free($n$) \Comment{Free if cache full}
                \EndIf
                \State \Return true
            \EndIf
            \State \Return false \Comment{Queue empty}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{\acf{uSPSC}}
An unbounded queue that links multiple already wait-free Lamport Queues to combine the cache efficiency of Lamport's circular buffer queues with unlimited capacity, as shown in \cref{alg:uspsc}. Unlike \ac{dSPSC} which uses scattered linked list nodes, uSPSC maintains spatial locality by keeping data in contiguous circular buffers whilst only linking the buffers themselves. The implementation uses two pointers: \texttt{buf\_w} pointing to the producer's current write buffer and \texttt{buf\_r} pointing to the consumer's current read buffer. When pushing, the producer checks if the current buffer is full in line 2, and if so, requests a new buffer from the pool via \texttt{next\_w()} in line 3 before writing the data to \texttt{buf\_w}. The consumer first checks if its current buffer is empty in line 10. If empty, it determines whether the queue is truly empty by comparing read and write buffer pointers in line 11. If they point to the same buffer, no more data exists. Otherwise, after rechecking emptiness to prevent race conditions in line 14, the consumer obtains the next buffer via \texttt{next\_r()} and releases the empty buffer back to the pool for recycling in lines 15 to 17. This double-check prevents data loss when the producer writes to the current buffer between the initial emptiness check and the buffer comparison. By reusing entire buffers rather than individual nodes, \ac{uSPSC} matches bounded \ac{SPSC} queue's cache behaviour whilst providing unbounded capacity. The logic built on top of the wait-free Lamport queues as seen does still maintain wait-freedom, since the synchronisation is happening inside each individual Lamport queue whilst neither the consumer nor producer gets stuck in a loop waiting for each other. \cite{torquati2010singleproducersingleconsumerqueuessharedcache}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{\ac{uSPSC} Operations\cite{torquati2010singleproducersingleconsumerqueuessharedcache}}
    \label{alg:uspsc}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{uspsc\_push}{$q$, $data$}
            \If{$q.buf\_w.full()$} \Comment{Current buffer full}
                \State $q.buf\_w \gets q.pool.next\_w()$ \Comment{Get new buffer}
            \EndIf
            \State $q.buf\_w.push(data)$
            \State \Return \texttt{true}
        \EndFunction
        
        \State
        
        \Function{uspsc\_pop}{$q$, $data$}
            \If{$q.buf\_r.empty()$}
                \If{$q.buf\_r = q.buf\_w$} \Comment{Same buffer?}
                    \State \Return \texttt{false} \Comment{Queue truly empty}
                \EndIf
                \If{$q.buf\_r.empty()$} \Comment{Recheck after comparison}
                    \State $tmp \gets q.pool.next\_r()$
                    \State $q.pool.release(q.buf\_r)$ \Comment{Recycle buffer}
                    \State $q.buf\_r \gets tmp$
                \EndIf
            \EndIf
            \State \Return $q.buf\_r.pop(data)$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{\acf{mSPSC}}
Reduces the ping-pong effect in Lamport's circular buffer by batching multiple elements before insertion, as shown in \cref{alg:mspsc}. Instead of writing elements one by one directly to the shared buffer, \ac{mSPSC} accumulates items in a thread-local array \texttt{batch}. The producer stores incoming data in the batch array in lines 2 and 3, and when the batch reaches \texttt{BATCH\_SIZE} in line 4, the producer calls \texttt{multipush} to insert all elements at once in line 5. The \texttt{multipush} function first calculates the final write position in line 11 and checks if sufficient space exists in line 12. As seen in lines 15 to 17, elements are written in reverse order, starting from the furthest position and working backwards. This backward insertion creates distance between the write pointer and where the consumer is reading, ensuring they operate on different cache lines. A write memory barrier in line 18 ensures all batch writes are visible before updating the write pointer in line 19. The batch counter resets in line 20, preparing for the next batch. The \texttt{flush} function in lines 24 to 29 allows forcing partial batch writes when needed. Whilst adding an extra copy per element from batch to buffer, the improved cache behaviour from reduced traffic from the coherence protocol compensates for this overhead. As seen, this queue just adds extra logic on top of the Lamport queue, making it still wait-free, since \texttt{BATCH\_SIZE} is a constant and the loops in the queue are therefore bounded. \cite{torquati2010singleproducersingleconsumerqueuessharedcache}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{\ac{mSPSC} Operations\cite{torquati2010singleproducersingleconsumerqueuessharedcache}}
    \label{alg:mspsc}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{mspsc\_push}{$q$, $data$}
            \State $q.batch[q.count] \gets data$
            \State $q.count \gets q.count + 1$
            \If{$q.count = $ \texttt{BATCH\_SIZE}}
                \State \Return \Call{multipush}{$q$, $q.batch$, $q.count$}
            \EndIf
            \State \Return \texttt{true}
        \EndFunction
        
        \State
        
        \Function{multipush}{$q$, $batch$, $len$}
            \State $last \gets q.write + len - 1$ \Comment{Calculate end position}
            \If{$q.slots[last \bmod q.size] \neq$ \texttt{NULL}}
                \State \Return \texttt{false} \Comment{Not enough space}
            \EndIf
            \For{$i \gets len - 1$ \textbf{downto} $0$} \Comment{Reverse order}
                \State $q.slots[(q.write + i) \bmod q.size] \gets batch[i]$
            \EndFor
            \State \textbf{WMB}() \Comment{Ensure all writes visible}
            \State $q.write \gets (last + 1) \bmod q.size$
            \State $q.count \gets 0$ \Comment{Reset batch counter}
            \State \Return \texttt{true}
        \EndFunction
        
        \State
        
        \Function{flush}{$q$}
            \If{$q.count > 0$}
                \State \Return \Call{multipush}{$q$, $q.batch$, $q.count$}
            \EndIf
            \State \Return \texttt{true}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{\ac{JPQ} (\ac{SPSC} variant)}\label{subsub:jayanti-spsc-queue}
A queue specifically for composability in larger \ac{MPSC} structures, as shown in \cref{alg:jayanti-spsc}. Unlike traditional \ac{SPSC} queues, this implementation includes \texttt{readFront} operations that enable observation of the queue's head element, crucial for the \ac{MPSC} construction. The queue maintains a linked list where the tail always points to a dummy node. When enqueueing, the producer converts the current dummy node into a data node by writing the value in line 4. Then the producer links a new dummy node in line 5 and updates the tail pointer in line 6. This ensures the consumer never sees a partially constructed node. The \texttt{Help} variable in line 15 stores the dequeued value, allowing concurrent \texttt{readFront\_e} operations to obtain valid data even after the original node is removed. The announcement mechanism prevents use-after-free errors. When the producer calls \texttt{readFront\_e}, it writes the front node pointer to \texttt{Announce} in line 32, signalling the consumer not to immediately free that node. If the consumer encounters an announced node in line 17, it defers the node's deallocation to \texttt{FreeLater} in lines 18 to 20, ensuring the producer can safely read the node's value. The separate \texttt{readFront\_d} operation in lines 41 to 47 is simpler since the consumer knows no concurrent dequeue can occur. This coordination enables wait-free progress whilst supporting the propagation mechanism, the process of pushing each local queue's minimum timestamp up through a binary tree to maintain a global minimum, needed for the logarithmic-time \ac{MPSC} operations as seen in \cref{subsub:jayanti-mpsc-queue}. Wait-freedom is achieved because there are no loops in the enqueue and dequeue functions. \cite{JayantiLog}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{\ac{JPQ} (\ac{SPSC} variant) Operations \cite{JayantiLog}}
    \label{alg:jayanti-spsc}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{enqueue}{$q$, $v$}
            \State $newNode \gets$ new Node() \Comment{Create new dummy node}
            \State $tmp \gets q.Last$ \Comment{Get current dummy tail}
            \State $tmp.val \gets v$ \Comment{Convert dummy to data node}
            \State $tmp.next \gets newNode$ \Comment{Link new dummy}
            \State $q.Last \gets newNode$ \Comment{Update tail pointer}
        \EndFunction
        
        \State
        
        \Function{dequeue}{$q$}
            \State $tmp \gets q.First$ \Comment{Get head node}
            \If{$tmp = q.Last$} \Comment{Only dummy remains?}
                \State \Return $\bot$ \Comment{Queue empty}
            \EndIf
            \State $retval \gets tmp.val$ \Comment{Read value}
            \State $q.Help \gets retval$ \Comment{Help concurrent readFront}
            \State $q.First \gets tmp.next$ \Comment{Remove from queue}
            \If{$tmp = q.Announce$} \Comment{Was announced by readFront?}
                \State $tmp' \gets q.FreeLater$ \Comment{Get old deferred node}
                \State $q.FreeLater \gets tmp$ \Comment{Defer current node}
                \State free($tmp'$) \Comment{Free old deferred node}
            \Else
                \State free($tmp$) \Comment{Free immediately}
            \EndIf
            \State \Return $retval$
        \EndFunction
        
        \State
        
        \Function{readFront\_e}{$q$} \Comment{Called by enqueuer}
            \State $tmp \gets q.First$ \Comment{Read head pointer}
            \If{$tmp = q.Last$} \Comment{Queue empty?}
                \State \Return $\bot$
            \EndIf
            \State $q.Announce \gets tmp$ \Comment{Announce to prevent free}
            \If{$tmp \neq q.First$} \Comment{Head changed (was dequeued)?}
                \State $retval \gets q.Help$ \Comment{Use helped value}
            \Else
                \State $retval \gets tmp.val$ \Comment{Read directly}
            \EndIf
            \State \Return $retval$
        \EndFunction
        
        \State
        
        \Function{readFront\_d}{$q$} \Comment{Called by dequeuer}
            \State $tmp \gets q.First$
            \If{$tmp = q.Last$}
                \State \Return $\bot$
            \EndIf
            \State \Return $tmp.val$ \Comment{Safe - no concurrent dequeue}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{\acf{MPSC}}\label{subsec:multiple-producer-and-single-consumer}
This is a bit more complex to implement than the \ac{SPSC} case. Multiple producers can enqueue items at the same time whilst a single consumer dequeues items. This includes implementing other strategies, such as helping and atomic primitives to maintain wait-freedom and consistency between the producers. There are multiple approaches that are available from different papers to achieve this:

\subsubsection{\acf{JPQ} (\ac{MPSC} variant)}\label{subsub:jayanti-mpsc-queue}
Achieves logarithmic time complexity by distributing the global queue across $n$ local \ac{SPSC} queues implemented like in \cref{subsub:jayanti-spsc-queue} and organised under a binary tree, as shown in \cref{alg:jayanti-mpsc}. Each producer owns a dedicated local queue, eliminating producer-producer contention. When enqueueing, a producer obtains a global timestamp via \ac{LL/SC} on a shared counter in lines 2 and 3, creating a unique ordering even if the \ac{SC} fails, since some other producer must have incremented it. If \ac{LL/SC} is not supported on system architecture, it can be replaced with versioned \ac{CAS}. The producer then inserts a timestamped pair into its local queue in line 4 and propagates this timestamp up the tree in line 5. The tree maintains the invariant that each internal node holds the minimum timestamp of its subtree. The \texttt{propagate} function in lines 18 to 26 walks from leaf to root, calling \texttt{refresh} at each node. The double \texttt{refresh} pattern in lines 22 to 24 ensures correctness. If the first refresh fails, another process updated the node. If the second refresh also fails, that process must have read the updated children values and installed the correct minimum. The \texttt{refresh} function uses \ac{LL/SC} in lines 28 to 33 to atomically update a node with the minimum of its children's timestamps. The consumer reads the root to find the producer with the earliest element in line 9 and then dequeues from that local queue in line 13 and propagates any changes in line 14. This design transforms the $O(n)$ scan of all queues into $O(\log n)$ tree traversals, whilst the space complexity remains $O(n + m)$ where $m$ is the number of queued items. The wait-freedom is achieved by letting the producers work only in their own local queue, avoiding any contention with other producers. The dequeue is just a set of instructions without loops. \cite{JayantiLog}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{\ac{JPQ} (\ac{MPSC} variant) Operations \cite{JayantiLog}}
    \label{alg:jayanti-mpsc}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{enqueue}{$q$, $p$, $v$}
            \State $tok \gets$ LL($q.counter$) \Comment{Read timestamp}
            \State SC($q.counter$, $tok + 1$) \Comment{Try increment}
            \State enqueue2($q.Q[p]$, $(v, (tok, p))$) \Comment{Add timestamp to local queue}
            \State \Call{propagate}{$q$, $q.Q[p]$}
        \EndFunction
        
        \State
        
        \Function{dequeue}{$q$, $p$}
            \State $[t, id] \gets$ read($q.T.root$) \Comment{Get min producer}
            \If{$id = \bot$}
                \State \Return $\bot$
            \EndIf
            \State $ret \gets$ dequeue2($q.Q[id]$)
            \State \Call{propagate}{$q$, $q.Q[id]$}
            \State \Return $ret.val$
        \EndFunction
        
        \State
        
        \Function{propagate}{$q$, $localQueue$}
            \State $currentNode \gets localQueue$
            \Repeat
                \State $currentNode \gets$ parent($currentNode$)
                \If{$\neg$\Call{refresh}{$q$, $currentNode$}} \Comment{First try}
                    \State \Call{refresh}{$q$, $currentNode$} \Comment{Second ensures correctness}
                \EndIf
            \Until{$currentNode = q.T.root$}
        \EndFunction
        
        \State
        
        \Function{refresh}{$q$, $node$}
            \State LL($node$) \Comment{Load-link node}
            \State $stamps \gets$ read timestamps from $node$'s children
            \State $minT \gets$ minimum timestamp from $stamps$
            \State \Return SC($node$, $minT$) \Comment{Store-conditional}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{Drescher Queue}\label{subsubsec:drescher-mpsc-queue}
Uses a linked list with a dummy head node to eliminate producer contention, as shown in \cref{alg:drescher-mpsc}. Unlike traditional \ac{MPSC} queues that require retry loops, producers complete enqueue in exactly three steps. First, the producers clear the item's \texttt{next} pointer in line 6, then it atomically swaps the tail pointer via \texttt{\ac{FAS}} in line 7 and further links the previous tail to the new item in line 8. The \texttt{\ac{FAS}} operation ensures multiple producers can enqueue concurrently without interference. The consumer reads the head and its \texttt{next} pointer in lines 12 and 13 and afterwards advances the head in line 17 if the queue is non-empty. Subsequently, the consumer handles the special case of the dummy node in lines 18 to 24. When the dummy is dequeued, it's immediately re-enqueued in line 19 to maintain the invariant that the queue always contains at least one element to prevent complex empty queue conditions. The wait-freedom is achieved by the single atomic \ac{FAS} for producers. \cite{Drescher2015GuardedSections}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{Drescher's Wait-Free \ac{MPSC} Queue Operations}
    \label{alg:drescher-mpsc}
    \scriptsize
    \begin{algorithmic}[1]
        \State $dummy.next \gets 0$
        \State $head \gets$ \&$dummy$
        \State $tail \gets$ \&$dummy$
        
        \State
        
        \Procedure{Enqueue}{$guard$, $item$}
            \State $item.next \gets 0$ \Comment{Clear next pointer}
            \State $prev \gets$ \texttt{FAS}$(guard.tail, item)$ \Comment{Atomic swap tail}
            \State $prev.next \gets item$ \Comment{Link to new item}
        \EndProcedure
        
        \State
        
        \Function{Dequeue}{$guard$}
            \State $item \gets guard.head$
            \State $next \gets guard.head.next$
            \If{$next = 0$} \Comment{Empty queue?}
                \State \Return $\bot$
            \EndIf
            \State $guard.head \gets next$
            \If{$item = $ \&$dummy$} \Comment{Dequeued dummy?}
                \State \Call{Enqueue}{$guard$, $item$} \Comment{Re-enqueue dummy}
                \If{$guard.head.next = 0$} \Comment{Still empty?}
                    \State \Return $\bot$
                \EndIf
                \State $guard.head \gets guard.head.next$
                \State \Return $next$
            \EndIf
            \State \Return $item$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{Jiffy Queue}\label{subsubsec:jiffy-mpsc-queue}
A queue that uses a linked list of fixed-size arrays (buffers), as shown in function \newline \texttt{ENQUEUE(data)} in \cref{alg:jiffy-enqueue} and in \texttt{DEQUEUE} function in \cref{alg:jiffy-dequeue}. Unlike other linked-list queues that allocate nodes per element, Jiffy amortises allocation overhead by storing multiple elements in each buffer. Producers use \ac{FAA} on a global tail counter to reserve slots in line 2 of \texttt{enqueue}, eliminating producer-producer synchronisation except during buffer allocation. Each buffer contains an array of nodes with data and a 2-bit \texttt{isSet} flag indicating the node's state: \texttt{empty} (uninitialised), \texttt{set} (data written), or \texttt{handled} (already dequeued). When the current buffer fills, producers allocate new buffers and link them via \ac{CAS} in lines 7 and 8. To reduce allocation contention, the producer obtaining the second slot in each buffer proactively allocates the next buffer in lines 21 to 26, ensuring smooth transitions between buffers. The consumer maintains a local head pointer and scans for the first non-handled element in lines 3 to 9 of \texttt{dequeue}. To ensure linearisability when producers stall: if the head element is still \texttt{empty}, the consumer scans forward to find a \texttt{set} element in line 20, then rescans backward in line 24 to ensure no earlier element became \texttt{set} during the scan. This prevents violating FIFO ordering when a slow producer completes after a faster one. The consumer can "fold" the queue by deleting fully-handled buffers in the middle of the list during scans, to not use too much memory even with stalled producers. This achieves wait-free progress guarantees with minimal synchronisation. Producers only need one \ac{FAA} per enqueue, whilst the consumer performs no atomic operations at all, skipping still unfinished enqueues. \cite{jiffy}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{Jiffy \ac{MPSC} Queue Enqueue Operation \cite{jiffy}}
    \label{alg:jiffy-enqueue}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{enqueue}{$data$}
            \State $location \gets$ FAA($tail$, 1) \Comment{Reserve global index}
            \State $tempTail \gets tailOfQueue$
            \While{$location$ is in unallocated buffer} \Comment{Beyond last buffer?}
                \If{$tempTail.next = $ NULL}
                    \State $newArr \gets$ new BufferList()
                    \If{CAS($tempTail.next$, NULL, $newArr$)}
                        \State CAS($tailOfQueue$, $tempTail$, $newArr$)
                    \Else
                        \State delete $newArr$ \Comment{Another thread succeeded}
                    \EndIf
                \EndIf
                \State $tempTail \gets tailOfQueue$ \Comment{Move to new buffer}
            \EndWhile
            \While{$location$ not in $tempTail$'s buffer} \Comment{Location in earlier buffer?}
                \State $tempTail \gets tempTail.prev$ \Comment{Walk backward}
            \EndWhile
            \State $index \gets location - tempTail.startIndex$ \Comment{Buffer-local index}
            \State $tempTail.buffer[index].data \gets data$
            \State $tempTail.buffer[index].isSet \gets$ SET \Comment{Mark as ready}
            \If{$index = 1$ AND $tempTail$ is last buffer} \Comment{Second slot?}
                \State $newArr \gets$ new BufferList() \Comment{Proactive allocation}
                \If{NOT CAS($tempTail.next$, NULL, $newArr$)}
                    \State delete $newArr$
                \EndIf
            \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{Jiffy \ac{MPSC} Queue Dequeue Operation \cite{jiffy}}
    \label{alg:jiffy-dequeue}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{dequeue}{}
            \State $n \gets headOfQueue.buffer[head]$
            \While{$n.isSet = $ HANDLED} \Comment{Skip dequeued items}
                \State $head \gets head + 1$
                \If{end of buffer}
                    \State move to next buffer and delete current
                \EndIf
                \State $n \gets headOfQueue.buffer[head]$
            \EndWhile
            \If{queue is empty}
                \State \Return $\bot$
            \EndIf
            \If{$n.isSet = $ SET} \Comment{Ready to dequeue?}
                \State $data \gets n.data$
                \State $n.isSet \gets$ HANDLED
                \State $head \gets head + 1$
                \State \Return $data$
            \EndIf
            \If{$n.isSet = $ EMPTY} \Comment{Incomplete enqueue?}
                \State $tempN \gets$ Scan(find first SET element)
                \If{no SET element found}
                    \State \Return $\bot$
                \EndIf
                \State Rescan($n$, $tempN$) \Comment{Check for newly set elements}
                \State $data \gets tempN.data$
                \State $tempN.isSet \gets$ HANDLED
                \State \Return $data$
            \EndIf
        \EndFunction
        
        \State
        
        \Function{Scan}{} \Comment{Find first SET element}
            \For{each element from current position}
                \If{element.isSet = SET}
                    \State \Return element
                \EndIf
                \If{entire buffer is HANDLED}
                    \State fold queue (delete buffer)
                \EndIf
            \EndFor
            \State \Return NULL
        \EndFunction
        
        \State
        
        \Function{Rescan}{$start$, $end$} \Comment{Check for ordering violations}
            \For{each element from $start$ to $end$}
                \If{element.isSet = SET}
                    \State $end \gets$ element \Comment{Found earlier SET element}
                    \State restart scan from $start$
                \EndIf
            \EndFor
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{DQueue}
Combines local buffering with a segmented shared queue to minimise synchronisation overhead, as shown in function \texttt{ENQUEUE(data)} in \cref{alg:dqueue-enqueue} and in \texttt{DEQUEUE} function in \cref{alg:dqueue-dequeue}. DQueue reduces contention by having producers accumulate enqueue requests in thread-local ring buffers before writing them to the shared queue in batches. Producers reserve slots using \ac{FAA} on a global tail counter in line 7 of \texttt{enqueue}, storing both the data and the reserved cell index (\texttt{cid}) in their local buffer. Each producer maintains a buffer of \texttt{Request} structures with capacity $L$, using \texttt{local\_head} and \texttt{local\_tail} pointers to track buffer state. When the buffer fills, detected in line 2, the producer calls \texttt{dump\_local\_buffer} to flush all buffered requests. During flushing, producers write values directly to their reserved cells in line 15 without synchronisation, as each cell is exclusively owned by the reserving producer. Producers cache their current segment pointer (\texttt{pseg}) and update it when moving to newer segments in line 18. The \texttt{find\_segment} function traverses the segment list and allocates new segments on-demand using \ac{CAS} in line 29. To maintain wait-freedom when producers stall, the consumer encountering an empty cell that should contain data, checked in line 7 of \texttt{dequeue}, distinguishes between an empty queue in line 8 and a pending enqueue by checking if head equals tail. For pending enqueues, \texttt{help\_enqueue} in line 11 iterates through all producers' local buffers from lines 19 to 31, writing any buffered values to their reserved cells in line 28. The helper skips producers that have already moved past the target segment in line 24, avoiding unnecessary work. This ensures minimal synchronisation with only one \ac{FAA} per enqueue and no atomics for dequeue and improves cache locality via batched writes that reduce false sharing and writes that directly write to known cell locations without searching. The consumer's dequeue operation has its linearisation point at line 14 where it increments the head pointer. \cite{WangCacheCoherent}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{DQueue \ac{MPSC} Queue Enqueue Operation \cite{WangCacheCoherent}}
    \label{alg:dqueue-enqueue}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{enqueue}{$Producer$ $p$, $data$}
            \If{next($p.local\_tail$) = $p.local\_head$}
                \State dump\_local\_buffer($p$) \Comment{Flush when full}
            \EndIf
            \State $tail \gets p.local\_tail$
            \State $p.local\_buffer[tail].val \gets data$
            \State $p.local\_buffer[tail].cid \gets$ FAA($q.tail$, 1) \Comment{Reserve slot}
            \State $p.local\_tail \gets$ next($p.local\_tail$)
        \EndFunction
        \State
        \Function{dump\_local\_buffer}{$Producer$ $p$}
            \While{$p.local\_head \neq p.local\_tail$}
                \State $r \gets p.local\_buffer[p.local\_head]$
                \State $seg \gets$ find\_segment($p.pseg$, $r.cid$)
                \State $seg.cell[r.cid \bmod N] \gets r.val$ \Comment{Write in batch}
                \State $p.local\_head \gets$ next($p.local\_head$)
                \If{$p.pseg \neq seg$}
                    \State $p.pseg \gets seg$ \Comment{Update segment cache}
                \EndIf
            \EndWhile
        \EndFunction
        \State
        \Function{find\_segment}{$Segment$ $*sp$, $int$ $cid$}
            \State $curr \gets sp$
            \For{$i \gets curr\rightarrow id$; $i < cid / N$; $i$++}
                \State $next \gets curr\rightarrow next$
                \If{$next = $ NULL}
                    \State $new \gets$ new\_segment($i + 1$)
                    \If{CAS($curr\rightarrow next$, NULL, $new$)}
                        \State $next \gets new$
                    \Else
                        \State delete $new$ \Comment{Another thread succeeded}
                    \EndIf
                \EndIf
                \State $curr \gets next$
            \EndFor
            \State \Return $curr$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{DQueue \ac{MPSC} Queue Dequeue Operation \cite{WangCacheCoherent}}
    \label{alg:dqueue-dequeue}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{dequeue}{$Consumer$ $c$}
            \State $seg \gets$ find\_segment($c.cseg$, $q.head$)
            \If{$c.cseg \neq seg$}
                \State $c.cseg \gets seg$ \Comment{Update segment cache}
            \EndIf
            \State $cell \gets seg.cell[q.head \bmod N]$
            \If{$cell = \bot$} \Comment{Empty cell?}
                \If{$q.head = q.tail$}
                    \State \Return EMPTY
                \Else
                    \State help\_enqueue() \Comment{Help stalled producers}
                \EndIf
            \EndIf
            \State $q.head \gets q.head + 1$ \Comment{Linearisation point}
            \State \Return $cell$
        \EndFunction
        \State
        \Function{help\_enqueue}{}
            \For{each $Producer$ $p$ in system}
                \For{each $Request$ $r$ in $p.local\_buffer$}
                    \State $pos \gets r.cid$
                    \State $val \gets r.val$
                    \State $seg \gets$ find\_segment($p.pseg$, $pos$)
                    \If{$seg.id > pos/N$}
                        \State \textbf{break} \Comment{Producer moved past}
                    \EndIf
                    \If{$seg.cell[pos \bmod N] = \bot$}
                        \State $seg.cell[pos \bmod N] \gets val$ \Comment{Help write}
                    \EndIf
                \EndFor
            \EndFor
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{\acf{SPMC}}\label{subsec:single-producer-and-multiple-consumer}
This case is trickier to implement, since now multiple reading workers have to be synchronised to read consistently without any unwanted behaviour. Multiple producers was simpler, since making producers write specific data each is not so hard. In this case every consumer has to be synchronised so that when one item is consumed by a consumer, it cannot be consumed again from another and also the ordering has to be kept. That is most probably also the reason why only one algorithm for this contention category was found. The approach to achieve this in a wait-free manner is the following:

\subsubsection{David Queue}\label{subsubsec:david-queue}
Uses a two-dimensional array of Swap objects to handle the race condition where consumers overtake the producer, as shown in \cref{alg:david-queue}. David's queue allows the producer to detect when it has been overtaken and adapt by jumping to a fresh row. The producer maintains two persistent local variables, an \texttt{enq\_row} (current row) and \texttt{tail} (next column to write) variable. During enqueue, the producer swaps the value into \texttt{ITEMS[enq\_row, tail]} in line 10 and checks if the retrieved value is $\top$, indicating a consumer already accessed this cell. If so, the producer jumps to the next row in lines 12 to 15, writing the value to the new row and updating the shared \texttt{ROW} register. This jump mechanism ensures that enqueued values are never lost. Consumers read the active row from \texttt{ROW} in line 22 of \texttt{dequeue}, then increment using Fetch and Increment (Like \ac{FAA}, but incrementing) on \texttt{HEAD[deq\_row]} in line 23 to reserve a unique column index. They swap $\top$ into the reserved cell in line 24, retrieving either the enqueued value or $\bot$ (empty). The use of swap instead of plain registers is important, because it allows the producer to detect consumer interference (by finding $\top$) and consumers to mark cells as processed. Each cell in \texttt{ITEMS} is accessed at most once by an enqueue and once by a dequeue operation. The algorithm achieves 3-bounded wait-freedom with constant time operations. The producer completes in at most 3 steps (regular enqueue: 1 step, jump enqueue: 3 steps), whilst consumers always complete in exactly 3 steps. \cite{Mateíspmc}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{David's Queue Operations \cite{Mateíspmc}}
    \label{alg:david-queue}
    \scriptsize
    \begin{algorithmic}[1]
        \State \textbf{Shared variables:}
        \State $HEAD$: array of Fetch\&Increment objects, initially 0
        \State $ITEMS$: 2D array of Swap objects, initially $\bot$
        \State $ROW$: Register, initially 0
        \State
        \State \textbf{Enqueuer's persistent local variables:}
        \State $enq\_row \gets 0$, $tail \gets 0$
        
        \State
        
        \Procedure{Enqueue}{$x$} \Comment{For enqueuer E only}
            \State $val \gets$ Swap($ITEMS[enq\_row, tail]$, $x$) \Comment{Try to enqueue}
            \If{$val = \top$} \Comment{Dequeuer overtook us?}
                \State $enq\_row \gets enq\_row + 1$ \Comment{Jump to next row}
                \State $tail \gets 0$
                \State Swap($ITEMS[enq\_row, tail]$, $x$) \Comment{Write to new row}
                \State Write($ROW$, $enq\_row$) \Comment{Publish new row}
            \EndIf
            \State $tail \gets tail + 1$
            \State \Return OK
        \EndProcedure
        
        \State
        
        \Function{Dequeue}{} \Comment{For dequeuers $D_1, \ldots, D_n$}
            \State $deq\_row \gets$ Read($ROW$) \Comment{Get active row}
            \State $head \gets$ Fetch\&Increment($HEAD[deq\_row]$) \Comment{Reserve column}
            \State $val \gets$ Swap($ITEMS[deq\_row, head]$, $\top$) \Comment{Get value}
            \If{$val = \bot$} \Comment{Empty cell?}
                \State \Return $\varepsilon$ \Comment{Queue was empty}
            \Else
                \State \Return $val$ \Comment{Return dequeued value}
            \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{\acf{MPMC}}\label{subsec:multiple-producer-and-multiple-consumer}
Finally, a look into the \ac{MPMC} case can be made. Here we need to think about synchronising producer-producer contention, consumer-consumer contention and producer-consumer contention. This includes helping methods for the producer and consumer and different atomic primitives. Multiple approaches are available to achieve this:

\subsubsection{Kogan and Petrank's queue} 
Uses a priority-based helping scheme with Michael and Scott's lock-free queue from \cref{alg:michael-scott} as the foundation to achieve wait-freedom, as shown in \cref{alg:kogan-petrank-enqueue,alg:kogan-petrank-dequeue,alg:kogan-petrank-dequeue-help}. Threads complete operations in bounded steps by helping at most n other slower threads. Each thread chooses a monotonically increasing phase number in line 2 of \texttt{ENQUEUE} in \cref{alg:kogan-petrank-enqueue} and line 2 of \texttt{DEQUEUE} of \cref{alg:kogan-petrank-dequeue}, then records operation details in a shared \texttt{state} array in line 3. The helping mechanism in \texttt{HELP} from lines 15 to 26 in \cref{alg:kogan-petrank-dequeue-help} ensures all operations with phases $\leq$ the current phase complete. Threads traverse the state array and invoke \texttt{HELP\_ENQ} or \texttt{HELP\_DEQ} based on pending operations in lines 19 to 23 of \cref{alg:kogan-petrank-dequeue-help}. For enqueue, threads utilise the tail update like in Michael and Scott by first appending the node via \ac{CAS} in line 15 of \texttt{HELP\_ENQ} in \cref{alg:kogan-petrank-enqueue}, then finally helping by updating the tail in line 36 of \texttt{HELP\_FINISH\_ENQ}. The three-step scheme ensures exactly-once execution by first appending a node to the list in line 15, then clearing the pending flag in line 35 of \texttt{HELP\_FINISH\_ENQ}, and finally updating the tail pointer in line 36. For dequeue, threads write their ID to the \texttt{deqTid} field of the head node in line 41 of \texttt{HELP\_DEQ} in \cref{alg:kogan-petrank-dequeue} to "lock" it logically. The consumer then updates the pending flag in line 9 of \texttt{HELP\_FINISH\_DEQ} in \cref{alg:kogan-petrank-dequeue-help} and advances head in line 10. Special handling for empty queues occurs in lines 20 to 25 of \texttt{HELP\_DEQ} of \cref{alg:kogan-petrank-dequeue}, where threads update state with null to indicate emptiness. The phase selection using \texttt{MAXPHASE} in lines 41 to 50 in \cref{alg:kogan-petrank-enqueue} ensures threads help all concurrent operations before returning, preventing starvation. So after a thread helps at most n other threads, the thread's own operation has been completed by a helper, or all n threads are now helping this thread, guaranteeing it completes. This achieves wait-free progress with $O(n)$ steps per operation where $n$ is the number of threads, leading to an $O(n^2)$ bound. \cite{Kogan2011WaitFreeQueues}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{Kogan and Petrank's Queue Enqueue Operation \cite{Kogan2011WaitFreeQueues}}
    \label{alg:kogan-petrank-enqueue}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{Enqueue}{$value$}
            \State $phase \gets$ \Call{MaxPhase}{} $+ 1$ \Comment{Choose phase}
            \State $state[tid] \gets$ \texttt{OpDesc}$(phase, true, true,$ \texttt{Node}$(value, tid))$
            \State \Call{Help}{$phase$} \Comment{Help all ops $\leq$ phase}
            \State \Call{Help\_Finish\_Enq}{} \Comment{Ensure tail updated}
        \EndFunction
        
        \State
        
        \Procedure{Help\_Enq}{$tid, phase$}
            \While{\Call{IsStillPending}{$tid, phase$}}
                \State $last \gets tail$
                \State $next \gets last.next$
                \If{$last = tail$} \Comment{Validate read}
                    \If{$next = $ null} \Comment{Can append?}
                        \If{\Call{IsStillPending}{$tid, phase$}}
                            \If{\texttt{CAS}$(last.next,$ null$, state[tid].node)$}
                                \State \Call{Help\_Finish\_Enq}{}
                                \State \Return
                            \EndIf
                        \EndIf
                    \Else \Comment{Help pending enqueue}
                        \State \Call{Help\_Finish\_Enq}{}
                    \EndIf
                \EndIf
            \EndWhile
        \EndProcedure
        
        \State
        
        \Procedure{Help\_Finish\_Enq}{}
            \State $last \gets tail$
            \State $next \gets last.next$
            \If{$next \neq $ null}
                \State $tid \gets next.enqTid$ \Comment{Thread that owns node}
                \State $desc \gets state[tid]$
                \If{$last = tail$ \textbf{and} $state[tid].node = next$}
                    \State $newDesc \gets$ \texttt{OpDesc}$(state[tid].phase,$ false$,$ true$, next)$
                    \State \texttt{CAS}$(state[tid], desc, newDesc)$ \Comment{Clear pending}
                    \State \texttt{CAS}$(tail, last, next)$ \Comment{Update tail}
                \EndIf
            \EndIf
        \EndProcedure
        
        \State
        
        \Function{MaxPhase}{}
            \State $max \gets -1$
            \For{$i \gets 0$ \textbf{to} $NUM\_THREADS - 1$}
                \State $phase \gets state[i].phase$
                \If{$phase > max$}
                    \State $max \gets phase$
                \EndIf
            \EndFor
            \State \Return $max$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{Kogan and Petrank's Queue Dequeue Operation \cite{Kogan2011WaitFreeQueues}}
    \label{alg:kogan-petrank-dequeue}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{Dequeue}{}
            \State $phase \gets$ \Call{MaxPhase}{} $+ 1$
            \State $state[tid] \gets$ \texttt{OpDesc}$(phase,$ true$,$ false$,$ null$)$
            \State \Call{Help}{$phase$}
            \State \Call{Help\_Finish\_Deq}{}
            \State $node \gets state[tid].node$
            \If{$node = $ null}
                \State \textbf{throw} \texttt{EmptyException}
            \EndIf
            \State \Return $node.next.value$
        \EndFunction
        
        \State
        
        \Procedure{Help\_Deq}{$tid, phase$}
            \While{\Call{IsStillPending}{$tid, phase$}}
                \State $first \gets head$
                \State $last \gets tail$
                \State $next \gets first.next$
                \If{$first = head$} \Comment{Validate read}
                    \If{$first = last$} \Comment{Queue might be empty}
                        \If{$next = $ null} \Comment{Confirmed empty}
                            \State $desc \gets state[tid]$
                            \If{$last = tail$ \textbf{and} \Call{IsStillPending}{$tid, phase$}}
                                \State $newDesc \gets$ \texttt{OpDesc}$(state[tid].phase,$ false$,$ false$,$ null$)$
                                \State \texttt{CAS}$(state[tid], desc, newDesc)$
                            \EndIf
                        \Else
                            \State \Call{Help\_Finish\_Enq}{} \Comment{Help enqueue}
                        \EndIf
                    \Else \Comment{Queue not empty}
                        \State $desc \gets state[tid]$
                        \State $node \gets desc.node$
                        \If{\textbf{not} \Call{IsStillPending}{$tid, phase$}}
                            \State \textbf{break}
                        \EndIf
                        \If{$first = head$ \textbf{and} $node \neq first$}
                            \State $newDesc \gets$ \texttt{OpDesc}$(state[tid].phase,$ true$,$ false$, first)$
                            \If{\textbf{not} \texttt{CAS}$(state[tid], desc, newDesc)$}
                                \State \textbf{continue}
                            \EndIf
                        \EndIf
                        \State \texttt{CAS}$(first.deqTid, -1, tid)$ \Comment{Lock node}
                        \State \Call{Help\_Finish\_Deq}{}
                    \EndIf
                \EndIf
            \EndWhile
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{Kogan and Petrank's Queue Dequeue Helping Operations \cite{Kogan2011WaitFreeQueues}}
    \label{alg:kogan-petrank-dequeue-help}
    \scriptsize
    \begin{algorithmic}[1]
        \Procedure{Help\_Finish\_Deq}{}
            \State $first \gets head$
            \State $next \gets first.next$
            \State $tid \gets first.deqTid$ \Comment{Thread that locked node}
            \If{$tid \neq -1$}
                \State $desc \gets state[tid]$
                \If{$first = head$ \textbf{and} $next \neq $ null}
                    \State $newDesc \gets$ \texttt{OpDesc}$(state[tid].phase,$ false$,$ false$, state[tid].node)$
                    \State \texttt{CAS}$(state[tid], desc, newDesc)$ \Comment{Clear pending}
                    \State \texttt{CAS}$(head, first, next)$ \Comment{Advance head}
                \EndIf
            \EndIf
        \EndProcedure
        
        \State
        
        \Procedure{Help}{$phase$}
            \For{$i \gets 0$ \textbf{to} $NUM\_THREADS - 1$}
                \State $desc \gets state[i]$
                \If{$desc.pending$ \textbf{and} $desc.phase \leq phase$}
                    \If{$desc.enqueue$}
                        \State \Call{Help\_Enq}{$i, phase$}
                    \Else
                        \State \Call{Help\_Deq}{$i, phase$}
                    \EndIf
                \EndIf
            \EndFor
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\subsubsection{Turn Queue} 
Uses a novel turn-based consensus mechanism to achieve wait-freedom without requiring \ac{FAA} instructions. As shown in \cref{alg:turn-enqueue,alg:turn-dequeue,alg:turn-dequeue-help}, the queue maintains two arrays: \texttt{enqueuers} for enqueue requests and \texttt{deqself}/\texttt{deqhelp} for dequeue operations. Each thread has a unique index used as its thread ID (\texttt{enqTid} or \texttt{deqTid}). For the producers in \cref{alg:turn-enqueue}, threads publish their intent by storing a node pointer in \texttt{enqueuers[myIdx]} in line 6 of the function \texttt{ENQUEUE}. The consensus mechanism uses the \texttt{enqTid} of the tail node to determine whose turn is next. Threads scan the \texttt{enqueuers} array starting from position \texttt{(tail->enqTid + 1) \% maxThreads} in line 20, helping the first non-null request they find. This creates a circular turn order ensuring fairness. The algorithm guarantees that after publishing a request, at most \texttt{maxThreads-1} other nodes will be enqueued first, achieving wait-free bounded progress. The enqueue operation protects the tail pointer using hazard pointers in line 12 of the \texttt{ENQUEUE} function, then clears any completed request from the tail's position in lines 15 to 18. It searches for the next request to help in lines 19 to 26, attempting to append the found node via \ac{CAS} in line 23. Finally, it advances the tail pointer if a node was successfully appended in line 29. The operation completes when the thread's own request has been processed (detected by checking if \texttt{enqueuers[myIdx]} is null in line 8). For consumers in \cref{alg:turn-dequeue}, the algorithm uses a dual-array approach with \texttt{deqself} and \texttt{deqhelp} to avoid excessive hazard pointer usage. Threads open a request by making \texttt{deqself[myIdx]} equal to \texttt{deqhelp[myIdx]} in lines 4 and 5 of the \texttt{DEQUEUE} function. The turn order follows the \texttt{deqTid} of the head node. When assigning nodes, threads use \ac{CAS} to set the next node's \texttt{deqTid} field in line 8 of \texttt{SEARCHNEXT} in \cref{alg:turn-dequeue-help}. This assignment is permanent and indicates ownership. The dequeue handles empty queues through a "give-up" mechanism implemented in \texttt{GIVEUP} (lines 28 to 44 of \cref{alg:turn-dequeue-help}). When detecting an empty queue (head equals tail) in line 12 of \texttt{DEQUEUE} in \cref{alg:turn-dequeue}, threads roll back their request in line 13 but must ensure no concurrent thread assigned them a node. This involves re-checking the queue state and potentially self-assigning the first node if no other requests exist (line 41 of \texttt{GIVEUP}). The algorithm closes requests by updating \texttt{deqhelp[i]} in line 18 of \texttt{CASDEQANDHEAD} in \cref{alg:turn-dequeue-help}, making it differ from \texttt{deqself[i]}. Memory reclamation uses wait-free bounded hazard pointers integrated into the algorithm. Nodes are retired in line 35 of \texttt{DEQUEUE} in \cref{alg:turn-dequeue} only after ensuring they're no longer accessible through shared variables. The algorithm requires only one allocation per enqueued item (the node itself), achieving minimal memory overhead of $O(N_{threads})$ compared to the $O(N_{threads}^2)$ of other wait-free queues. \cite{RamalheteQueue}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{Turn Queue Enqueue Operation \cite{RamalheteQueue}}
    \label{alg:turn-enqueue}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{Enqueue}{$item$}
            \If{$item = $ null} \textbf{throw} \texttt{InvalidArgument}
            \EndIf
            \State $myIdx \gets $ \Call{GetIndex}{}
            \State $myNode \gets $ \textbf{new} \texttt{Node}$(item, myIdx)$
            \State $enqueuers[myIdx] \gets myNode$
            \For{$i \gets 0$ \textbf{to} $maxThreads - 1$}
                \If{$enqueuers[myIdx] = $ null} \Comment{Request completed}
                    \State $hp.$\Call{Clear}{}
                    \State \Return
                \EndIf
                \State $ltail \gets hp.$\Call{ProtectPtr}{$kHpTail, tail$}
                \If{$ltail \neq tail$} \textbf{continue}
                \EndIf
                \If{$enqueuers[ltail.enqTid] = ltail$} \Comment{Clear old request}
                    \State $tmp \gets ltail$
                    \State \texttt{CAS}$(enqueuers[ltail.enqTid], tmp,$ null$)$
                \EndIf
                \For{$j \gets 1$ \textbf{to} $maxThreads$} \Comment{Find next request}
                    \State $nodeHelp \gets enqueuers[(j + ltail.enqTid) \bmod maxThreads]$
                    \If{$nodeHelp \neq $ null}
                        \State $nullnode \gets $ null
                        \State \texttt{CAS}$(ltail.next, nullnode, nodeHelp)$ \Comment{Try append}
                        \State \textbf{break}
                    \EndIf
                \EndFor
                \State $lnext \gets ltail.next$
                \If{$lnext \neq $ null} \Comment{Advance tail if needed}
                    \State \texttt{CAS}$(tail, ltail, lnext)$
                \EndIf
            \EndFor
            \State $enqueuers[myIdx] \gets $ null \Comment{Cleanup if not helped}
            \State $hp.$\Call{Clear}{}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{Turn Queue Dequeue Operation \cite{RamalheteQueue}}
    \label{alg:turn-dequeue}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{Dequeue}{}
            \State $myIdx \gets $ \Call{GetIndex}{}
            \State $prReq \gets deqself[myIdx]$ \Comment{Save previous request}
            \State $myReq \gets deqhelp[myIdx]$
            \State $deqself[myIdx] \gets myReq$ \Comment{Open request}
            \For{$i \gets 0$ \textbf{to} $maxThreads - 1$}
                \If{$deqhelp[myIdx] \neq myReq$} \textbf{break} \Comment{Request satisfied}
                \EndIf
                \State $lhead \gets hp.$\Call{ProtectPtr}{$kHpHead, head$}
                \If{$lhead \neq head$} \textbf{continue}
                \EndIf
                \If{$lhead = tail$} \Comment{Queue empty}
                    \State $deqself[myIdx] \gets prReq$ \Comment{Rollback}
                    \State \Call{GiveUp}{$myReq, myIdx$}
                    \If{$deqhelp[myIdx] \neq myReq$} \Comment{Check if helped}
                        \State $deqself[myIdx] \gets myReq$
                        \State \textbf{break}
                    \EndIf
                    \State $hp.$\Call{Clear}{}
                    \State \Return null
                \EndIf
                \State $lnext \gets hp.$\Call{ProtectPtr}{$kHpNext, lhead.next$}
                \If{$lhead \neq head$} \textbf{continue}
                \EndIf
                \If{\Call{SearchNext}{$lhead, lnext$} $\neq NOIDX$}
                    \State \Call{CasDeqAndHead}{$lhead, lnext, myIdx$}
                \EndIf
            \EndFor
            \State $myNode \gets deqhelp[myIdx]$
            \State $lhead \gets hp.$\Call{ProtectPtr}{$kHpHead, head$}
            \If{$lhead = head$ \textbf{and} $myNode = lhead.next$}
                \State \texttt{CAS}$(head, lhead, myNode)$ \Comment{Help advance head}
            \EndIf
            \State $hp.$\Call{Clear}{}
            \State $hp.$\Call{Retire}{$prReq$} \Comment{Retire previous node}
            \State \Return $myNode.item$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{Turn Queue Dequeue Helper Functions \cite{RamalheteQueue}}
    \label{alg:turn-dequeue-help}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{SearchNext}{$lhead, lnext$}
            \State $turn \gets lhead.deqTid$
            \For{$idx \gets turn + 1$ \textbf{to} $turn + maxThreads$}
                \State $idDeq \gets idx \bmod maxThreads$
                \If{$deqself[idDeq] \neq deqhelp[idDeq]$} \textbf{continue}
                \EndIf
                \If{$lnext.deqTid = NOIDX$}
                    \State \texttt{CAS}$(lnext.deqTid, NOIDX, idDeq)$ \Comment{Assign node}
                \EndIf
                \State \textbf{break}
            \EndFor
            \State \Return $lnext.deqTid$
        \EndFunction
        
        \State
        
        \Procedure{CasDeqAndHead}{$lhead, lnext, myIdx$}
            \State $ldeqTid \gets lnext.deqTid$
            \If{$ldeqTid = myIdx$} \Comment{My node}
                \State $deqhelp[ldeqTid] \gets lnext$ \Comment{Close request}
            \Else
                \State $ldeqhelp \gets hp.$\Call{ProtectPtr}{$kHpDeq, deqhelp[ldeqTid]$}
                \If{$ldeqhelp \neq lnext$ \textbf{and} $lhead = head$}
                    \State \texttt{CAS}$(deqhelp[ldeqTid], ldeqhelp, lnext)$ \Comment{Help close}
                \EndIf
            \EndIf
            \State \texttt{CAS}$(head, lhead, lnext)$ \Comment{Advance head}
        \EndProcedure
        
        \State
        
        \Procedure{GiveUp}{$myReq, myIdx$}
            \State $lhead \gets head$
            \If{$deqhelp[myIdx] \neq myReq$} \Return \Comment{Already helped}
            \EndIf
            \If{$lhead = tail$} \Return \Comment{Still empty}
            \EndIf
            \State $hp.$\Call{ProtectPtr}{$kHpHead, lhead$}
            \If{$lhead \neq head$} \Return
            \EndIf
            \State $lnext \gets hp.$\Call{ProtectPtr}{$kHpNext, lhead.next$}
            \If{$lhead \neq head$} \Return
            \EndIf
            \If{\Call{SearchNext}{$lhead, lnext$} $= NOIDX$}
                \State \texttt{CAS}$(lnext.deqTid, NOIDX, myIdx)$ \Comment{Self-assign}
            \EndIf
            \State \Call{CasDeqAndHead}{$lhead, lnext, myIdx$}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\subsubsection{\ac{YMC} queue} 
Uses \ac{FAA} and \ac{CAS} combined with Kogan's and Petrank's fast-path-slow-path methodology mentioned in \cref{ch:related-work} to use the advantage of lock-free algorithms if possible whilst still maintaining wait-freedom. As shown in \cref{alg:ymc-enqueue,alg:ymc-enqueue-help,alg:ymc-dequeue,alg:ymc-dequeue-help}, the queue represents cells as an infinite array emulated through linked segments. Each segment contains \texttt{N} cells and a pointer to the next segment. The queue maintains global indices \texttt{H} (head) and \texttt{T} (tail) that are accessed using \ac{FAA}, ensuring atomic increments without retry loops. For producers in \cref{alg:ymc-enqueue}, threads obtain a unique cell index via \ac{FAA} on \texttt{T} in line 11. They then locate the corresponding cell using \texttt{find\_cell} which traverses segments and allocates new ones if needed. The fast-path attempts a simple \ac{CAS} to deposit the value in line 13. If this fails, due to concurrent dequeue marking the cell unusable, the thread switches to the slow-path starting at line 20. The slow-path employs a helping mechanism where threads publish enqueue requests in their handle structure in line 23. In \cref{alg:ymc-dequeue-help} consumers help pending enqueues through \texttt{help\_enq} in line 13 when they mark cells unusable. This creates a symbiotic relationship: producers get help depositing values, whilst consumers ensure values are available to dequeue. For consumers in \cref{alg:ymc-dequeue}, threads obtain cell indices via \ac{FAA} on \texttt{H} in line 18. The algorithm uses \texttt{help\_enq} to secure values, which may involve helping slow-path enqueues. If a value is found, the consumer claims it using \ac{CAS} on the cell's \texttt{deq} field in line 23. The slow-path beginning at line 30 publishes dequeue requests that helpers can satisfy by finding unclaimed values or determining the queue is empty. The algorithm maintains linearisability through careful ordering. Enqueues linearise when \texttt{T} moves past their cell index, whilst dequeues linearise when \texttt{H} moves past theirs. The helping mechanism ensures that after at most $O(n^2)$ failed attempts, all threads become helpers for a pending operation, guaranteeing completion. The slow-path is always entered after at most a thread tried the fast-path \texttt{PATIENCE} times (line 2), which is a defined constant. \cite{FastFetchAndAddWaitFreeQueue}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{YMC Queue Enqueue Operation \cite{FastFetchAndAddWaitFreeQueue}}
    \label{alg:ymc-enqueue}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{Enqueue}{$q, h, v$}
            \For{$p \gets \texttt{PATIENCE}$ \textbf{downto} $0$} \Comment{Try fast-path first}
                \If{\Call{Enq\_Fast}{$q, h, v, \&\mathit{cell\_id}$}}
                    \State \Return
                \EndIf
            \EndFor
            \State \Call{Enq\_Slow}{$q, h, v, \mathit{cell\_id}$} \Comment{Fall back to slow-path}
        \EndFunction
        
        \State
        
        \Function{Enq\_Fast}{$q, h, v, \mathit{cid}$}
            \State $i \gets \texttt{FAA}(\&q \rightarrow T, 1)$ \Comment{Get unique cell index}
            \State $c \gets \Call{Find\_Cell}{\&h \rightarrow \mathit{tail}, i}$ \Comment{Locate cell, allocate segment if needed}
            \If{\texttt{CAS}$(c.\mathit{val}, \bot, v)$} \Comment{Try to deposit value}
                \State \Return \textbf{true}
            \EndIf
            \State $*\mathit{cid} \gets i$ \Comment{Return cell index for slow-path}
            \State \Return \textbf{false}
        \EndFunction
        
        \State
        
        \Function{Enq\_Slow}{$q, h, v, \mathit{cell\_id}$}
            \State $r \gets \&h \rightarrow \mathit{enq}.\mathit{req}$
            \State $r \rightarrow \mathit{val} \gets v$ \Comment{Publish value}
            \State $r \rightarrow \mathit{state} \gets (1, \mathit{cell\_id})$ \Comment{Set pending=1, id=cell\_id}
            \State $\mathit{tmp\_tail} \gets h \rightarrow \mathit{tail}$
            \Repeat
                \State $i \gets \texttt{FAA}(\&q \rightarrow T, 1)$
                \State $c \gets \Call{Find\_Cell}{\&\mathit{tmp\_tail}, i}$
                \If{\texttt{CAS}$(c \rightarrow \mathit{enq}, \bot_e, r)$ \textbf{and} $c.\mathit{val} = \bot$} \Comment{Reserve cell}
                    \State \Call{Try\_To\_Claim\_Req}{$\&r \rightarrow \mathit{state}, \mathit{id}, i$} \Comment{Claim request for this cell}
                    \State \textbf{break}
                \EndIf
            \Until{$\neg r \rightarrow \mathit{state}.\mathit{pending}$} \Comment{Until helped by dequeuer}
            \State $\mathit{id} \gets r \rightarrow \mathit{state}.\mathit{id}$ \Comment{Get claimed cell index}
            \State $c \gets \Call{Find\_Cell}{\&h \rightarrow \mathit{tail}, \mathit{id}}$
            \State \Call{Enq\_Commit}{$q, c, v, \mathit{id}$} \Comment{Write value to claimed cell}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{YMC Queue Enqueue Help Operation \cite{FastFetchAndAddWaitFreeQueue}}
    \label{alg:ymc-enqueue-help}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{Help\_Enq}{$q, h, c, i$}
        \If{$\neg$\texttt{CAS}$(c \rightarrow \mathit{val}, \bot, \top)$ \textbf{and} $c \rightarrow \mathit{val} \neq \top$} \Comment{Value already present}
            \State \Return $c \rightarrow \mathit{val}$
        \EndIf
        \If{$c \rightarrow \mathit{enq} = \bot_e$} \Comment{No request yet, find one to help}
            \Repeat
                \State $p \gets h \rightarrow \mathit{enq}.\mathit{peer}$
                \State $r \gets \&p \rightarrow \mathit{enq}.\mathit{req}$
                \State $s \gets r \rightarrow \mathit{state}$
                \If{$h \rightarrow \mathit{enq}.\mathit{id} = 0$ \textbf{or} $h \rightarrow \mathit{enq}.\mathit{id} = s.\mathit{id}$} \Comment{Haven't helped this request}
                    \State \textbf{break}
                \EndIf
                \State $h \rightarrow \mathit{enq}.\mathit{id} \gets 0$
                \State $h \rightarrow \mathit{enq}.\mathit{peer} \gets p \rightarrow \mathit{next}$ \Comment{Move to next peer}
            \Until{\textbf{true}}
            \If{$s.\mathit{pending}$ \textbf{and} $s.\mathit{id} \leq i$ \textbf{and} $\neg$\texttt{CAS}$(c \rightarrow \mathit{enq}, \bot_e, r)$} \Comment{Try to reserve cell for peer}
                \State $h \rightarrow \mathit{enq}.\mathit{id} \gets s.\mathit{id}$ \Comment{Remember we tried to help}
            \Else
                \State $h \rightarrow \mathit{enq}.\mathit{peer} \gets p \rightarrow \mathit{next}$ \Comment{Peer doesn't need help}
            \EndIf
            \If{$c \rightarrow \mathit{enq} = \bot_e$}
                \State \texttt{CAS}$(c \rightarrow \mathit{enq}, \bot_e, \top_e)$ \Comment{Mark no enqueue will use this cell}
            \EndIf
        \EndIf
        \If{$c \rightarrow \mathit{enq} = \top_e$} \Comment{No enqueue will fill this cell}
            \State \Return $(q \rightarrow T \leq i$ ? \texttt{EMPTY} : $\top)$ \Comment{Check if queue was empty}
        \EndIf
        \State $r \gets c \rightarrow \mathit{enq}$ \Comment{Cell has enqueue request}
        \State $s \gets r \rightarrow \mathit{state}$
        \State $v \gets r \rightarrow \mathit{val}$
        \If{$s.\mathit{id} > i$} \Comment{Request unsuitable for this cell}
            \If{$c \rightarrow \mathit{val} = \top$ \textbf{and} $q \rightarrow T \leq i$}
                \State \Return \texttt{EMPTY}
            \EndIf
        \ElsIf{\Call{Try\_To\_Claim\_Req}{$\&r \rightarrow \mathit{state}, s.\mathit{id}, i$} \textbf{or} $(s = (0, i)$ \textbf{and} $c \rightarrow \mathit{val} = \top)$}
            \State \Call{Enq\_Commit}{$q, c, v, i$} \Comment{Help commit the value}
        \EndIf
        \State \Return $c \rightarrow \mathit{val}$
    \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{YMC Queue Dequeue Operation \cite{FastFetchAndAddWaitFreeQueue}}
    \label{alg:ymc-dequeue}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{Dequeue}{$q, h$}
            \For{$p \gets \texttt{PATIENCE}$ \textbf{downto} $0$} \Comment{Try fast-path first}
                \State $v \gets \Call{Deq\_Fast}{q, h, \&\mathit{cell\_id}}$
                \If{$v \neq \top$} \textbf{break}
                \EndIf
            \EndFor
            \If{$v = \top$} \Comment{Fast-path failed}
                \State $v \gets \Call{Deq\_Slow}{q, h, \mathit{cell\_id}}$ \Comment{Use slow-path}
            \EndIf
            \If{$v \neq \texttt{EMPTY}$} \Comment{Got value, help peer dequeue}
                \State \Call{Help\_Deq}{$q, h, h \rightarrow \mathit{deq}.\mathit{peer}$}
                \State $h \rightarrow \mathit{deq}.\mathit{peer} \gets h \rightarrow \mathit{deq}.\mathit{peer} \rightarrow \mathit{next}$
            \EndIf
            \State \Return $v$
        \EndFunction
        
        \State
        
        \Function{Deq\_Fast}{$q, h, \mathit{id}$}
            \State $i \gets \texttt{FAA}(\&q \rightarrow H, 1)$ \Comment{Get unique cell index}
            \State $c \gets \Call{Find\_Cell}{\&h \rightarrow \mathit{head}, i}$
            \State $v \gets \Call{Help\_Enq}{q, h, c, i}$ \Comment{Try to get/help produce value}
            \If{$v = \texttt{EMPTY}$} \Return \texttt{EMPTY}
            \EndIf
            \If{$v \neq \top$ \textbf{and} \texttt{CAS}$(c \rightarrow \mathit{deq}, \bot_d, \top_d)$} \Comment{Claim the value}
                \State \Return $v$
            \EndIf
            \State $*\mathit{id} \gets i$ \Comment{Return cell index for slow-path}
            \State \Return $\top$
        \EndFunction
        
        \State
        
        \Function{Deq\_Slow}{$q, h, \mathit{cid}$}
            \State $r \gets \&h \rightarrow \mathit{deq}.\mathit{req}$
            \State $r \rightarrow \mathit{id} \gets \mathit{cid}$ \Comment{Set request ID}
            \State $r \rightarrow \mathit{state} \gets (1, \mathit{cid})$ \Comment{Publish pending request}
            \State \Call{Help\_Deq}{$q, h, h$} \Comment{Help complete own request}
            \State $i \gets r \rightarrow \mathit{state}.\mathit{idx}$ \Comment{Get index where value found}
            \State $c \gets \Call{Find\_Cell}{\&h \rightarrow \mathit{head}, i}$
            \State $v \gets c \rightarrow \mathit{val}$
            \State \Call{Advance\_End\_For\_Linearizability}{$\&q \rightarrow H, i + 1$} \Comment{Ensure linearisability}
            \State \Return $(v = \top$ ? \texttt{EMPTY} : $v)$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{YMC Queue Dequeue Help Operation \cite{FastFetchAndAddWaitFreeQueue}}
    \label{alg:ymc-dequeue-help}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{Help\_Deq}{$q, h, \mathit{helpee}$}
        \State $r \gets \mathit{helpee} \rightarrow \mathit{deq}.\mathit{req}$
        \State $s \gets r \rightarrow \mathit{state}$
        \State $\mathit{id} \gets r \rightarrow \mathit{id}$
        \If{$\neg s.\mathit{pending}$ \textbf{or} $s.\mathit{idx} < \mathit{id}$} \Return \Comment{Request complete or invalid}
        \EndIf
        \State $\mathit{ha} \gets \mathit{helpee} \rightarrow \mathit{head}$ \Comment{Segment pointer for announced cells}
        \State $s \gets r \rightarrow \mathit{state}$ \Comment{Re-read after getting head}
        \State $\mathit{prior} \gets \mathit{id}$; $i \gets \mathit{id}$; $\mathit{cand} \gets 0$
        \While{\textbf{true}}
            \For{$\mathit{hc} \gets \mathit{ha}$; $\neg\mathit{cand}$ \textbf{and} $s.\mathit{idx} = \mathit{prior}$;} \Comment{Find candidate}
                \State $c \gets \Call{Find\_Cell}{\&\mathit{hc}, ++i}$
                \State $v \gets \Call{Help\_Enq}{q, \mathit{hc}, c, i}$
                \If{$v = \texttt{EMPTY}$ \textbf{or} $(v \neq \top$ \textbf{and} $c \rightarrow \mathit{deq} = \bot_d)$} \Comment{Found candidate}
                    \State $\mathit{cand} \gets i$
                \Else
                    \State $s \gets r \rightarrow \mathit{state}$ \Comment{Check if announced}
                \EndIf
            \EndFor
            \If{$\mathit{cand}$}
                \State \texttt{CAS}$(\&r \rightarrow \mathit{state}, (1, \mathit{prior}), (1, \mathit{cand}))$ \Comment{Try announce candidate}
                \State $s \gets r \rightarrow \mathit{state}$
            \EndIf
            \If{$\neg s.\mathit{pending}$ \textbf{or} $r \rightarrow \mathit{id} \neq \mathit{id}$} \Return \Comment{Request completed}
            \EndIf
            \State $c \gets \Call{Find\_Cell}{\&\mathit{ha}, s.\mathit{idx}}$ \Comment{Get announced cell}
            \If{$c \rightarrow \mathit{val} = \top$ \textbf{or} \texttt{CAS}$(c \rightarrow \mathit{deq}, \bot_d, r)$ \textbf{or} $c \rightarrow \mathit{deq} = r$}
                \State \texttt{CAS}$(\&r \rightarrow \mathit{state}, s, (0, s.\mathit{idx}))$ \Comment{Complete request}
                \State \Return
            \EndIf
            \State $\mathit{prior} \gets s.\mathit{idx}$
            \If{$s.\mathit{idx} \geq i$} \Comment{Announced cell is ahead}
                \State $\mathit{cand} \gets 0$; $i \gets s.\mathit{idx}$ \Comment{Jump forward}
            \EndIf
        \EndWhile
    \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{Feldman-Dechev Queue} 
Uses a sequence number-based mechanism with bitmarking to achieve bounded completion. As shown in \cref{alg:rb-enqueue,alg:rb-dequeue,alg:rb-helper}, the ring buffer maintains two atomic counters accessed via \texttt{NextTailSeq} in line 7 of \cref{alg:rb-enqueue} and \texttt{NextHeadSeq} in line 7 of \cref{alg:rb-dequeue}. Each position in the buffer array stores either an \texttt{ElemNode} containing an element and sequence ID, or an \texttt{EmptyNode} containing only a sequence ID. For producers in \cref{alg:rb-enqueue}, threads acquire a sequence ID via \ac{FAA} on the tail counter in line 7. The position is determined as \texttt{seqid mod capacity} in line 8. In the common case, threads replace an \texttt{EmptyNode} with matching or lower sequence ID with their prepared \texttt{ElemNode} via \ac{CAS} in line 32. The algorithm handles thread delays through backoff and retry mechanisms. If a position contains an \texttt{ElemNode} or has a higher sequence ID than assigned, the thread breaks from the inner loop in line 36 to acquire a new sequence ID in the outer loop and attempt insertion at a new position. Bitmarking, setting a flag on a node, is used to indicate positions needing correction by delayed threads in line 20. For consumers in \cref{alg:rb-dequeue}, threads similarly acquire a sequence ID via \ac{FAA} on the head counter in line 7. They prepare an \texttt{EmptyNode} with sequence ID incremented by the buffer capacity in line 9. The dequeue succeeds when replacing an \texttt{ElemNode} with matching sequence ID in line 42 via \ac{CAS}. If encountering an \texttt{EmptyNode} or lower sequence ID, threads use backoff and may bitmark \texttt{ElemNodes} to maintain FIFO ordering in line 32. The algorithm achieves bounded completion through a progress assurance scheme. After \texttt{MAX\_FAILS} attempts in line 11, threads post their operation to an announcement table and execute a wait-free slow path in lines 12 to 14. The helping mechanism works as follows: \texttt{TryHelpAnother} in lines 33 to 45 of \cref{alg:rb-helper} checks one entry in the announcement table per call, cycling through all threads. When an announced operation is found, helpers attempt to complete it using the Associate functions. These functions either claim and complete the operation via \ac{CAS} or clean up failed attempts by replacing nodes. This ensures every operation completes within $O(N_{threads}^2)$ steps, as all threads will eventually help any announced operation. \cite{FeldmanDechevV2}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{Feldman-Dechev Queue's Enqueue Operation \cite{FeldmanDechevV2}}
    \label{alg:rb-enqueue}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{Enqueue}{$val$}
            \State \Call{TryHelpAnother}{}
            \State $fails \gets 0$
            \While{true}
                \If{\Call{IsFull}{}} \Return false
                \EndIf
                \State $seqid \gets $ \Call{NextTailSeq}{} \Comment{FAA on tail}
                \State $pos \gets seqid \bmod capacity$
                \State $n\_node \gets $ \textbf{new} \texttt{ElemNode}$(seqid, val)$
                \While{true}
                    \If{$fails$++ $= MAX\_FAILS$}
                        \State $op \gets $ \textbf{new} \texttt{EnqueueOp}$(val)$
                        \State \Call{MakeAnnouncement}{$op$}
                        \State \Return $op.$\Call{Result}{}
                    \EndIf
                    \State $node \gets buffer[pos].$\Call{Load}{}
                    \If{$node.op \neq $ null} \Comment{Operation record}
                        \State $node.op.$\Call{Associate}{$node, \&buffer[pos]$}
                        \State \textbf{continue}
                    \ElsIf{\Call{IsSkipped}{$node$}} \Comment{Bitmarked}
                        \State \textbf{break} \Comment{Get new seqid}
                    \ElsIf{$node.seqid < seqid$}
                        \State \Call{Backoff}{}
                        \If{$node = buffer[pos].$\Call{Load}{}}
                            \If{\Call{IsEmptyNode}{$node$}}
                                \If{$buffer[pos].$\Call{CAS}{$node, n\_node$}}
                                    \State \Return true
                                \EndIf
                            \EndIf
                        \EndIf
                    \ElsIf{$node.seqid \leq seqid$ \textbf{and} \Call{IsEmptyNode}{$node$}}
                        \If{$buffer[pos].$\Call{CAS}{$node, n\_node$}}
                            \State \Return true
                        \EndIf
                    \Else \Comment{$node.seqid > seqid$ or ElemNode}
                        \State \textbf{break} \Comment{Get new seqid}
                    \EndIf
                \EndWhile
            \EndWhile
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{Feldman-Dechev Queue's Dequeue Operation \cite{FeldmanDechevV2}}
    \label{alg:rb-dequeue}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{Dequeue}{$\&result$}
            \State \Call{TryHelpAnother}{}
            \State $fails \gets 0$
            \While{true}
                \If{\Call{IsEmpty}{}} \Return false
                \EndIf
                \State $seqid \gets $ \Call{NextHeadSeq}{} \Comment{FAA on head}
                \State $pos \gets seqid \bmod capacity$
                \State $n\_node \gets $ \textbf{new} \texttt{EmptyNode}$(seqid + capacity)$
                \While{true}
                    \If{$fails$++ $= MAX\_FAILS$}
                        \State $op \gets $ \textbf{new} \texttt{DequeueOp}{}
                        \State \Call{MakeAnnouncement}{$op$}
                        \State \Return $op.$\Call{Result}{$result$}
                    \EndIf
                    \State $node \gets buffer[pos].$\Call{Load}{}
                    \If{$node.op \neq $ null}
                        \State $node.op.$\Call{Associate}{$node, \&buffer[pos]$}
                        \State \textbf{continue}
                    \ElsIf{\Call{IsSkipped}{$node$} \textbf{and} \Call{IsEmptyNode}{$node$}}
                        \If{$buffer[pos].$\Call{CAS}{$node, n\_node$}}
                            \State \textbf{break}
                        \EndIf
                    \ElsIf{$seqid > node.seqid$} \Comment{Delayed element}
                        \State \Call{Backoff}{}
                        \If{$node = buffer[pos].$\Call{Load}{}}
                            \If{\Call{IsEmptyNode}{$node$}}
                                \If{$buffer[pos].$\Call{CAS}{$node, n\_node$}}
                                    \State \textbf{break}
                                \EndIf
                            \Else
                                \State \Call{SetSkipped}{$\&buffer[pos]$} \Comment{Bitmark}
                            \EndIf
                        \EndIf
                    \ElsIf{$seqid < node.seqid$}
                        \State \textbf{break} \Comment{Get new seqid}
                    \Else \Comment{$seqid = node.seqid$}
                        \If{\Call{IsElemNode}{$node$}}
                            \If{\Call{IsSkipped}{$node$}}
                                \State $n\_node \gets $ \Call{SetSkipped}{$n\_node$}
                            \EndIf
                            \If{$buffer[pos].$\Call{CAS}{$node, n\_node$}}
                                \State $result \gets node.value$
                                \State \Return true
                            \EndIf
                        \Else \Comment{EmptyNode with matching seqid}
                            \State \Call{Backoff}{}
                            \If{$node = buffer[pos].$\Call{Load}{}}
                                \If{$buffer[pos].$\Call{CAS}{$node, n\_node$}}
                                    \State \textbf{break}
                                \EndIf
                            \EndIf
                        \EndIf
                    \EndIf
                \EndWhile
            \EndWhile
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{Feldman-Dechev Queue's Helper Functions \cite{FeldmanDechevV2}}
    \label{alg:rb-helper}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{EnqueueOp::Associate}{$node, address$}
            \State $success \gets helper.$\Call{CAS}{null, $node$}
            \If{$success$ \textbf{or} $helper.$\Call{Load}{} $= node$}
                \State $node.op.$\Call{Store}{null} \Comment{Remove op reference}
            \Else
                \State $n\_node \gets $ \textbf{new} \texttt{EmptyNode}$(node.seqid)$
                \If{\textbf{not} $address.$\Call{CAS}{$node, n\_node$}}
                    \State $node \gets $ \Call{SetSkipped}{$node$}
                    \If{$address.$\Call{Load}{} $= node$}
                        \State $n\_node \gets $ \Call{SetSkipped}{$n\_node$}
                        \State $address.$\Call{CAS}{$node, n\_node$}
                    \EndIf
                \EndIf
            \EndIf
        \EndFunction
        
        \State
        
        \Function{DequeueOp::Associate}{$node, address$}
            \State $success \gets helper.$\Call{CAS}{null, $node$}
            \If{$success$ \textbf{or} $helper.$\Call{Load}{} $= node$}
                \State $n\_node \gets $ \textbf{new} \texttt{EmptyNode}$(node.seqid + capacity)$
                \If{\textbf{not} $address.$\Call{CAS}{$node, n\_node$}}
                    \State $node \gets $ \Call{SetSkipped}{$node$}
                    \If{$address.$\Call{Load}{} $= node$}
                        \State $n\_node \gets $ \Call{SetSkipped}{$n\_node$}
                        \State $address.$\Call{CAS}{$node, n\_node$}
                    \EndIf
                \EndIf
            \Else
                \State $node.op.$\Call{Store}{null}
            \EndIf
        \EndFunction
        \State
        \Function{TryHelpAnother}{}
            \State \Comment{Check announcement table and help one operation}
            \State $helpIdx \gets $ thread-local helping index
            \State $op \gets announcementTable[helpIdx]$
            \If{$op \neq $ null \textbf{and} $op.$\Call{InProgress}{}}
                \If{$op$ is \texttt{EnqueueOp}}
                    \State \Call{WaitFreeEnqueue}{$op$}
                \Else
                    \State \Call{WaitFreeDequeue}{$op$}
                \EndIf
            \EndIf
            \State $helpIdx \gets (helpIdx + 1) \bmod MAX\_THREADS$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{Verma's Queue}\label{subsubsec:verma-queue}
Uses an external helper thread that works on a dedicated core to help other processes to finish their work in a finite number of steps. As shown in \cref{alg:verma-ops}, the queue maintains a state array where each worker has a dedicated slot for operation requests. The queue uses a linked list with head and tail pointers managed exclusively by the helper thread. For producers in \cref{alg:verma-ops}, threads create a request object containing the operation type and element in line 14, then place it in their designated position in the state array via direct assignment in line 15. They wait until the helper marks the operation as completed in line 16. The algorithm achieves simplicity by delegating all queue modifications to a single helper thread, eliminating the need for complex synchronisation. For consumers in \cref{alg:verma-ops}, threads similarly create a dequeue request in line 3 and place it in their state array slot in line 4. They wait for completion in line 5, after which the dequeued element is available in the request object in line 9. The helper thread in \cref{alg:verma-ops} continuously traverses the state array in round-robin fashion in lines 24 to 52. When encountering an enqueue request, it creates a new node in line 29, appends it to the tail in line 30, and updates the tail reference in line 31. For dequeue requests, the helper checks if the queue is empty in line 35 and removes the head element in lines 39 and 40. The helper then updates the request with the dequeued value in line 44. This achieves bounded completion of every process as each operation completes within $O(N)$ steps, where $N$ is the number of workers, since the helper visits each state array position in fixed order. \cite{Verma2013Scalable}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{Verma's Queue Operations \cite{Verma2013Scalable}}
    \label{alg:verma-ops}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{Dequeue}{}
            \State $id \gets $ \Call{GetThreadId}{} \Comment{Get worker's unique ID}
            \State $req \gets $ \Call{CreateRequest}{null, DEQUEUE} \Comment{Create dequeue request}
            \State $stateArr[id] \gets req$ \Comment{Place request in dedicated slot}
            \While{$\neg req.isCompleted$} \Comment{Wait for helper to process}
                \State \textbf{wait}
            \EndWhile
            \State $stateArr[id] \gets $ null \Comment{Clear request from state array}
            \State \Return $req.e$ \Comment{Return dequeued element}
        \EndFunction

        \State

        \Function{Enqueue}{$e$}
            \State $id \gets $ \Call{GetThreadId}{} \Comment{Get worker's unique ID}
            \State $req \gets $ \Call{CreateRequest}{$e$, ENQUEUE} \Comment{Create enqueue request with element}
            \State $stateArr[id] \gets req$ \Comment{Place request in dedicated slot}
            \While{$\neg req.isCompleted$} \Comment{Wait for helper to process}
                \State \textbf{wait}
            \EndWhile
            \State $stateArr[id] \gets $ null \Comment{Clear request from state array}
            \State \Return true
        \EndFunction

        \State

        \Function{Helper}{}
            \State $id \gets 0$ \Comment{Start at first worker slot}
            \While{true} \Comment{Continuous helper loop}
                \State $req \gets stateArr[id]$ \Comment{Check current slot for request}
                \If{$req \neq $ null $\land \neg req.isCompleted$} \Comment{Found pending request}
                    \If{$req.operation = $ ENQUEUE}
                        \State $n \gets $ \textbf{new} Node$(req.e)$ \Comment{Create new node}
                        \State $tail.next \gets n$ \Comment{Append to tail}
                        \State $tail \gets n$ \Comment{Update tail reference}
                        \State $size \gets size + 1$
                        \State $req.isCompleted \gets $ true \Comment{Mark request complete}
                    \ElsIf{$req.operation = $ DEQUEUE}
                        \If{$head.next = $ null} \Comment{Queue is empty}
                            \State $req.e \gets $ null
                            \State $req.isCompleted \gets $ true
                        \Else
                            \State $n \gets head.next$ \Comment{Get first element}
                            \State $head.next \gets n.next$ \Comment{Remove from queue}
                            \If{$n.next = $ null} \Comment{Queue now empty}
                                \State $tail \gets head$ \Comment{Reset tail}
                            \EndIf
                            \State $req.e \gets n.e$ \Comment{Store dequeued value}
                            \State $size \gets size - 1$
                            \State $req.isCompleted \gets $ true
                        \EndIf
                    \EndIf
                \EndIf
                \State $id \gets (id + 1) \bmod workers$ \Comment{Round-robin to next slot}
            \EndWhile
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{\acf{wCQ}}\label{subsubsec:wcq}
The \ac{wCQ} also uses the fast-path-slow-path by Kogan and Petrank methodology where threads first attempt lock-free operations and fall back to a slow path with helping mechanisms to achieve bounded completion of threads. As shown in \cref{alg:wcq-ops,alg:wcq-helper,alg:wcq-structures}, \ac{wCQ} extends the lock-free \ac{sCQ} shown in \cref{alg:scq-deq,alg:scq-enq} with a variation of Kogan and Petrank's fast-path-slow-path methodology to guarantee wait-freedom. Like \ac{sCQ}, \ac{wCQ} uses a ring buffer of size $2n$ whilst only using $n$ entries at any time, with Head and Tail counters initialised to $2n$ (as in line 32 of \cref{alg:wcq-structures}). For producers in \cref{alg:wcq-ops}, threads first help others via \texttt{HELP\_THREADS} in line 37. The fast path from lines 38 to 44 attempts \ac{sCQ}'s \texttt{TRY\_ENQ} function of \cref{alg:scq-enq} up to \texttt{MAX\_PATIENCE} times. If unsuccessful, the slow path begins in line 45. The thread records its request in a per-thread descriptor containing the tail value, index, and sequence numbers for integrity checks in lines 46 to 53. It then calls \texttt{ENQUEUE\_SLOW} in line 54, which repeatedly attempts to insert the element using collaborative synchronisation until successful. For consumers in \cref{alg:wcq-ops}, after checking for empty queue at lines 2 to 4 and helping others at line 5, threads attempt the fast path using \texttt{TRY\_DEQ} defined in lines 6 to 13 in \cref{alg:scq-deq}. On failure, they record their dequeue request in lines 14 to 21 and call \texttt{DEQUEUE\_SLOW} in line 22, which similarly uses collaborative synchronisation to ensure the dequeue completes. Results are gathered from the slow path in lines 25 to 33. The difference to the other queues is the \texttt{SLOW\_F\&A} operation in \cref{alg:wcq-helper} at lines 17 to 39, which ensures all cooperative threads (helpee and helpers) increment global counters only once per iteration. The operation works in two phases as seen in line 32 where it atomically updates the global counter with a phase2 pointer using \ac{DWCAS} (the paper mistakenly calls this \ac{DCAS} (CAS2)), and line 35 where it clears the \texttt{INC} flag. If the system does not support \ac{DWCAS}, \cref{alg:wcq-cas2-llsc} shows how to substitute it with \ac{LL/SC}, which can then again be substituted with versioned \ac{CAS} shown in \cref{subsub:jayanti-mpsc-queue}. This mechanism allows \texttt{ENQUEUE\_SLOW} and \texttt{DEQUEUE\_SLOW} to coordinate multiple threads working on the same operation, ensuring exactly one succeeds whilst others detect the completion and terminate. Progress is guaranteed through the helping mechanism. \texttt{HELP\_THREADS} in lines 1 to 15 of \cref{alg:wcq-helper} checks one thread per call, cycling through all threads. When finding a pending request, it calls \texttt{HELP\_ENQUEUE} or \texttt{HELP\_DEQUEUE} in lines 8 and 10. After \texttt{MAX\_PATIENCE} failed attempts, all threads eventually converge to help stuck threads, ensuring wait-freedom with $O(N_{threads}^2)$ complexity. \cite{wCQWaitFreeQueue}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{wCQ's Operations \cite{wCQWaitFreeQueue}}
    \label{alg:wcq-ops}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{Dequeue\_wCQ}{}
            \If{\Call{Load}{$\&Threshold$} $< 0$}
                \State \Return $\emptyset$ \Comment{Empty}
            \EndIf
            \State \Call{help\_threads}{}
            \State \Comment{Fast path (SCQ)}
            \State $count \gets MAX\_PATIENCE$
            \While{$--count \neq 0$}
                \State $idx$
                \State $head \gets $ \Call{try\_deq}{$\&idx$}
                \If{$head = OK$} \Return $idx$
                \EndIf
            \EndWhile
            \State \Comment{Slow path (wCQ)}
            \State $r \gets \&Record[TID]$
            \State $seq \gets r.seq1$
            \State $r.localHead \gets head$
            \State $r.initHead \gets head$
            \State $r.enqueue \gets $ \textbf{false}
            \State $r.seq2 \gets seq$
            \State $r.pending \gets $ \textbf{true}
            \State \Call{dequeue\_slow}{$head, r$}
            \State $r.pending \gets $ \textbf{false}
            \State $r.seq1 \gets seq + 1$
            \State \Comment{Get slow-path results}
            \State $h \gets $ \Call{Counter}{$r.localHead$}
            \State $j \gets $ \Call{Cache\_Remap}{$h \bmod 2n$}
            \State $Ent \gets $ \Call{Load}{$\&Entry[j].Value$}
            \If{$Ent.Cycle = $ \Call{Cycle}{$h$} \textbf{and} $Ent.Index \neq \bot$}
                \State \Call{consume}{$h, j, Ent$}
                \State \Return $Ent.Index$
            \EndIf
            \State \Return $\emptyset$
        \EndFunction
        \State
        \Function{Enqueue\_wCQ}{$index$}
            \State \Call{help\_threads}{}
            \State \Comment{Fast path (SCQ)}
            \State $count \gets MAX\_PATIENCE$
            \While{$--count \neq 0$}
                \State $tail \gets $ \Call{try\_enq}{$index$}
                \If{$tail = OK$} \Return \textbf{true}
                \EndIf
            \EndWhile
            \State \Comment{Slow path (wCQ)}
            \State $r \gets \&Record[TID]$
            \State $seq \gets r.seq1$
            \State $r.localTail \gets tail$
            \State $r.initTail \gets tail$
            \State $r.index \gets index$
            \State $r.enqueue \gets $ \textbf{true}
            \State $r.seq2 \gets seq$
            \State $r.pending \gets $ \textbf{true}
            \State \Call{enqueue\_slow}{$tail, index, r$}
            \State $r.pending \gets $ \textbf{false}
            \State $r.seq1 \gets seq + 1$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{wCQ's Helper Functions \cite{wCQWaitFreeQueue}}
    \label{alg:wcq-helper}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{help\_threads}{}
            \State $r \gets \&Record[TID]$
            \If{$--r.nextCheck \neq 0$} \Return
            \EndIf
            \State $thr \gets \&Record[r.nextTid]$
            \If{$thr.pending$}
                \If{$thr.enqueue$}
                    \State \Call{help\_enqueue}{$thr$}
                \Else
                    \State \Call{help\_dequeue}{$thr$}
                \EndIf
            \EndIf
            \State $r.nextCheck \gets HELP\_DELAY$
            \State $r.nextTid \gets (r.nextTid + 1) \bmod NUM\_THRDS$
        \EndFunction
        
        \State
        
        \Function{slow\_F\&A}{$globalp, local, v, thld$}
            \State $phase2 \gets \&Record[TID].phase2$
            \Repeat
                \State $cnt \gets $ \Call{load\_global\_help\_phase2}{$globalp, local$}
                \If{$cnt = \emptyset$ \textbf{or} !\Call{CAS}{$local, *v, cnt | INC$}}
                    \State $*v \gets *local$
                    \If{$*v \& FIN$} \Return \textbf{false}
                    \EndIf
                    \If{!$($$*v \& INC$$)$} \Return \textbf{true}
                    \EndIf
                    \State $cnt \gets $ \Call{Counter}{$*v$}
                \Else
                    \State $*v \gets cnt | INC$
                \EndIf
                \State \Call{prepare\_phase2}{$phase2, local, cnt$}
            \Until{\Call{CAS2}{$globalp, \{cnt, $ \textbf{null}$\}, \{cnt + 1, phase2\}$}}
            \If{$thld$} \Call{F\&A}{$thld, -1$}
            \EndIf
            \State \Call{CAS}{$local, cnt | INC, cnt$}
            \State \Call{CAS2}{$globalp, \{cnt + 1, phase2\}, \{cnt + 1, $ \textbf{null}$\}$}
            \State $*v \gets cnt$
            \State \Return \textbf{true}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{wCQ's Data Structures \cite{wCQWaitFreeQueue}}
    \label{alg:wcq-structures}
    \scriptsize
    \begin{algorithmic}[1]
        \State \textbf{struct} $phase2rec\_t$ \{
        \State \hspace{1em} $seq1$ : int = 1
        \State \hspace{1em} $*local$ : int
        \State \hspace{1em} $cnt$ : int
        \State \hspace{1em} $seq2$ : int = 0
        \State \}
        \State
        \State \textbf{struct} $entpair\_t$ \{
        \State \hspace{1em} $Note$ : int = -1
        \State \hspace{1em} $Value$ : $entry\_t$ = \{ .Cycle=0, .IsSafe=1, .Enq=1, .Index=$\bot$ \}
        \State \}
        \State
        \State $Entry[2n]$ : $entpair\_t$
        \State
        \State \textbf{struct} $thrdrec\_t$ \{
        \State \hspace{1em} \Comment{=== Private Fields ===}
        \State \hspace{1em} $nextCheck$ : int = $HELP\_DELAY$
        \State \hspace{1em} $nextTid$ : int \Comment{Thread ID}
        \State \hspace{1em} \Comment{=== Shared Fields ===}
        \State \hspace{1em} $phase2$ : $phase2rec\_t$ \Comment{Phase 2}
        \State \hspace{1em} $seq1$ : int = 1
        \State \hspace{1em} $enqueue$ : bool
        \State \hspace{1em} $pending$ : bool = false
        \State \hspace{1em} $localTail$, $initTail$ : int
        \State \hspace{1em} $localHead$, $initHead$ : int
        \State \hspace{1em} $index$ : int
        \State \hspace{1em} $seq2$ : int = 0
        \State \}
        \State
        \State $Record[NUM\_THRDS]$ : $thrdrec\_t$
        \State $Threshold$ : int = -1 \Comment{Empty wCQ}
        \State $Tail$ : int = $2n$, $Head$ : int = $2n$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{Lock-free Circular Queue (SCQ): Enqueue Operations \cite{wCQWaitFreeQueue}}
    \label{alg:scq-enq}
    \scriptsize
    \begin{algorithmic}[1]
        \State $Threshold$ : int = -1 \Comment{Empty SCQ}
        \State $Tail$ : int = $2n$, $Head$ : int = $2n$
        \State \Comment{Init entries: \{.Cycle=0, .IsSafe=1, .Index=$\bot$\}}
        \State $Entry[2n]$ : $entry\_t$
        \State
        \Function{Enqueue\_SCQ}{$index$}
            \While{\Call{try\_enq}{$index$} $\neq$ OK}
                \State \Comment{Try again}
            \EndWhile
        \EndFunction
        \State
        \Function{try\_enq}{$index$}
            \State $T \gets$ \Call{F\&A}{$\&Tail, 1$}
            \State $j \gets$ \Call{Cache\_Remap}{$T \bmod 2n$}
            \State $E \gets$ \Call{Load}{$\&Entry[j]$}
            \If{$E.Cycle < $ \Call{Cycle}{$T$} \textbf{and} ($E.IsSafe$ \textbf{or} \Call{Load}{$\&Head$} $\leq T$) \textbf{and} ($E.Index = \bot$ \textbf{or} $\bot_c$)}
                \State $New \gets \{$\Call{Cycle}{$T$}$, 1, index\}$
                \If{!\Call{CAS}{$\&Entry[j], E, New$}}
                    \State \textbf{goto} 22
                \EndIf
                \If{\Call{Load}{$\&Threshold$} $\neq 3n-1$}
                    \State \Call{Store}{$\&Threshold, 3n-1$}
                \EndIf
                \State \Return OK \Comment{Success}
            \EndIf
            \State \Return $T$ \Comment{Try again}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{Lock-free Circular Queue (SCQ): Dequeue Operations \cite{wCQWaitFreeQueue}}
    \label{alg:scq-deq}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{try\_deq}{$*index$}
        \State $H \gets$ \Call{F\&A}{$\&Head, 1$}
        \State $j \gets$ \Call{Cache\_Remap}{$H \bmod 2n$}
        \State $E \gets$ \Call{Load}{$\&Entry[j]$}
        \If{$E.Cycle = $ \Call{Cycle}{$H$}}
            \State \Call{consume}{$H, j, E$}
            \State $*index \gets E.Index$
            \State \Return OK \Comment{Success}
        \EndIf
        \State $New \gets \{E.Cycle, 0, E.Index\}$
        \If{$E.Index = \bot$ \textbf{or} $\bot_c$}
            \State $New \gets \{$\Call{Cycle}{$H$}$, E.IsSafe, \bot\}$
        \EndIf
        \If{$E.Cycle < $ \Call{Cycle}{$H$}}
            \If{!\Call{CAS}{$\&Entry[j], E, New$}}
                \State \textbf{goto} 33
            \EndIf
            \State $T \gets$ \Call{Load}{$\&Tail$} \Comment{Exit if}
            \If{$T \leq H + 1$} \Comment{empty}
                \State \Call{catchup}{$T, H + 1$}
            \EndIf
            \State \Call{F\&A}{$\&Threshold, -1$}
            \State $*index \gets \emptyset$ \Comment{Empty}
            \State \Return OK \Comment{Success}
        \EndIf
        \If{\Call{F\&A}{$\&Threshold, -1$} $\leq 0$}
            \State $*index \gets \emptyset$ \Comment{Empty}
            \State \Return OK \Comment{Success}
        \EndIf
        \State \Return $H$ \Comment{Try again}
    \EndFunction
    \State
    \Function{Dequeue\_SCQ}{}
        \If{\Call{Load}{$\&Threshold$} $< 0$}
            \State \Return $\emptyset$ \Comment{Empty}
        \EndIf
        \While{\Call{try\_deq}{$\&idx$} $\neq$ OK}
            \State \Comment{Try again}
        \EndWhile
        \State \Return $idx$
    \EndFunction
    \State
    \Function{consume}{$h, j, e$}
        \State \Call{OR}{$\&Entry[j], \{0, 0, \bot_c\}$}
    \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
    \centering
    \captionsetup{justification=centering}
    \caption{CAS2 implementation for wCQ using LL/SC \cite{wCQWaitFreeQueue}}
    \label{alg:wcq-cas2-llsc}
    \scriptsize
    \begin{algorithmic}[1]
        \Function{CAS2\_Value}{$Var$: $entpair\_t$*, $Expect$: $entpair\_t$, $New$: $entpair\_t$}
            \State $Prev.Value \gets$ \Call{LL}{$\&Var \rightarrow Value$}
            \State $Prev.Note \gets$ \Call{Load}{$\&Var \rightarrow Note$}
            \If{$Prev \neq Expect$}
                \State \Return \textbf{false}
            \EndIf
            \State \Return \Call{SC}{$\&Var \rightarrow Value, New.Value$}
        \EndFunction
        
        \State
        
        \Function{CAS2\_Note}{$Var$: $entpair\_t$*, $Expect$: $entpair\_t$, $New$: $entpair\_t$}
            \State $Prev.Note \gets$ \Call{LL}{$\&Var \rightarrow Note$}
            \State $Prev.Value \gets$ \Call{Load}{$\&Var \rightarrow Value$}
            \If{$Prev \neq Expect$}
                \State \Return \textbf{false}
            \EndIf
            \State \Return \Call{SC}{$\&Var \rightarrow Note, New.Note$}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsubsection{Excluded but valuable queues}
The following two queues were not included because they rely on theoretical hardware primitives that are not available in current hardware. However, they are valuable to know about, since they show how the performance of wait-free queues could be improved with special hardware.
\begin{itemize}
    \item The queue Khanchandani and Wattenhofer \cite{halfincrementhalfmax} created uses theoretical atomic primitives called half-increment and half-max. Half-increment would be an operation on a theoretical register with two values, first half and second half, that increments the first half if $\leq$ second half. Half-max(x) updates the second half to the maximum of its current value and x. The reason this was even introduced was to show that, when combined with \ac{CAS}, the time complexity of \ac{CAS} with $O(n)$ would be reduced to $O(\sqrt{n})$, if such hardware would exist.
    \item Bédin et al. \cite{memorytomemory} who created a queue using a theoretical atomic primitive called memory-to-memory swap, which changes two memory locations atomically with each other (so a \ac{DCAS} without the compare part), to show that it would be possible to create wait-free queues as fast as lock-free queues, if the hardware would exist. Whilst this is valuable to know how special hardware could improve the performance of wait-free queues, it is not relevant for this thesis, since it is not possible to implement these algorithms on current hardware.
\end{itemize}
This algorithm was not included in the thesis, because it was running too slow to create valuable benchmark results, but is still important to mention, since it shows an interesting improvement for time complexity of wait-free queues:
\begin{itemize}
    \item Naderibeni and Ruppert \cite{polylog} showed that it is possible to create a wait-free queue using \ac{CAS} and still achieve a polylogarithmic time complexity by integrating a binary tree structure like \ac{JPQ}. Later in \cref{ch:results} it is seen that even though polylogarithmic, a binary tree structure is not suitable for \ac{IPC} over shared memory.
\end{itemize}