\chapter{Analyzing existing Wait-Free Data Structures and Algorithms}\label{ch:choosing-the-optimal-wait-free-data-structure}

\section{Optimal Wait-Free Data Structure}\label{sec:optimal-wait-free-data-structure}

An important question is what data structure to use for the implementation of a wait-free synchronisation technique for \ac{IPC}. M. Herlihy showed that every sequential data structure can be made wait-free \cite{herlihy1991wait}. So it is important to choose the optimal data structure for our use. Considering that the reason of this work is to optimize modern manufacturing and automation, some form of correct data flow order is as well necessary for correct work flow for instance in an modern manufacturing line or more critical in a driverless car. Hence an already natural fit like \ac{FIFO} queues. Natural because in such queues a producer process can enqueue messages and the consumer process can dequeue messages sequentially. This models real-world data flows (sensor readings, commands, network packets), which are inherently sequential. Consequently with such queues the order of the data flow is preversed without the need of implementing additional functionalities. In contrast, data structures like stacks, sets, or maps do not maintain this kind of arrival order and moreover add semantics like \ac{LIFO} order or key-value pairs, which are in most cases not desired or even unnecessary. This would bring in the need of additional functions to just get rid of undesired side effects. Furthermore in a queue only two operations exist, an enqueue and an dequeue operation. All the other data structures introduce more operations and therefore more complexity and therefore more performance overhead. The less operations exist, the less complex the implementation will be. Because of these advantages and also because of the fact that in most publications in the wait-free domain queues are beeing used, limiting this thesis to queues only is reasonable. \cite{jiffy}

\section{Wait-Free Algorithms}\label{sec:wait-free-alg}
With the appropriate data structure established, an important consideration is the selection of suitable algorithms. In \cref{ch:methodology} 4 different contention categories are defined. Which kind of algorithm going to be used will be decided contention based. Since all of them have different complexity in runtime, it is important to choose the right contention category for the right use case to save resources and have faster execution times to meet the timing constraints of \ac{HRTS}. In modern manufacturing and automation devices are used which can run multiple applications on a single device. This could mean that every application running on one device could be a producer and a consumer to each other (\ac{MPMC}) and also maybe some single application of all applications running on one device produces data for just a single other consuming application (\ac{SPSC}). And maybe some single application is a producer for multiple consuming applications (\ac{SPMC}) and multiple applications are producers of a single consuming application (\ac{MPSC}). So it can be that all cases can occur in just one device. This means that all the different cases of contention have to be considered. In the following the different cases and their algorithms will be discussed. Moreover they will be implemented and their performance tested via a benchmarked (how fast an algorithm can produce and consume items concurrently). Subsequently from each category the best algorithm will be chosen and their performance will be compared with each other to Identify, if 4 different categories are necessary. The reason for that is, that for instance the best performed \ac{MPMC} algorithm could outperform all other algorithms even for their contention category, since a \ac{MPMC} approach can cover all contention cases. The goal with this approach is to have as little overhead as possible, since an algorithm explicitly implemented for a \ac{MPMC} use case could have extreme overhead for a \ac{SPSC} case. The implementation will be discussed in \cref{ch:implementation} and the results of the performance readings will be discussed in \cref{ch:results}. The following subsections will give an overview of the different contention categories and their algorithms found. The subsections will also shortlyy describe how the enqueue and dequeue in these algorithms work. Given that all algorithms found are about inter-thread communication and not \ac{IPC}, the following explanation of the specific algorithms will include the terminology of threads to be precise about the papers. Later it will be explained how this was adapted to \ac{IPC} in \cref{ch:implementation}.

\subsection{Single Producer and Single Consumer}\label{subsec:single-producer-and-single-consumer}
This is the most simple form of \ac{IPC}. In \ac{SPSC} there is nearly no contention from other processes, because only one producer and one consumer is working. The only contention is between the consumer and producer. In this subsection the focus will lie on benchmarking all the different \ac{SPSC} approaches found so that the optimal algorithm for the \ac{SPSC} use case can be selected. The following list shows the algorithms that are used for the \ac{SPSC} contention case. Since \ac{BLQ}, \ac{LLQ}, \ac{BIFFQ} and \ac{IFFQ} are from the same paper, explanations and variables are shared between these algorithms to avoid redundancy:
\subsubsection{Lamport's Queue}
Uses a circular array with two shared indices for synchronization, based on the algorithm originally proposed by Leslie Lamport in 1983 \cite{Lamport1983SPSCCircularBuffer} and shown here in \cref{alg:lamport-queue} following Maffione et al.'s version \cite{MaffioneCacheAware}. The producer first checks if the queue is full by computing \texttt{write - read = N}, which requires reading the consumer's \texttt{read} index. If the queue is not full, it writes the input data to the slot at position \texttt{write} and increments \texttt{write} to signal that the written data is available. The consumer mirrors this behavior by checking if the queue is empty with \texttt{read = write} by reading the producer's \texttt{write} index. If the queue is not empty the consumer reads that data from the slot at position \texttt{read} and increments its \texttt{read} index to signal that the slot is available. Each operation requires accessing both shared indices plus the data slot, causing up to three cache misses per item when the queue moves between nearly empty and nearly full states, which reduces performance. According to Ulrich Drepper in 2007 cache misses occur when requested data is not in the local CPU core's cache and must be fetched from another core's cache, with the cache coherence protocol ensuring memory consistency across all cores \cite{drepper2007every}. Drepper showed that the performance can degrade by 390\%, 734\%, and 1,147\% for 2, 3, and 4 threads respectively and happens because cache lines, the 64-byte blocks (on x86 architecures) that move between CPU caches, ping-pong between the producer's and consumer's cores as they take turns accessing the same memory locations \cite{drepper2007every}.

\begin{algorithm}[!ht]
   \centering
   \captionsetup{justification=centering}
   \caption{Lamport Queue Operations \cite{MaffioneCacheAware}}
   \label{alg:lamport-queue}
   \scriptsize
   \begin{algorithmic}[1]
       \Function{lq\_enqueue}{$q$, $e$}
           \If{$q.write - q.read = N$} \Comment{Check if full}
               \State \Return $-1$ \Comment{No space}
           \EndIf
           \State $q.slots[q.write \land q.mask] \gets e$
           \State \texttt{store\_release\_barrier()}
           \State $q.write \gets q.write + 1$
           \State \Return $0$
       \EndFunction
       
       \State
       
       \Function{lq\_dequeue}{$q$}
           \If{$q.read = q.write$} \Comment{Check if empty}
               \State \Return \texttt{NULL\_ELEM} \Comment{Queue empty}
           \EndIf
           \State \texttt{load\_acquire\_barrier()}
           \State $e \gets q.slots[q.read \land q.mask]$
           \State $q.read \gets q.read + 1$
           \State \Return $e$
       \EndFunction
   \end{algorithmic}
   \cite{MaffioneCacheAware}
\end{algorithm}

\subsubsection{\acf{LLQ}}
Reduces the described cache misses of Lamports queue by postponing index reads until necessary, as shown in \cref{alg:llq}. The producer maintains a local \texttt{read\_shadow} copy and only updates it when running out of known free slots. Similarly, the consumer uses \texttt{write\_shadow} to avoid repeatedly checking for new items. Additionally, LLQ keeps $K$ slots (where $K$ is slots per cache line, typically 8) permanently empty, preventing producer and consumer from touching the same cache line when the queue is full. These optimizations work particularly well when one thread is faster than the other, reducing worst-case misses from 3 to about 2 per item. \cite{MaffioneCacheAware}

\begin{algorithm}[!ht]
   \centering
   \captionsetup{justification=centering}
   \caption{Lazy Lamport Queue Operations \cite{MaffioneCacheAware}}
   \label{alg:llq}
   \scriptsize
   \begin{algorithmic}[1]
       \Function{llq\_enqueue}{$q$, $e$}
           \If{$q.write - q.read\_shadow = N - K$} \Comment{Lazy load check}
               \State $q.read\_shadow \gets q.read$ \Comment{Update shadow}
               \If{$q.write - q.read\_shadow = N - K$}
                   \State \Return $-1$ \Comment{No space}
               \EndIf
           \EndIf
           \State $q.slots[q.write \land q.mask] \gets e$
           \State \texttt{store\_release\_barrier()}
           \State $q.write \gets q.write + 1$
           \State \Return $0$
       \EndFunction
       
       \State
       
       \Function{llq\_dequeue}{$q$}
           \If{$q.read = q.write\_shadow$} \Comment{Lazy load check}
               \State $q.write\_shadow \gets q.write$ \Comment{Update shadow}
               \If{$q.read = q.write\_shadow$}
                   \State \Return \texttt{NULL\_ELEM}
               \EndIf
           \EndIf
           \State \texttt{load\_acquire\_barrier()}
           \State $e \gets q.slots[q.read \land q.mask]$
           \State $q.read \gets q.read + 1$
           \State \Return $e$
       \EndFunction
   \end{algorithmic}
\end{algorithm}

\subsubsection{\acf{BLQ}}
Extends LLQ with explicit cache batching to further reduce synchronization costs, as detailed in \cref{alg:blq}. The producer accumulates items using private \texttt{write\_priv}, filling slots without updating the shared \texttt{write} index. Only \texttt{blq\_enqueue\_publish} makes the batch visible by advancing \texttt{write}. The consumer works symmetrically, using \texttt{read\_priv} for local progress before updating \texttt{read}. With typical batch sizes like $B = 32$, synchronization overhead is amortized across many operations, reducing the cache misses. This particularly benefits applications that naturally process data in batches, such as network packet processing. \cite{MaffioneCacheAware}

\begin{algorithm}[!ht]
   \centering
   \captionsetup{justification=centering}
   \caption{Batched Lamport Queue Operations \cite{MaffioneCacheAware}}
   \label{alg:blq}
   \scriptsize
   \begin{algorithmic}[1]
       \Function{blq\_enqueue\_space}{$q$, $needed$}
           \State $space \gets N - K - (q.write\_priv - q.read\_shadow)$
           \If{$space < needed$}
               \State $q.read\_shadow \gets q.read$ \Comment{Update shadow}
               \State $space \gets N - K - (q.write\_priv - q.read\_shadow)$
           \EndIf
           \State \Return $space$
       \EndFunction
       
       \State
       
       \Function{blq\_enqueue\_local}{$q$, $e$}
           \State $q.slots[q.write\_priv \land q.mask] \gets e$
           \State $q.write\_priv \gets q.write\_priv + 1$
       \EndFunction
       
       \State
       
       \Function{blq\_enqueue\_publish}{$q$}
           \State \texttt{store\_release\_barrier()}
           \State $q.write \gets q.write\_priv$
       \EndFunction
       
       \State
       
       \Function{blq\_dequeue\_space}{$q$}
           \State $available \gets q.write\_shadow - q.read\_priv$
           \If{$available = 0$}
               \State $q.write\_shadow \gets q.write$ \Comment{Update shadow}
               \State $available \gets q.write\_shadow - q.read\_priv$
           \EndIf
           \State \Return $available$
       \EndFunction
       
       \State
       
       \Function{blq\_dequeue\_local}{$q$}
           \State \texttt{load\_acquire\_barrier()}
           \State $e \gets q.slots[q.read\_priv \land q.mask]$
           \State $q.read\_priv \gets q.read\_priv + 1$
           \State \Return $e$
       \EndFunction
       
       \State
       
       \Function{blq\_dequeue\_publish}{$q$}
           \State $q.read \gets q.read\_priv$
       \EndFunction
   \end{algorithmic}
\end{algorithm}

\subsubsection{\ac{FFQ}}
Synchronization by embedding control information directly within the data slots, eliminating separate shared indices. Instead of checking head or tail variables, the producer writes data to slots and marks them as full, while the consumer reads data and marks slots as empty using a \enquote*{NULL} value. This coupling of control and data means each operation touches only one shared cache line instead of three. The producer maintains a private \enquote*{write} index tracking the next slot to fill, checking if it contains \enquote*{NULL} before writing. The consumer keeps a private \enquote*{read} index, checking if slots contain valid data before reading and clearing them. While this eliminates memory barriers and reduces cache misses from 3 to at most 2 per item, it still has a problem. When producer and consumer work on the same cache line, the cache line moves back and forth between CPU cores worsening execution times. \cite{ffq}

\subsubsection{\acf{IFFQ}}
Prevents cache conflicts through spatial separation using a look-ahead mechanism shown in \cref{alg:iffq}. The producer checks if a slot $H$ positions ahead (typically 32) is empty before proceeding, ensuring it works far ahead of the consumer. This check happens only once every $H$ items when \texttt{write} reaches \texttt{limit}. The consumer delays clearing slots through \texttt{iffq\_dequeue\_publish}, maintaining separation between producer and consumer regions. With $2H$ permanently unused slots as a buffer zone, producer and consumer operate on different cache lines in steady state, reducing cache misses to nearly optimal levels. \cite{MaffioneCacheAware}

\begin{algorithm}[!ht]
   \centering
   \captionsetup{justification=centering}
   \caption{Improved FastForward Queue Operations\cite{MaffioneCacheAware}}
   \label{alg:iffq}
   \scriptsize
   \begin{algorithmic}[1]
       \Function{iffq\_enqueue}{$q$, $e$}
           \If{$q.write = q.limit$} \Comment{Check limit}
               \State $next\_limit \gets q.limit + H$
               \If{$q.slots[next\_limit \land q.mask] \neq \texttt{NULL\_ELEM}$}
                   \State \Return $-1$ \Comment{No space}
               \EndIf
               \State $q.limit \gets next\_limit$ \Comment{Free partition}
           \EndIf
           \State $q.slots[q.write \land q.mask] \gets e$
           \State $q.write \gets q.write + 1$
           \State \Return $0$
       \EndFunction
       
       \State
       
       \Function{iffq\_dequeue\_local}{$q$}
           \State $e \gets q.slots[q.read \land q.mask]$
           \If{$e = \texttt{NULL\_ELEM}$}
               \State \Return \texttt{NULL\_ELEM}
           \EndIf
           \State $q.read \gets q.read + 1$
           \State \Return $e$
       \EndFunction
       
       \State
       
       \Function{iffq\_dequeue\_publish}{$q$}
           \While{$q.clear \neq$ next\_clear$(q.read)$}
               \State $q.slots[q.clear \land q.mask] \gets \texttt{NULL\_ELEM}$
               \State $q.clear \gets q.clear + 1$
           \EndWhile
       \EndFunction
   \end{algorithmic}
\end{algorithm}

\subsubsection{\acf{BIFFQ}}
Addresses IFFQ's weakness when the queue is nearly empty by adding producer-side buffering, as shown in \cref{alg:biffq}. Items first accumulate in a thread-local buffer, then \newline \texttt{biffq\_enqueue\_publish} writes them to the queue in a rapid burst. This creates a beneficial race condition - if all writes complete before the consumer notices, the cache line stays with the producer, avoiding ping-pong effects. The consumer side remains unchanged from IFFQ. While theoretical worst-case behavior is similar to IFFQ, practical measurements show significant improvement when the queue operates near empty, making BIFFQ effective across all operating conditions. \cite{MaffioneCacheAware}

\begin{algorithm}[!ht]
   \centering
   \captionsetup{justification=centering}
   \caption{Batched Improved FastForward Queue Operations \cite{MaffioneCacheAware}}
   \label{alg:biffq}
   \scriptsize
   \begin{algorithmic}[1]
       \Function{biffq\_wspace}{$q$, $needed$}
           \State $space \gets q.limit - q.write$
           \If{$space < needed$}
               \State \Return $space$ \Comment{Force limit update}
           \EndIf
           \State \Return $space$
       \EndFunction
       
       \State
       
       \Function{biffq\_enqueue\_local}{$q$, $e$}
           \State $q.buf[q.buffered] \gets e$ \Comment{Store in buffer}
           \State $q.buffered \gets q.buffered + 1$
       \EndFunction
       
       \State
       
       \Function{biffq\_enqueue\_publish}{$q$}
           \For{$i \gets 0$ \textbf{to} $q.buffered - 1$}
               \State $q.slots[q.write \land q.mask] \gets q.buf[i]$ \Comment{Fast burst}
               \State $q.write \gets q.write + 1$
           \EndFor
           \State $q.buffered \gets 0$
           \State $q.limit \gets q.write + H$ \Comment{Update limit}
       \EndFunction
   \end{algorithmic}
\end{algorithm}

\subsubsection{B-Queue}
Addresses cache thrashing through adaptive batching with a backtracking mechanism for deadlock prevention. The producer maintains local head and \enquote*{batch\_head} pointers, probing \enquote*{BATCH\_SIZE} slots ahead to reserve a block of empty slots when needed. Elements are written sequentially with an associated validity flag set via release ordering. The consumer uses tail and \enquote*{batch\_tail} pointers to extract elements in batches, clearing validity flags after reading. When insufficient elements are available for a full batch, the backtracking algorithm performs binary search by repeatedly halving the batch size to find available data, preventing deadlock without auxiliary threads or timers. This approach maintains producer-consumer separation across cache lines while gracefully handling varying production rates. \cite{Wang2013BQueue}

\subsubsection{\ac{uSPSC}}
Combines bounded SPSC buffers into a queue using a pool of circular buffers linked as a list. The producer writes to the current buffer until full, then requests a new buffer from the pool via the \enquote*{next\_w()} method, updates the write pointer, and continues. The consumer reads from its current buffer, and when empty, checks if more buffers exist by comparing the read and write pointers. If buffers differ, it switches to the next buffer obtained via \enquote*{next\_r()} and releases the empty one back to the pool for recycling. A write memory barrier ensures all data writes complete before buffer transitions, preventing data loss on weakly ordered memory models. \cite{torquati2010singleproducersingleconsumerqueuessharedcache,Aldinucci2012EfficientSync}

\subsubsection{\ac{dSPSC}}
Implements a queue through a linked list where each node contains data and a next pointer. The producer obtains nodes from a pool, fills them with data, inserts a write memory barrier to ensure data visibility, then updates the tail pointer to link the node. The consumer reads from the head node, extracts data, advances the head pointer, and returns the consumed node to the pool. To minimize overhead, freed nodes are cached in a bounded SPSC queue that acts as a return path from consumer to producer, enabling efficient node reuse. \cite{torquati2010singleproducersingleconsumerqueuessharedcache,Aldinucci2012EfficientSync}

\subsubsection{\ac{mSPSC}}
Reduces cache-line thrashing in Lamport's circular buffer by batching multiple elements before insertion. The producer accumulates items in a thread-local array, then writes them to the SPSC buffer in reverse order using the multipush method when the batch fills or flush is called. This backward insertion maintains distance between read and write pointers across cache lines, preventing false sharing. While adding an extra copy per element, the improved cache utilization from reduced coherence traffic more than compensates for this overhead, achieving better performance then standard push operations. \cite{torquati2010singleproducersingleconsumerqueuessharedcache}

\subsection{Multiple Producer and Single Consumer}\label{subsec:multiple-producer-and-single-consumer}
This is a bit more complex to implement than the \ac{SPSC} case. Multiple producers can enqueue items at the same time while a single consumer dequeues items. Also here multiple approaches are available from different papers. These are the algorithms that found for this case: 
\begin{itemize}
   \item Jayanti Queue: Distributes the global queue across n local queues per producer. Producers obtain timestamps via atomic counter reads, insert [element, timestamp] pairs into their local queues, then propagate front elements up a binary tree of minimum values (propagating means updating values level by level from leaves toward the root). The consumer reads the root to find the earliest timestamp, dequeues from that local queue, and propagates changes back up the tree. Internal nodes maintain minimum timestamps of their subtrees, enabling O(log n) operations using \ac{LL/SC} (Since the hardware used for this work was accomplished on x86, \ac{LL/SC} was substituted with \ac{CAS} linked with version numbers to avoid ABA). \cite{JayantiLog}
   \item Drescher Queue: Uses a linked list with dummy node recycling to achieve wait-freedom. Producers atomically swap the tail pointer via \ac{FAS} to append nodes, then link the previous tail to the new node. The consumer advances the head pointer and recycles the dummy node by re-enqueueing it when encountered, ensuring the queue never becomes empty. Instead helping atomic pointer swaps are used. \cite{Drescher2015GuardedSections}
   \item Jiffy Queue: Maintains elements in a linked list of fixed-size arrays to minimize memory usage, but still have good execution times. Producers use \ac{FAA} to atomically claim indices, allocate new buffers via \ac{CAS} if their index exceeds current capacity, then traverse to their target buffer and write elements. When the consumer finds an empty head cell, it scans forward for completed entries, marking them as handled and folding away fully consumed buffers. The second enqueuer in each buffer preemptively allocates the next segment to reduce contention. \cite{jiffy}
   \item DQueue: Buffers enqueue requests in thread-local arrays before batch writing to shared segments. Producers obtain indices via \ac{FAA}, accumulate requests locally, then flush to shared memory when buffers fill, exploiting spatial locality. The consumer helps suspended producers by scanning all local buffers when finding empty cells. Key optimizations include eliminating \ac{CAS} operations to avoid write buffer drains and helping all producers indiscriminately. \cite{WangCacheCoherent}
\end{itemize}

\subsection{Single Producer and Multiple Consumer}\label{subsec:single-producer-and-multiple-consumer}
This case is trickier to implement, since multiple consumers have to be synchronized to not get any double read or loosing an item. Multiple producers was simpler, since making producers write specific data each is not so hard. In this case every consumer has to be synchronized so that when one item is consumed by a consumer, it cannot be consumed again from another. That is most probably also the reason only one algorithm for this contention category was found. No benchmark will be performed for this category, since there is no other algorithm to compete with in this category. The algorithm is the following:
\begin{itemize}
   \item David Queue: Uses a 2D array ITEMS where each cell is a Swap object, a 1D array HEAD where each element is an \ac{FAA} object, and a shared ROW register. The producer writes to consecutive cells in the current row of ITEMS and when the producer detects it gets overtaken by a consumer (reads a value indicating the cell was read), it jumps to the next row. Consumers read the active row from ROW, use \ac{FAA} on HEAD[row] to get a unique index, then swap the value, indicating the cell was read, into ITEMS[row, index] to retrieve the element. The algorithm achieves O(1) time complexity with 3-bounded wait-free operations. Row migration ensures linearizability by preventing consumers from accessing future write locations. (David assumes threads rather than processes like in the context of this work, requiring additional synchronization steps to avoid data loss (A recovery mechanism \cite{githubMA}).  Also instead of items, pointers were stored into the cells.). \cite{Mateíspmc}
\end{itemize}

\subsection{Multiple Producer and Multiple Consumer}\label{subsec:multiple-producer-and-multiple-consumer}
Finally a look into the \ac{MPMC} case can be made. Synchronizing \ac{MPMC} is a bit simpler then synchronizing \ac{SPMC}, because here each consumer consumes the items of one other producer instead of sharing the same pool of data that has to be consumed. These are the algorithms that were found:
\begin{itemize}
   \item  Kogan and Petranks queue: Uses a priority-based helping mechanism built on the Michael-Scott lock-free queue. Threads obtain phase numbers by calculating the maximum phase across all threads in a shared state array and adding one. Each thread records its operation details (phase, pending flag, operation type, node reference) in the state array at its thread ID index. During execution, threads traverse the state array and help all operations with phase numbers less than or equal to their own. Producers append nodes to the tail using \ac{CAS} then update the pending flag to false in the state array and then advance the tail pointer. Consumers write their ID into the deqTid field of the head node then update their state entry and then advance head. Empty queues are handled by checking if head equals tail with a null next pointer. The algorithm ensures each operation completes within O(n²) steps through systematic helping, where threads can only bypass each other a bounded number of times. Linearization occurs at the \ac{CAS} operations that modify the queue structure. \cite{Kogan2011WaitFreeQueues}
   \item Turn Queue: Uses a circular turn-based consensus mechanism. Producers publish nodes in \enquote*{enqueuers[tid]} array then the next turn is determined by tail's \enquote*{enqTid}, with all threads helping the first non-null request to the right (modulo the array size), guaranteeing completion within \enquote*{MAX\_THREADS} iterations. Consumers use dual arrays (\enquote*{deqself} and \enquote*{deqhelp}) where \enquote*{deqself[tid]} = \enquote*{deqhelp[tid]} opens a request. Threads assign nodes via \ac{CAS} on \enquote*{node.deqTid} and publish results in \enquote*{deqhelp[tid]} to close requests. Empty queues trigger the \enquote*{giveUp()} to rollback while preserving concurrent assignments. Linearization occurs at tail and head advances. \cite{RamalheteQueue}
   \item YMC queue: Uses an unbounded array (emulated via linked list of segments) with \ac{FAA}-based indexing and fast-path/slow-path design. Producers obtain slots via \ac{FAA} on tail counter and attempt \ac{CAS} insertion. On failure, the producers publish requests (\enquote*{value}, \enquote*{pending}, \enquote*{cell\_id}) and enter slow path. Helpers use Dijkstra's protocol—after marking cells unusable with top or buttom markers and they check for pending enqueue requests. Helpers traverse a peer ring, advancing only after completing requests, guaranteeing progress within (n-1)² failures. Consumers \ac{FAA} the head counter and call \enquote*{help\_enq} to secure values. Slow-path dequeues announce candidate cells with monotonically increasing indices via \ac{CAS} on request state. Linearizability requires enqueued values have indices < T (tail) and dequeued values have indices < H (head), maintained by \enquote*{advance\_end\_for\_linearizability} calls. Failed operations advance the helper ring until all threads become helpers ensuring all threads finish. \cite{FastFetchAndAddWaitFreeQueue}
   \item Feldman-Dechev Queue: Uses a ring buffer with sequence number distribution for contention management. Producers obtain a unique position via \ac{FAA} on the tail counter, receiving a sequence ID (\enquote*{seqid}) that determines their slot (\enquote*{seqid} modulo the capacity). They attempt to replace an \enquote*{EmptyNode} (containing only a \enquote*{seqid}) with a \enquote*{ValueNode} (containing both \enquote*{seqid} and data) via \ac{CAS}, retrying while temporarily pausing and rechecking if the slot's \enquote*{seqid} indicates a delayed operation. Consumers similarly use \ac{FAA} on the head counter and replace \enquote*{ValueNodes} with \enquote*{EmptyNodes} containing \enquote*{seqid} + \enquote*{capacity}. Both operations employ bitmarking to handle out-of-order completions meaning when encountering a delayed element, threads mark it with a delay bit to signal correction is needed. After exceeding \enquote*{MAX\_FAILS} attempts, operations switch to a slow path using an announcement table where threads publish operation descriptors. All threads periodically check this table (every \enquote*{CHECK\_DELAY} operations) and help announced operations, guaranteeing completion within \enquote*{MAX\_FAILS} + \enquote*{NUM\_THREADS²} steps. The sequence numbers additionally prevent ABA problems and enable progress despite random delays. \cite{FeldmanDechev2015WaitFreeRingBuffer,FeldmanDechevV2,FeldmanDechevV3}
   \item wCQ: Uses a circular array with fast and slow paths. Producers claim slots via \ac{FAA} on a tail counter, then attempt \ac{CAS} to insert if the slot's cycle number indicates availability. After exceeding a patience threshold, producers record their operation in per-thread descriptors and enter a slow path where helpers execute the same operation on behalf of a stuck thread. The slow path replaces uses a modified non-atomic \ac{FAA}, which is a distributed protocol that coordinates helpers through \enquote*{INC} (increment pending) and \enquote*{FIN} (finalized) bits in thread-local counters, ensuring all helpers converge to the same counter value and exactly one successfully increments the global counter. Consumers similarly use \ac{FAA} on the head counter and mark consumed entries with a special bottom value. Both operations use a two-step protocol meaning enqueuers initially insert with \enquote*{Enq=false}, then set \enquote*{Enq=true} after finalizing their request, allowing concurrent dequeuers to help finalize pending operations. The queue maintains 2n slots for n threads with note fields storing cycle numbers to prevent helpers from incorrectly modifying entries from previous cycles. \cite{wCQWaitFreeQueue}
   \item Verma Queue: Uses an helper that runs on a dedicated core and mediates all queue operations through a shared state array. Producers submit enqueue requests by writing [operation, element]tuples to their designated position in the state array indexed by a thread ID, then spin until the \enquote*{isCompleted} flag is set. The helper continuously traverses the state array in round-robin (a traversal algorithm not deeper explained in this work) order, processing pending requests by appending nodes to the tail of an underlying linked list for enqueues or removing from the head for dequeues. The traversal time is bounded (at most n iterations for n threads) so that every thread finishes in a bounded time, while contention is eliminated by serializing all queue changes through the single helper thread. The helper thread includes volatile variables to ensure cache coherence and padding to avoid false sharing between state array entries. \cite{Verma2013Scalable}
\end{itemize}
