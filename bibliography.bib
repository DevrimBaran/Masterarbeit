@article{herlihy1991wait,
	author = {Herlihy, Maurice},
	title = {Wait-free synchronization},
	year = {1991},
	issue_date = {Jan. 1991},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {13},
	number = {1},
	issn = {0164-0925},
	url = {https://doi.org/10.1145/114005.102808},
	doi = {10.1145/114005.102808},
	abstract = {A wait-free implementation of a concurrent data object is one that guarantees that any process can complete any operation in a finite number of steps, regardless of the execution speeds of the other processes. The problem of constructing a wait-free implementation of one data object from another lies at the heart of much recent work in concurrent algorithms, concurrent data structures, and multiprocessor architectures. First, we introduce a simple and general technique, based on reduction to a concensus protocol, for proving statements of the form, “there is no wait-free implementation of X by Y.” We derive a hierarchy of objects such that no object at one level has a wait-free implementation in terms of objects at lower levels. In particular, we show that atomic read/write registers, which have been the focus of much recent attention, are at the bottom of the hierarchy: thay cannot be used to construct wait-free implementations of many simple and familiar data types. Moreover, classical synchronization primitives such astest&set and fetch&add, while more powerful than read and write, are also computationally weak, as are the standard message-passing primitives. Second, nevertheless, we show that there do exist simple universal objects from which one can construct a wait-free implementation of any sequential object.},
	journal = {ACM Trans. Program. Lang. Syst.},
	month = jan,
	pages = {124–149},
	numpages = {26},
	keywords = {wait-free synchronization, linearization},
}

@conference{FeldmanDechevV2,
	author       = {Dechev, Damian and Feldman, Steven and Barrington, Andrew},
	title        = {A Scalable Multi-Producer Multi-Consumer Wait-Free Ring Buffer.},
	annote       = {Abstract not provided.},
	url          = {https://www.osti.gov/biblio/1531271},
	place        = {United States},
	organization = {Sandia National Lab. (SNL-NM), Albuquerque, NM (United States)},
	year         = {2015},
	month        = {04}}



@inproceedings{FeldmanDechevV3,
	author = {Barrington, Andrew and Feldman, Steven and Dechev, Damian},
	title = {A scalable multi-producer multi-consumer wait-free ring buffer},
	year = {2015},
	isbn = {9781450331968},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2695664.2695924},
	doi = {10.1145/2695664.2695924},
	abstract = {A ring buffer or cyclical queue is a First In, First Out (FIFO) queue that stores elements on a fixed-length array. This allows for efficient O(1) operations, cache-aware optimizations, and low memory overhead. Because ring buffers are limited to only the array and two counters they are desirable for systems with limited memory. Many applications (e.g. cloud-based services) depend on ring buffers to pass work from one thread to another. The rise in many-core architecture has resulted in increased performance from shared data structures such as ring buffers. Existing research has forgone the use of locks and permitted greater scalability and core utilization for such designs. Such non-blocking designs are categorized by the level of progress they guarantee with wait-freedom being the most desirable categorization. Such a guarantee provides freedom from deadlock, livelock, and thread starvation. Lock-free and obstruction-free designs are not safe from all of these pitfalls [5].},
	booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
	pages = {1321–1328},
	numpages = {8},
	keywords = {concurrent, non-blocking, parallel, queue, ring buffer, wait-free},
	location = {Salamanca, Spain},
	series = {SAC '15}
}


@article{kogan2012methodology,
	author = {Kogan, Alex and Petrank, Erez},
	title = {A methodology for creating fast wait-free data structures},
	year = {2012},
	issue_date = {August 2012},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {47},
	number = {8},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/2370036.2145835},
	doi = {10.1145/2370036.2145835},
	abstract = {Lock-freedom is a progress guarantee that ensures overall program progress. Wait-freedom is a stronger progress guarantee that ensures the progress of each thread in the program. While many practical lock-free algorithms exist, wait-free algorithms are typically inefficient and hardly used in practice. In this paper, we propose a methodology called fast-path-slow-path for creating efficient wait-free algorithms. The idea is to execute the efficient lock-free version most of the time and revert to the wait-free version only when things go wrong. The generality and effectiveness of this methodology is demonstrated by two examples. In this paper, we apply this idea to a recent construction of a wait-free queue, bringing the wait-free implementation to perform in practice as efficient as the lock-free implementation. In another work, the fast-path-slow-path methodology has been used for (dramatically) improving the performance of a wait-free linked-list.},
	journal = {SIGPLAN Not.},
	month = feb,
	pages = {141–150},
	numpages = {10},
	keywords = {wait-free queues, non-blocking synchronization, lock-free algorithms, concurrent data structures}
}

@article{timnat2014practical,
	author = {Timnat, Shahar and Petrank, Erez},
	title = {A practical wait-free simulation for lock-free data structures},
	year = {2014},
	issue_date = {August 2014},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {49},
	number = {8},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/2692916.2555261},
	doi = {10.1145/2692916.2555261},
	abstract = {Lock-free data structures guarantee overall system progress, whereas wait-free data structures guarantee the progress of each and every thread, providing the desirable non-starvation guarantee for concurrent data structures. While practical lock-free implementations are known for various data structures, wait-free data structure designs are rare. Wait-free implementations have been notoriously hard to design and often inefficient. In this work we present a transformation of lock-free algorithms to wait-free ones allowing even a non-expert to transform a lock-free data-structure into a practical wait-free one. The transformation requires that the lock-free data structure is given in a normalized form defined in this work. Using the new method, we have designed and implemented wait-free linked-list, skiplist, and tree and we measured their performance. It turns out that for all these data structures the wait-free implementations are only a few percent slower than their lock-free counterparts, while still guaranteeing non-starvation.},
	journal = {SIGPLAN Not.},
	month = feb,
	pages = {357–368},
	numpages = {12},
	keywords = {lock-freedom, wait-freedom}
}

@inproceedings{michael1996simple,
	author = {Michael, Maged M. and Scott, Michael L.},
	title = {Simple, fast, and practical non-blocking and blocking concurrent queue algorithms},
	year = {1996},
	isbn = {0897918002},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/248052.248106},
	doi = {10.1145/248052.248106},
	booktitle = {Proceedings of the Fifteenth Annual ACM Symposium on Principles of Distributed Computing},
	pages = {267–275},
	numpages = {9},
	keywords = {compare_and_swap, concurrent queue, lock-free, multiprogramming, non-blocking},
	location = {Philadelphia, Pennsylvania, USA},
	series = {PODC '96}
}

@InProceedings{xu2023rust,
	author = {Xu, Baowen and Chu, Bei and Fan, Hongcheng and Feng, Yang},
	title = {An Analysis of the Rust Programming Practice for Memory Safety Assurance},
	year = {2023},
	isbn = {978-981-99-6221-1},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
	url = {https://doi.org/10.1007/978-981-99-6222-8_37},
	doi = {10.1007/978-981-99-6222-8_37},
	abstract = {Memory safety is a critical concern in software development, as related issues often lead to program crashes, vulnerabilities, and security breaches, leading to severe consequences for applications and systems. This paper provides a detailed analysis of how Rust effectively addresses memory safety concerns. The paper first introduces the concepts of ownership, reference and lifetime in Rust, highlighting how they contribute to ensuring memory safety. It then delves into an examination of common memory safety issues and how they manifest in popular programming languages. Rust’s solutions to these issues are compared to those of other languages, emphasizing the benefits of using Rust for enhanced memory safety. In conclusion, this paper offers a comprehensive exploration of prevalent memory safety issues in programming and demonstrates how Rust effectively addresses them. With its encompassing mechanisms and strict rules, Rust proves to be a reliable choice for developers aiming to achieve enhanced memory safety in their programming endeavors.},
	booktitle = {Web Information Systems and Applications: 20th International Conference, WISA 2023,  Chengdu, China, September 15–17, 2023,  Proceedings},
	pages = {440–451},
	numpages = {12},
	keywords = {Memory safety, Rust, Ownership, Reference},
	location = {Chengdu, China}
}

@misc{sharma2024rustembeddedsystemscurrent,
      title={Rust for Embedded Systems: Current State, Challenges and Open Problems (Extended Report)}, 
      author={Ayushi Sharma and Shashank Sharma and Santiago Torres-Arias and Aravind Machiry},
      year={2024},
      eprint={2311.05063},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2311.05063}, 
}

@article{memorytomemory,
author = {Bédin, Denis and Lépine, François and Mostéfaoui, Achour and Perez, Damien and Perrin, Matthieu},
year = {2024},
month = {03},
pages = {},
title = {Wait-free Algorithms: the Burden of the Past},
doi = {10.21203/rs.3.rs-4125819/v1}
}

@misc{brandenburg2019multiprocessorrealtimelockingprotocols,
      title={Multiprocessor Real-Time Locking Protocols: A Systematic Review}, 
      author={Björn B. Brandenburg},
      year={2019},
      eprint={1909.09600},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/1909.09600}, 
}

@misc{kode2024analysisSynchronization,
      title={Analysis of Synchronization Mechanisms in Operating Systems}, 
      author={Oluwatoyin Kode and Temitope Oyemade},
      year={2024},
      eprint={2409.11271},
      archivePrefix={arXiv},
      primaryClass={cs.OS},
      url={https://arxiv.org/abs/2409.11271}, 
}

@inproceedings {huang2002improvingWaitFree,
	author = {Hai Huang and Padmanabhan Pillai and Kang G. Shin},
	title = {Improving {Wait-Free} Algorithms for Interprocess Communication in Embedded {Real-Time} Systems},
	booktitle = {2002 USENIX Annual Technical Conference (USENIX ATC 02)},
	year = {2002},
	address = {Monterey, CA},
	url = {https://www.usenix.org/conference/2002-usenix-annual-technical-conference/improving-wait-free-algorithms-interprocess},
	publisher = {USENIX Association},
	month = jun
}

@misc{pellegrini2020relevancewaitfreecoordinationalgorithms,
      title={On the Relevance of Wait-free Coordination Algorithms in Shared-Memory HPC:The Global Virtual Time Case}, 
      author={Alessandro Pellegrini and Francesco Quaglia},
      year={2020},
      eprint={2004.10033},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2004.10033}, 
}

@inproceedings{dolz2016SPSC,
	author = {Dolz, Manuel F. and del Rio Astorga, David and Fern\'{a}ndez, Javier and Garc\'{\i}a, J. Daniel and Garc\'{\i}a-Carballeira, F\'{e}lix and Danelutto, Marco and Torquati, Massimo},
	title = {Embedding Semantics of the Single-Producer/Single-Consumer Lock-Free Queue into a Race Detection Tool},
	year = {2016},
	isbn = {9781450341967},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2883404.2883406},
	doi = {10.1145/2883404.2883406},
	abstract = {The rapid progress of multi-/many-core architectures has caused data-intensive parallel applications not yet be fully suited for getting the maximum performance. The advent of parallel programming frameworks offering structured patterns has alleviated developers' burden adapting such applications to parallel platforms. For example, the use of synchronization mechanisms in multithreaded applications is essential on shared-cache multi-core architectures. However, ensuring an appropriate use of their interfaces can be challenging, since different memory models plus instruction reordering at compiler/processor levels may influence the occurrence of data races. The benefits of race detectors are formidable in this sense, nevertheless if lock-free data structures with no high-level atomics are used, they may emit false positives. In this paper, we extend the ThreadSanitizer race detection tool in order to support semantics of the general Single-Producer/Single-Consumer (SPSC) lock-free parallel queue and to detect benign data races where it was correctly used. To perform our analysis, we leverage the FastFlow SPSC bounded lock-free queue implementation to test our extensions over a set of μ-benchmarks and real applications on a dual-socket Intel Xeon CPU E5-2695 platform. We demonstrate that this approach can reduce, on average, 30\% the number of data race warning messages.},
	booktitle = {Proceedings of the 7th International Workshop on Programming Models and Applications for Multicores and Manycores},
	pages = {20–29},
	numpages = {10},
	keywords = {Wait-/lock-free parallel structures, Semantics, Parallel programming, Data race detectors},
	location = {Barcelona, Spain},
	series = {PMAM'16}
}

@inproceedings{Aldinucci2012EfficientSync,
	author    = {Aldinucci, Marco and Danelutto, Marco and Kilpatrick, Peter and Meneghin, Massimiliano and Torquati, Massimo},
	title     = {An Efficient Synchronisation Mechanism for Multi-Core Systems},
	booktitle = {Euro-Par 2012 Parallel Processing},
	year      = {2012},
	editor    = {Kaklamanis, Christos and Papatheodorou, Theodore and Spirakis, Paul G.},
	series    = {Lecture Notes in Computer Science},
	volume    = {7484},
	publisher = {Springer},
	address   = {Berlin, Heidelberg},
	note      = {The provided PDF indicates this work is related to Euro-Par 2012. Exact page numbers for this specific title within the proceedings were not found in the immediate search; the closely related paper "An Efficient Unbounded Lock-Free Queue for Multi-core Systems" by the same authors appears on pages 662-673 of the same volume.}
}

@article{Wang2013BQueue,
	author    = {Wang, Junchang and Zhang, Kai and Tang, Xinan and Hua, Bei},
	title     = {B-Queue: Efficient and Practical Queuing for Fast Core-to-Core Communication},
	journal   = {International Journal of Parallel Programming},
	year      = {2013},
	volume    = {41},
	number    = {1},
	pages     = {137--159},
	month     = {2},
	abstract  = {Core-to-core communication is critical to the effective use of multi-core processors. A number of software based concurrent lock-free queues have been proposed to address this problem. Existing solutions, however, suffer from performance degradation in real testbeds, or rely on auxiliary hardware or software timers to handle the deadlock problem when batching is used, making those solutions good in theory but difficult to use in practice. This paper describes the pros and cons of existing concurrent lock-free queues in both dummy and real testbeds and proposes B-Queue, an efficient and practical single-producer-single-consumer concurrent lock-free queue that solves the deadlock problem gracefully by introducing a self-adaptive backtracking mechanism. Experiments show that in real massively-parallel applications, B-Queue is faster than FastForward and MCRingBuffer, the two state-of-the-art concurrent lock-free queues, by up to 10x and 5x, respectively. Moreover, B-Queue outperforms FastForward and MCRingBuffer in terms of stability and scalability, making it a good candidate for fast core-to-core communication on multi-core architectures.},
	issn      = {1573-7640},
	doi       = {10.1007/s10766-012-0213-x},
	url       = {https://doi.org/10.1007/s10766-012-0213-x}
}

@software{githubMA,
  author = {Demir, Devrim Baran},
  howpublished = {\url{https://github.com/DevrimBaran/MA.git}},
}

@software{mirigithub,
  title = {miri},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/rust-lang/miri.git}},
}

@inproceedings{ffq,
	author = {Giacomoni, John and Moseley, Tipp and Vachharajani, Manish},
	year = {2008},
	month = {01},
	pages = {},
	title = {FastForward for Efficient Pipeline Parallelism: A Cache-Optimized Concurrent Lock-Free Queue},
	journal = {Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP},
	doi = {10.1145/1345206.1345215}
}

@article{MaffioneCacheAware,
author = {Maffione, Vincenzo and Lettieri, Giuseppe and Rizzo, Luigi},
title = {Cache-aware design of general-purpose Single-Producer–Single-Consumer queues},
journal = {Software: Practice and Experience},
volume = {49},
number = {5},
pages = {748-779},
keywords = {cache-aware design, FastForward, Lamport lock-free queue, SPSC},
doi = {https://doi.org/10.1002/spe.2675},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2675},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.2675},
abstract = {Summary Data processing pipelines normally use lockless Single-Producer–Single-Consumer (SPSC) queues to efficiently decouple their processing threads and achieve high throughput, minimizing the cost of synchronization. SPSC queues have been widely studied, mostly for applications such as streaming data or network monitoring, where the main goal is maximizing throughput. There are now many applications, such as virtual-machine–virtual-machine communication, software-defined networking, and message-based kernels, where low latency is also important, and the tradeoffs between high-throughput and low-latency algorithms have not been studied equally well. Furthermore, at high or variable transaction rates, the effect of memory hierarchies and cache coherence subsystems may be dominant and yield surprising results. In this paper, we make two contributions. First, we provide a comprehensive study of the two main families of SPSC queues, namely, “Lamport” and “FastForward” queues, with a detailed analytical and experimental characterization of their behavior in terms of operating regimes, throughput, latency, and cache misses. Second, we propose two new queue variants, namely, improved FastForward and batched improved FastForward, which have better worst-case behavior than other variants in terms of cache misses, which is an important feature for a number of applications. Together, these two contributions provide practical guidelines to choose the best solution depending on the application requirements.},
year = {2019}
}


@misc{torquati2010singleproducersingleconsumerqueuessharedcache,
      title={Single-Producer/Single-Consumer Queues on Shared Cache Multi-Core Systems}, 
      author={Massimo Torquati},
      year={2010},
      eprint={1012.1824},
      archivePrefix={arXiv},
      primaryClass={cs.DS},
      url={https://arxiv.org/abs/1012.1824}, 
}

@inproceedings{Gidenstam2010CacheAwareLockFreeQueues,
	author = {Gidenstam, Anders and Sundell, H\r{a}kan and Tsigas, Philippas},
	title = {Cache-aware lock-free queues for multiple producers/consumers and weak memory consistency},
	year = {2010},
	isbn = {3642176526},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
	abstract = {A lock-free FIFO queue data structure is presented in this paper. The algorithm supports multiple producers and multiple consumers and weak memory models. It has been designed to be cache-aware and work directly on weak memory models. It utilizes the cache behavior in concert with lazy updates of shared data, and a dynamic lock-free memory management scheme to decrease unnecessary synchronization and increase performance. Experiments on an 8- way multi-core platform show significantly better performance for the new algorithm compared to previous fast lock-free algorithms.},
	booktitle = {Proceedings of the 14th International Conference on Principles of Distributed Systems},
	pages = {302–317},
	numpages = {16},
	location = {Tozeur, Tunisia},
	series = {OPODIS'10}
}

@article{Brandenburg2010SpinBasedReaderWriterSynchronization,
	author = {Brandenburg, Bj\"{o}rn B. and Anderson, James H.},
	title = {Spin-based reader-writer synchronization for multiprocessor real-time systems},
	year = {2010},
	issue_date = {September 2010},
	publisher = {Kluwer Academic Publishers},
	address = {USA},
	volume = {46},
	number = {1},
	issn = {0922-6443},
	url = {https://doi.org/10.1007/s11241-010-9097-2},
	doi = {10.1007/s11241-010-9097-2},
	abstract = {Reader preference, writer preference, and task-fair reader-writer locks are shown to cause undue blocking in multiprocessor real-time systems. Phase-fair reader writer locks, a new class of reader-writer locks, are proposed as an alternative. Three local-spin phase-fair lock algorithms, one with constant remote-memory-reference complexity, are presented and demonstrated to be efficiently implementable on common hardware platforms. Both task- and phase-fair locks are evaluated and contrasted to mutex locks in terms of hard and soft real-time schedulability--each under both global and partitioned scheduling--under consideration of runtime overheads on a multicore Sun "Niagara" UltraSPARC T1 processor. Formal bounds on worst-case blocking are derived for all considered lock types.},
	journal = {Real-Time Syst.},
	month = sep,
	pages = {25–87},
	numpages = {63},
	keywords = {Real-time, Reader-writer synchronization, Multiprocessor}
}

@article{stankovic1996real,
	author = {Stankovic, John A.},
	title = {Real-time and embedded systems},
	year = {1996},
	issue_date = {March 1996},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {28},
	number = {1},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/234313.234400},
	doi = {10.1145/234313.234400},
	journal = {ACM Comput. Surv.},
	month = mar,
	pages = {205–208},
	numpages = {4}
}

@Misc{real-time,
	title        = {Introduction to Real-time Systems},
	author       = {Kay, Jackie},
	howpublished = {\url{https://design.ros2.org/articles/realtime_background.html}},
	year = {2016}
}

@Misc{jitter,
	title        = {Determinism and Jitter in a Real-Time System},
	howpublished = {\url{https://www.ni.com/docs/de-DE/bundle/labview-nxg-rt-module-programming-with-rt-target/page/determinism-real-time.html}},
	year = {2016}
}

@Misc{IPC,
	title        = {Inter Process Communication (IPC)},
	howpublished = {\url{https://www.geeksforgeeks.org/inter-process-communication-ipc/}},
	year = {2025}
}

@Misc{OracleIPC,
	title        = {Interprocess Communication},
	howpublished = {\url{https://docs.oracle.com/cd/E19455-01/806-4750/6jdqdfltn/index.html}},
}

@Misc{IPCMethods,
	title        = {Methods in Inter process Communication},
	howpublished = {\url{https://www.geeksforgeeks.org/methods-in-interprocess-communication/}},
}

@Misc{Race-Condition,
	title        = {Race Condition, Synchronization, atomic operations and Volatile keyword.},
	author       = {Aniket Thakur},
	howpublished = {\url{https://opensourceforgeeks.blogspot.com/2014/01/race-condition-synchronization-atomic.html}},
	year = {2014},
}

@Misc{DiffProcessThread,
	title        = {Difference between Process and Thread},
	howpublished = {\url{https://opensourceforgeeks.blogspot.com/2014/01/race-condition-synchronization-atomic.html}},
	year = {2025},
}

@Misc{Deadlock,
	title        = {Introduction of Deadlock in Operating System},
	howpublished = {\url{https://www.geeksforgeeks.org/introduction-of-deadlock-in-operating-system/}},
	year = {2025},
}

@Misc{MutualExclusion,
	title        = {Managing Mutual Exclusion Mechanism for Real-Time Applications },
	howpublished = {\url{https://realtimepartner.com/articles/mutual-exclusion.html}},
	year = {2016},
}

@inproceedings{Atomics,
author = {Hoseini, Fazeleh and Atalar, Aras and Tsigas, Philippas},
title = {Modeling the Performance of Atomic Primitives on Modern Architectures},
year = {2019},
isbn = {9781450362955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3337821.3337901},
doi = {10.1145/3337821.3337901},
abstract = {Utilizing the atomic primitives of a processor to access a memory location atomically is key to the correctness and feasibility of parallel software systems. The performance of atomics plays a significant role in the scalability and overall performance of parallel software systems.In this work, we study the performance -in terms of latency, throughput, fairness, energy consumption- of atomic primitives in the context of the two common software execution settings that result in high and low contention access on shared memory. We perform and present an exhaustive study of the performance of atomics in these two application contexts and propose a performance model that captures their behavior. We consider two state-of-the-art architectures: Intel Xeon E5, Xeon Phi (KNL). We propose a model that is centered around the bouncing of cache lines between threads that execute atomic primitives on these shared cache lines. The model is very simple to be used in practice and captures the behavior of atomics accurately under these execution scenarios and facilitate algorithmic design decisions in multi-threaded programming.},
booktitle = {Proceedings of the 48th International Conference on Parallel Processing},
articleno = {28},
numpages = {11},
keywords = {Synchronization, Performance, Parallel Computing, Modeling, Concurrency, Atomic Primitives},
location = {Kyoto, Japan},
series = {ICPP '19}
}

@article{drepper2007every,
  title={What every programmer should know about memory},
  author={Drepper, Ulrich},
  journal={Red Hat, Inc},
  volume={11},
  number={2007},
  pages={2007},
  year={2007}
}

@inbook{HardSoftRealTime,
	author = {Kavi, Krishna and Akl, Robert and Hurson, Ali},
	year = {2009},
	month = {03},
	pages = {},
	title = {Real‐Time Systems: An Introduction and the State‐of‐the‐Art},
	isbn = {9780470050118},
	doi = {10.1002/9780470050118.ecse344}
}

@article{BrakeByWire,
	author = {Hua, Xuehui and Zeng, Jinbin and Li, Haoxin and Huang, Jingkai and Luo, Maolin and Feng, Xiaoming and Xiong, Huiyuan and Wu, Weibin},
	year = {2023},
	month = {03},
	pages = {994},
	title = {A Review of Automobile Brake-by-Wire Control Technology},
	volume = {11},
	journal = {Processes},
	doi = {10.3390/pr11040994}
}

@inproceedings{IPCMechanisms,
	title={Evaluation of Inter-Process Communication Mechanisms},
	author={Aditya Venkataraman and Kishore Kumar Jagadeesha},
	year={2015},
	url={https://api.semanticscholar.org/CorpusID:6899525}
}

@inproceedings{polylog,
	author = {Naderibeni, Hossein and Ruppert, Eric},
	title = {A Wait-free Queue with Polylogarithmic Step Complexity},
	year = {2023},
	isbn = {9798400701214},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3583668.3594565},
	doi = {10.1145/3583668.3594565},
	abstract = {We present a novel linearizable wait-free queue implementation using single-word CAS instructions. Previous lock-free queue implementations from CAS all have amortized step complexity of Ω(p) per operation in worst-case executions, where p is the number of processes that access the queue. Our new wait-free queue takes O(log p) steps per enqueue and O(log2 p + log q) steps per dequeue, where q is the size of the queue. A bounded-space version of the implementation has O(log p log(p + q)) amortized step complexity per operation.},
	booktitle = {Proceedings of the 2023 ACM Symposium on Principles of Distributed Computing},
	pages = {124–134},
	numpages = {11},
	keywords = {wait-free queues, concurrent data structures},
	location = {Orlando, FL, USA},
	series = {PODC '23}
}

@InProceedings{johnen_et_al:LIPIcs.OPODIS.2022.4,
	author =	{Johnen, Colette and Khattabi, Adnane and Milani, Alessia},
	title =	{{Efficient Wait-Free Queue Algorithms with Multiple Enqueuers and Multiple Dequeuers}},
	booktitle =	{26th International Conference on Principles of Distributed Systems (OPODIS 2022)},
	pages =	{4:1--4:19},
	series =	{Leibniz International Proceedings in Informatics (LIPIcs)},
	ISBN =	{978-3-95977-265-5},
	ISSN =	{1868-8969},
	year =	{2023},
	volume =	{253},
	editor =	{Hillel, Eshcar and Palmieri, Roberto and Rivi\`{e}re, Etienne},
	publisher =	{Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
	address =	{Dagstuhl, Germany},
	URL =		{https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.OPODIS.2022.4},
	URN =		{urn:nbn:de:0030-drops-176240},
	doi =		{10.4230/LIPIcs.OPODIS.2022.4},
	annote =	{Keywords: Distributed computing, distributed algorithms, FIFO queue, shared memory, fault tolerance, concurrent data structures, relaxed specifications, complexity}
}

@Inbook{SharedMemory,
	author="Wang, K. C.",
	title="Process Management in Embedded Systems",
	bookTitle="Embedded and Real-Time Operating Systems",
	year="2023",
	publisher="Springer International Publishing",
	address="Cham",
	pages="115--168",
	abstract="This chapter covers process management. It introduces the concept of processes and demonstrates the basic technique of multitasking by context switching. It shows how to create processes dynamically and discusses the goals, policy, and algorithms of process scheduling. It covers process synchronization and shows how to implement the various kinds of process synchronization mechanisms, which include sleep/wakeup, mutexes, and semaphores. It shows how to use the process synchronization mechanisms to implement event-driven embedded systems. It discusses interprocess communication (IPC) schemes, which include shared memory, pipes, and message passing. It shows how to integrate these concepts to implement a uniprocessor (UP) kernel for process management, and it shows the programming techniques for both non-preemptive and preemptive process scheduling. The UP kernel serves as the foundation for developing complete operating systems (OS) in later chapters.",
	isbn="978-3-031-28701-5",
	doi="10.1007/978-3-031-28701-5_5",
	url="https://doi.org/10.1007/978-3-031-28701-5_5"
}

@article{sharedmem,
author = {Mellor-Crummey, John M. and Scott, Michael L.},
title = {Algorithms for scalable synchronization on shared-memory multiprocessors},
year = {1991},
issue_date = {Feb. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {0734-2071},
url = {https://doi.org/10.1145/103727.103729},
doi = {10.1145/103727.103729},
abstract = {Busy-wait techniques are heavily used for mutual exclusion and barrier synchronization in shared-memory parallel programs. Unfortunately, typical implementations of busy-waiting tend to produce large amounts of memory and interconnect contention, introducing performance bottlenecks that become markedly more pronounced as applications scale. We argue that this problem is not fundamental, and that one can in fact construct busy-wait synchronization algorithms that induce no memory or interconnect contention. The key to these algorithms is for every processor to spin on separate locally-accessible flag variables, and for some other processor to terminate the spin with a single remote write operation at an appropriate time. Flag variables may be locally-accessible as a result of coherent caching, or by virtue of allocation in the local portion of physically distributed shared memory.We present a new scalable algorithm for spin locks that generates 0(1) remote references per lock acquisition, independent of the number of processors attempting to acquire the lock. Our algorithm provides reasonable latency in the absence of contention, requires only a constant amount of space per lock, and requires no hardware support other than a swap-with-memory instruction. We also present a new scalable barrier algorithm that generates 0(1) remote references per processor reaching the barrier, and observe that two previously-known barriers can likewise be cast in a form that spins only on locally-accessible flag variables. None of these barrier algorithms requires hardware support beyond the usual atomicity of memory reads and writes.We compare the performance of our scalable algorithms with other software approaches to busy-wait synchronization on both a Sequent Symmetry and a BBN Butterfly. Our principal conclusion is that contention due to synchronization need not be a problem in large-scale shared-memory multiprocessors. The existence of scalable algorithms greatly weakens the case for costly special-purpose hardware support for synchronization, and provides a case against so-called “dance hall” architectures, in which shared memory locations are equally far from all processors. —From the Authors' Abstract},
journal = {ACM Trans. Comput. Syst.},
month = feb,
pages = {21–65},
numpages = {45}
}

@article{SharedMemoryMessagePassing,
	author = {Sarvesh Mogare and Abhijeet Mahamune and Dipak Sathe and Harshal Bhangare and Minal Deshmukh and Anup Ingale},
	journal = {International Research Journal of Modernization in Engineering Technology and Science (IRJMETS)},
	title = {Message Passing VS Shared Memory-a Survey Of Trade Off In IPC},
	volume = {06},
	month = {11},
	year = {2024}
}

@inproceedings{criticalSectionMutex,
	author = {Suleman, M. Aater and Mutlu, Onur and Qureshi, Moinuddin K. and Patt, Yale N.},
	title = {Accelerating critical section execution with asymmetric multi-core architectures},
	year = {2009},
	isbn = {9781605584065},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1508244.1508274},
	doi = {10.1145/1508244.1508274},
	abstract = {To improve the performance of a single application on Chip Multiprocessors (CMPs), the application must be split into threads which execute concurrently on multiple cores. In multi-threaded applications, critical sections are used to ensure that only one thread accesses shared data at any given time. Critical sections can serialize the execution of threads, which significantly reduces performance and scalability.This paper proposes Accelerated Critical Sections (ACS), a technique that leverages the high-performance core(s) of an Asymmetric Chip Multiprocessor (ACMP) to accelerate the execution of critical sections. In ACS, selected critical sections are executed by a high-performance core, which can execute the critical section faster than the other, smaller cores. As a result, ACS reduces serialization: it lowers the likelihood of threads waiting for a critical section to finish. Our evaluation on a set of 12 critical-section-intensive workloads shows that ACS reduces the average execution time by 34\% compared to an equal-area 32T-core symmetric CMP and by 23\% compared to an equal-area ACMP. Moreover, for 7 out of the 12 workloads, ACS improves scalability by increasing the number of threads at which performance saturates.},
	booktitle = {Proceedings of the 14th International Conference on Architectural Support for Programming Languages and Operating Systems},
	pages = {253–264},
	numpages = {12},
	keywords = {cmp, critical sections, heterogeneous cores, locks, multi-core, parallel programming},
	location = {Washington, DC, USA},
	series = {ASPLOS XIV}
}

@INPROCEEDINGS{Semaphore,
	author={Gong, Yi and Chen, Minjie and Song, Lihua and Guo, Yanfei},
	booktitle={2022 IEEE 2nd International Conference on Power, Electronics and Computer Applications (ICPECA)}, 
	title={Study on the classification model of lock mechanism in operating system}, 
	year={2022},
	volume={},
	number={},
	pages={857-861},
	keywords={Operating systems;Computational modeling;Conferences;Software algorithms;Computer architecture;System recovery;Software;lock model;spinlock;semaphore;lock chain structure},
	doi={10.1109/ICPECA53709.2022.9718877}
}

@INPROCEEDINGS{MutexSemaphoreIPC,
	author={Raghunathan, Sriram},
	booktitle={2008 14th IEEE International Conference on Parallel and Distributed Systems}, 
	title={Extending Inter-process Synchronization with Robust Mutex and Variants in Condition Wait}, 
	year={2008},
	volume={},
	number={},
	pages={121-128},
	keywords={Robustness;Yarn;Libraries;Linux;Application software;Kernel;Content addressable storage;Context-aware services;Protection;Event detection;Condition Wait;Mutexes;FIFO Waiters;Semaphores;Signaling and Synchronization},
	doi={10.1109/ICPADS.2008.98}
}

@article{chahar2013deadlock,
	title={Deadlock resolution techniques: an overview},
	author={Chahar, Pooja and Dalal, Surjeet},
	journal={International Journal of Scientific and Research Publications},
	volume={3},
	number={7},
	pages={1--5},
	year={2013},
	publisher={Citeseer}
}

@Inbook{Starvation,
	author="Buhr, Peter A.",
	title="Concurrency Errors",
	bookTitle="Understanding Control Flow: Concurrent Programming Using $\mu$C++",
	year="2016",
	publisher="Springer International Publishing",
	address="Cham",
	pages="395--423",
	abstract="The introduction of threads and locks into a programming language introduces new kinds of errors not present in sequential programming; several of these errors have been mentioned in previous chapters. When writing concurrent programs, it is important to understand the new kinds of errors so they can be avoided or debugged when they occur. Therefore, it is appropriate to take a short diversion from the discussion of specifying concurrency to explain these new programming problems.",
	isbn="978-3-319-25703-7",
	doi="10.1007/978-3-319-25703-7_8",
	url="https://doi.org/10.1007/978-3-319-25703-7_8"
}

@ARTICLE{priorityInversion,
	author={Yun Wang and Anceaume, E. and Brasileiro, F. and Greve, F. and Hurfin, M.},
	journal={IEEE Transactions on Computers}, 
	title={Solving the group priority inversion problem in a timed asynchronous system}, 
	year={2002},
	volume={51},
	number={8},
	pages={900-915},
	keywords={Protocols;Fault tolerant systems;Real time systems;Delay;Synchronization;Computer Society;Processor scheduling;Mechanical factors;Predictive models;Detectors},
	doi={10.1109/TC.2002.1024738}
}

@article{MichaelScottQueue,
	author = {Michael, Maged and Scott, Michael},
	year = {1996},
	month = {03},
	pages = {},
	title = {Simple, Fast, and Practical Non-Blocking and Blocking Concurrent Queue Algorithms},
	journal = {Proceedings of the Annual ACM Symposium on Principles of Distributed Computing},
	doi = {10.1145/248052.248106}
}

@article{culic2022lowRust,
	title={A low-latency optimization of a rust-based secure operating system for embedded devices},
	author={Culic, Ioana and Vochescu, Alexandru and Radovici, Alexandru},
	journal={Sensors},
	volume={22},
	number={22},
	pages={8700},
	year={2022},
	publisher={MDPI}
}

@inproceedings{Fuchs2014EvaluationOT,
	title={Evaluation of Task Scheduling Algorithms and Wait-Free Data Structures for Embedded Multi-Core Systems},
	author={Tobias Fuchs and Haruki Murakami},
	year={2014},
	url={https://api.semanticscholar.org/CorpusID:218073330}
}

@techreport{Drescher2015GuardedSections,
	author      = {Gabor Drescher and Wolfgang Schr{\"o}der-Preikschat},
	title       = {An Experiment in Wait-Free Synchronisation of Priority-Controlled Simultaneous Processes: Guarded Sections},
	institution = {Friedrich-Alexander-Universit{\"a}t Erlangen-N{\"u}rnberg, Dept. of Computer Science},
	year        = {2015},
	number      = {CS-2015-01},
	month       = {1},
	url         = {https://www4.cs.fau.de/Publications/2015/drescher_15_cstr.pdf}
}

@InProceedings{Mateíspmc,
	author="David, Matei",
	editor="Guerraoui, Rachid",
	title="A Single-Enqueuer Wait-Free Queue Implementation",
	booktitle="Distributed Computing",
	year="2004",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="132--143",
	abstract="We study wait-free linearizable Queue implementations in asynchronous shared-memory systems from other consensus number 2 objects, such as Fetch{\&}Add and Swap. The best previously known implementation allows at most two processes to perform Dequeue operations. We provide a new implementation, when only one process performs Enqueue operations and any number of processes perform Dequeue operations. A nice feature of this implementation is the fact that both Enqueue and Dequeue operations take constant time.",
	isbn="978-3-540-30186-8"
}

@InProceedings{JayantiLog,
	author="Jayanti, Prasad
	and Petrovic, Srdjan",
	editor="Sarukkai, Sundar
	and Sen, Sandeep",
	title="Logarithmic-Time Single Deleter, Multiple Inserter Wait-Free Queues and Stacks",
	booktitle="FSTTCS 2005: Foundations of Software Technology and Theoretical Computer Science",
	year="2005",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="408--419",
	abstract="Despite the ubiquitous need for shared FIFO queues in parallel applications and operating systems, there are no sublinear-time wait-free queue algorithms that can support more than a single enqueuer and a single dequeuer. Two independently designed algorithms---David's recent algorithm [1] and the algorithm in this paper---break this barrier. While David's algorithm is capable of supporting multiple dequeuers (but only one enqueuer), our algorithm can support multiple enqueuers (but only one dequeuer). David's algorithm achieves O(1) time complexity for both enqueue and dequeue operations, but its space complexity is infinite because of the use of infinite sized arrays. The author states that he can bound the space requirement, but only at the cost of increasing the time complexity to O(n), where n is the number of dequeuers. A significant feature of our algorithm is that its time and space complexities are both bounded and small: enqueue and dequeue operations run in {\$}O({\backslash}lg n){\$}time, and the space complexity is O(n+m), where n is the number of enqueuers and m is the actual number of items currently present in the queue. David's algorithm uses fetch{\&}increment and swap instructions, which are both at level 2 of Herlihy's Consensus hierarchy, along with queue. Our algorithm uses the LL/SC instructions, which are universal. However, since these instructions have constant time wait-free implementation from CAS and restricted LL/SC that are widely supported on modern architectures, our algorithms can run efficiently on current machines. Thus, in applications where there are multiple producers and a single consumer (e.g., certain server queues and resource queues), our algorithm provides the best known solution to implementing a wait-free queue. Using similar ideas, we can also efficiently implement a stack that supports multiple pushers and a single popper.",
	isbn="978-3-540-32419-5"
}

@article{jiffy,
	author       = {Dolev Adas and
							Roy Friedman},
	title        = {Jiffy: {A} Fast, Memory Efficient, Wait-Free Multi-Producers Single-Consumer
							Queue},
	journal      = {CoRR},
	volume       = {abs/2010.14189},
	year         = {2020},
	url          = {https://arxiv.org/abs/2010.14189},
	eprinttype    = {arXiv},
	eprint       = {2010.14189},
	timestamp    = {Mon, 02 Nov 2020 18:17:09 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2010-14189.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Lamport1983SPSCCircularBuffer,
	author = {Lamport, Leslie},
	title = {Specifying Concurrent Program Modules},
	year = {1983},
	issue_date = {April 1983},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {5},
	number = {2},
	issn = {0164-0925},
	url = {https://doi.org/10.1145/69624.357207},
	doi = {10.1145/69624.357207},
	journal = {ACM Trans. Program. Lang. Syst.},
	month = apr,
	pages = {190–222},
	numpages = {33}
}

@article{Lamport1977ConcurrentReading,
	author = {Lamport, Leslie},
	title = {Concurrent reading and writing},
	year = {1977},
	issue_date = {Nov. 1977},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {20},
	number = {11},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/359863.359878},
	doi = {10.1145/359863.359878},
	abstract = {The problem of sharing data among asynchronous processes is considered. It is assumed that only one process at a time can modify the data, but concurrent reading and writing is permitted. Two general theorems are proved, and some algorithms are presented to illustrate their use. These include a solution to the general problem in which a read is repeated if it might have obtained an incorrect result, and two techniques for transmitting messages between processes. These solutions do not assume any synchronizing mechanism other than data which can be written by one process and read by other processes.},
	journal = {Commun. ACM},
	month = nov,
	pages = {806–811},
	numpages = {6},
	keywords = {asynchronous multiprocessing, multiprocess synchronization, readers/writers problem, shared data}
}

@article{HerlihyLinearizability,
	author = {Herlihy, Maurice P. and Wing, Jeannette M.},
	title = {Linearizability: a correctness condition for concurrent objects},
	year = {1990},
	issue_date = {July 1990},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {12},
	number = {3},
	issn = {0164-0925},
	url = {https://doi.org/10.1145/78969.78972},
	doi = {10.1145/78969.78972},
	abstract = {A concurrent object is a data object shared by concurrent processes. Linearizability is a correctness condition for concurrent objects that exploits the semantics of abstract data types. It permits a high degree of concurrency, yet it permits programmers to specify and reason about concurrent objects using known techniques from the sequential domain. Linearizability provides the illusion that each operation applied by concurrent processes takes effect instantaneously at some point between its invocation and its response, implying that the meaning of a concurrent object's operations can be given by pre- and post-conditions. This paper defines linearizability, compares it to other correctness conditions, presents and demonstrates a method for proving the correctness of implementations, and shows how to reason about concurrent objects, given they are linearizable.},
	journal = {ACM Trans. Program. Lang. Syst.},
	month = jul,
	pages = {463–492},
	numpages = {30}
}

@article{Verma2013Scalable,
	author       = {Mudit Verma},
	title        = {Scalable and Performance-Critical Data Structures for Multicores},
	school       = {Instituto Superior T\'{e}cnico, Universidade de Lisboa},
	year         = {2013},
	month        = {6},
	url          = {https://web.tecnico.ulisboa.pt/~ist14191/repository/Thesis_Mudit_Verma.pdf},
	note         = {Thesis to obtain the Master of Science Degree in Information Systems and Computer Engineering}
}

@inproceedings{wCQWaitFreeQueue,
	author = {Nikolaev, Ruslan and Ravindran, Binoy},
	title = {wCQ: A Fast Wait-Free Queue with Bounded Memory Usage},
	year = {2022},
	isbn = {9781450391467},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3490148.3538572},
	doi = {10.1145/3490148.3538572},
	abstract = {The concurrency literature presents a number of approaches for building non-blocking, FIFO, multiple-producer and multiple-consumer (MPMC) queues. However, only a fraction of them have high performance. In addition, many queue designs, such as LCRQ, trade memory usage for better performance. The recently proposed SCQ design achieves both memory efficiency as well as excellent performance. Unfortunately, both LCRQ and SCQ are only lock-free. On the other hand, existing wait-free queues are either not very performant or suffer from potentially unbounded memory usage. Strictly described, the latter queues, such as Yang \& Mellor-Crummey's (YMC) queue, forfeit wait-freedom as they are blocking when memory is exhausted. We present a wait-free queue, called wCQ. wCQ is based on SCQ and uses its own variation of fast-path-slow-path methodology to attain wait-freedom and bound memory usage. Our experimental studies on x86 and PowerPC architectures validate wCQ's great performance and memory efficiency. They also show that wCQ's performance is often on par with the best known concurrent queue designs.},
	booktitle = {Proceedings of the 34th ACM Symposium on Parallelism in Algorithms and Architectures},
	pages = {307–319},
	numpages = {13},
	keywords = {fifo queue, ring buffer, wait-free},
	location = {Philadelphia, PA, USA},
	series = {SPAA '22}
}

@article{RamalheteQueue,
author = {Ramalhete, Pedro and Correia, Andreia},
title = {POSTER: A Wait-Free Queue with Wait-Free Memory Reclamation},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019022},
doi = {10.1145/3155284.3019022},
note = {Full version available at \url{https://github.com/pramalhe/ConcurrencyFreaks/blob/master/papers/crturnqueue-2016.pdf}},
abstract = {Queues are a widely deployed data structure. They are used extensively in many multi threaded applications, or as a communication mechanism between threads or processes. We propose a new linearizable multi-producer-multi-consumer queue we named Turn queue, with wait-free progress bounded by the number of threads, and with wait-free bounded memory reclamation. Its main characteristics are: a simple algorithm that does no memory allocation apart from creating the node that is placed in the queue, a new wait-free consensus algorithm using only the atomic instruction compare-and-swap (CAS), and is easy to plugin with other algorithms for either enqueue or dequeue methods.},
journal = {SIGPLAN Not.},
month = jan,
pages = {453–454},
numpages = {2},
keywords = {low latency, non-blocking queue, wait-free}
}

@inproceedings{WaitFreeQueueWithWaitFreeMemoryReclamation,
	author = {Ramalhete, Pedro and Correia, Andreia},
	title = {POSTER: A Wait-Free Queue with Wait-Free Memory Reclamation},
	year = {2017},
	isbn = {9781450344937},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3018743.3019022},
	doi = {10.1145/3018743.3019022},
	abstract = {Queues are a widely deployed data structure. They are used extensively in many multi threaded applications, or as a communication mechanism between threads or processes. We propose a new linearizable multi-producer-multi-consumer queue we named Turn queue, with wait-free progress bounded by the number of threads, and with wait-free bounded memory reclamation. Its main characteristics are: a simple algorithm that does no memory allocation apart from creating the node that is placed in the queue, a new wait-free consensus algorithm using only the atomic instruction compare-and-swap (CAS), and is easy to plugin with other algorithms for either enqueue or dequeue methods.},
	booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
	pages = {453–454},
	numpages = {2},
	keywords = {low latency, non-blocking queue, wait-free},
	location = {Austin, Texas, USA},
	series = {PPoPP '17}
}

@article{FeldmanDechev2015WaitFreeRingBuffer,
	author = {Feldman, Steven and Dechev, Damian},
	title = {A wait-free multi-producer multi-consumer ring buffer},
	year = {2015},
	issue_date = {September 2015},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {15},
	number = {3},
	issn = {1559-6915},
	url = {https://doi.org/10.1145/2835260.2835264},
	doi = {10.1145/2835260.2835264},
	abstract = {The ring buffer is a staple data structure used in many algorithms and applications. It is highly desirable in high-demand use cases such as multimedia, network routing, and trading systems. This work presents a new ring buffer design that is, to the best of our knowledge, the only array-based first-in-first-out ring buffer to provide wait-freedom. Wait-freedom guarantees that each thread completes its operation within a finite number of steps. This property is desirable for real-time and mission critical systems.This work is an extension and refinement of our earlier work. We have improved and expanded algorithm descriptions and pseudo-code, and we have performed all new performance evaluations.In contrast to other concurrent ring buffer designs, our implementation includes new methodology to prevent thread starvation and livelock from occurring.},
	journal = {SIGAPP Appl. Comput. Rev.},
	month = oct,
	pages = {59–71},
	numpages = {13},
	keywords = {wait-free, ring buffer, parallel, non-blocking, concurrent}
}

@article{Kogan2011WaitFreeQueues,
	author = {Kogan, Alex and Petrank, Erez},
	title = {Wait-free queues with multiple enqueuers and dequeuers},
	year = {2011},
	issue_date = {August 2011},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {46},
	number = {8},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/2038037.1941585},
	doi = {10.1145/2038037.1941585},
	abstract = {The queue data structure is fundamental and ubiquitous. Lock-free versions of the queue are well known. However, an important open question is whether practical wait-free queues exist. Until now, only versions with limited concurrency were proposed. In this paper we provide a design for a practical wait-free queue. Our construction is based on the highly efficient lock-free queue of Michael and Scott. To achieve wait-freedom, we employ a priority-based helping scheme in which faster threads help the slower peers to complete their pending operations. We have implemented our scheme on multicore machines and present performance measurements comparing our implementation with that of Michael and Scott in several system configurations.},
	journal = {SIGPLAN Not.},
	month = feb,
	pages = {223–234},
	numpages = {12},
	keywords = {wait-free algorithms, concurrent queues}
}

@article{WangCacheCoherent,
	author={Wang, Junchang and Jin, Qi and Fu, Xiong and Li, Yun and Shi, Peichang},
	journal={IEEE Access}, 
	title={Accelerating Wait-Free Algorithms: Pragmatic Solutions on Cache-Coherent Multicore Architectures}, 
	year={2019},
	volume={7},
	number={},
	pages={74653-74669},
	keywords={Multicore processing;Acceleration;Hardware;Instruction sets;Concurrent computing;Indexes;Multicore;wait-free algorithm;MPSC queue},
	doi={10.1109/ACCESS.2019.2920781}}

@inproceedings{10.1145/121132.121151,
author = {Anderson, Thomas E. and Bershad, Brian N. and Lazowska, Edward D. and Levy, Henry M.},
title = {Scheduler activations: effective kernel support for the user-level management of parallelism},
year = {1991},
isbn = {0897914473},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/121132.121151},
doi = {10.1145/121132.121151},
abstract = {Threads are the vehicle for concurrency in many approaches to parallel programming. Threads separate the notion of a sequential execution stream from the other aspects of traditional UNIX-like processes, such as address spaces and I/O descriptors. The objective of this separation is to make the expression and control of parallelism sufficiently cheap that the programmer or compiler can exploit even fine-grained parallelism with acceptable overhead.Threads can be supported either by the operating system kernel or by user-level library code in the application address space, but neither approach has been fully satisfactory. This paper addresses this dilemma. First, we argue that the performance of kernel threads is inherently worse than that of user-level threads, rather than this being an artifact of existing implementations; we thus argue that managing parallelism at the user level is essential to high-performance parallel computing. Next, we argue that the lack of system integration exhibited by user-level threads is a consequence of the lack of kernel support for user-level threads provided by contemporary multiprocessor operating systems; we thus argue that kernel threads or processes, as currently conceived, are the wrong abstraction on which to support user-level management of parallelism. Finally, we describe the design, implementation, and performance of a new kernel interface and user-level thread package that together provide the same functionality as kernel threads without compromising the performance and flexibility advantages of user-level management of parallelism.},
booktitle = {Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles},
pages = {95–109},
numpages = {15},
location = {Pacific Grove, California, USA},
series = {SOSP '91}
}

@article{processesVSthreads,
author = {Anderson, Thomas E. and Bershad, Brian N. and Lazowska, Edward D. and Levy, Henry M.},
title = {Scheduler activations: effective kernel support for the user-level management of parallelism},
year = {1991},
issue_date = {Oct. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {5},
issn = {0163-5980},
url = {https://doi.org/10.1145/121133.121151},
doi = {10.1145/121133.121151},
abstract = {Threads are the vehicle for concurrency in many approaches to parallel programming. Threads separate the notion of a sequential execution stream from the other aspects of traditional UNIX-like processes, such as address spaces and I/O descriptors. The objective of this separation is to make the expression and control of parallelism sufficiently cheap that the programmer or compiler can exploit even fine-grained parallelism with acceptable overhead.Threads can be supported either by the operating system kernel or by user-level library code in the application address space, but neither approach has been fully satisfactory. This paper addresses this dilemma. First, we argue that the performance of kernel threads is inherently worse than that of user-level threads, rather than this being an artifact of existing implementations; we thus argue that managing parallelism at the user level is essential to high-performance parallel computing. Next, we argue that the lack of system integration exhibited by user-level threads is a consequence of the lack of kernel support for user-level threads provided by contemporary multiprocessor operating systems; we thus argue that kernel threads or processes, as currently conceived, are the wrong abstraction on which to support user-level management of parallelism. Finally, we describe the design, implementation, and performance of a new kernel interface and user-level thread package that together provide the same functionality as kernel threads without compromising the performance and flexibility advantages of user-level management of parallelism.},
journal = {SIGOPS Oper. Syst. Rev.},
month = sep,
pages = {95–109},
numpages = {15}
}

@inproceedings{adampsc,
	author = {Adas, Dolev and Friedman, Roy},
	title = {A Fast Wait-Free Multi-Producers Single-Consumer Queue},
	year = {2022},
	isbn = {9781450395601},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3491003.3491004},
	doi = {10.1145/3491003.3491004},
	abstract = {In sharded data processing systems, sharded in-memory key-value stores, data flow programming and load sharing, multiple concurrent data producers feed requests into the same data consumer. This can be naturally realized through concurrent queues, where each consumer pulls its tasks from its dedicated queue. For scalability, wait-free queues are preferred over lock based structures. The vast majority of wait-free queue implementations, and even lock-free ones, support the multi-producer multi-consumer model. Yet, this comes at a premium, since implementing wait-free multi-producer multi-consumer queues requires utilizing complex helper data structures. The latter increases the memory consumption of such queues and limits their performance and scalability. Many such designs employ (hardware) cache unfriendly access patterns. In this work we study the implementation of wait-free multi-producer single-consumer queues. Specifically, we propose Jiffy, an efficient memory frugal novel wait-free multi-producer single-consumer queue and formally prove its correctness. We compare the performance and memory requirements of Jiffy with other state of the art lock-free and wait-free queues. We show that indeed Jiffy can maintain good performance with up to 128 threads, delivers up to 50\% better throughput than the next best construction we compared against, and consumes ≈ 90\% less memory.},
	booktitle = {Proceedings of the 23rd International Conference on Distributed Computing and Networking},
	pages = {77–86},
	numpages = {10},
	keywords = {Multi Producer Single Consumer, Unboubnded Queue, Wait-free},
	location = {Delhi, AA, India},
	series = {ICDCN '22}
}

@inproceedings{FastFetchAndAddWaitFreeQueue,
author = {Yang, Chaoran and Mellor-Crummey, John},
title = {A wait-free queue as fast as fetch-and-add},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851168},
doi = {10.1145/2851141.2851168},
abstract = {Concurrent data structures that have fast and predictable performance are of critical importance for harnessing the power of multicore processors, which are now ubiquitous. Although wait-free objects, whose operations complete in a bounded number of steps, were devised more than two decades ago, wait-free objects that can deliver scalable high performance are still rare.In this paper, we present the first wait-free FIFO queue based on fetch-and-add (FAA). While compare-and-swap (CAS) based non-blocking algorithms may perform poorly due to work wasted by CAS failures, algorithms that coordinate using FAA, which is guaranteed to succeed, can in principle perform better under high contention. Along with FAA, our queue uses a custom epoch-based scheme to reclaim memory; on x86 architectures, it requires no extra memory fences on our algorithm's typical execution path. An empirical study of our new FAA-based wait-free FIFO queue under high contention on four different architectures with many hardware threads shows that it outperforms prior queue designs that lack a wait-free progress guarantee. Surprisingly, at the highest level of contention, the throughput of our queue is often as high as that of a microbenchmark that only performs FAA. As a result, our fast wait-free queue implementation is useful in practice on most multi-core systems today. We believe that our design can serve as an example of how to construct other fast wait-free objects.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {16},
numpages = {13},
keywords = {fast-path-slow-path, non-blocking queue, wait-free},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@inproceedings{halfincrementhalfmax,
	author = {Khanchandani, Pankaj and Wattenhofer, Roger},
	title = {On the Importance of Synchronization Primitives with Low Consensus Numbers},
	year = {2018},
	isbn = {9781450363723},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3154273.3154306},
	doi = {10.1145/3154273.3154306},
	abstract = {The consensus number of a synchronization primitive is the maximum number of processes for which the primitive can solve consensus. This has been the traditional measure of power of a synchronization primitive. Thus, the compare-and-swap primitive, which has infinite consensus number, is considered most powerful and has been the primitive of choice for implementing concurrent data structures. In this work, we show that the synchronization primitives with low consensus numbers can also be potentially powerful by using them along with the compare-and-swap primitive to design an O (√n) time wait-free and linearizable concurrent queue. The best known time bound for a wait-free and linearizable concurrent queue using only the compare-and-swap primitive is O(n). Here, n is the total number of processes that can access the queue.The queue object maintains a sequence of elements and supports the operations enqueue(x) and dequeue(). The wait-free property implies that every call to enqueue(x) and dequeue() finishes in a bounded number of steps irrespective of the schedule of other n --1 processes. The linearizable property implies that the enqueue(x) and dequeue() calls appear to be instantaneously applied within the duration of respective calls. We design a wait-free and a linearizable concurrent queue using shared memory registers that support the compare-and-swap primitive and two other primitives of consensus number one and two respectively. The enqueue(x) and dequeue() operations take O (√n) steps each. The total number of registers required are O(nm) of O (max{log n, log m }) bits each, where m is a bound on the total number of enqueue(x) operations.},
	booktitle = {Proceedings of the 19th International Conference on Distributed Computing and Networking},
	articleno = {18},
	numpages = {10},
	keywords = {wait-free queue, sublinear wait-free, concurrent queue},
	location = {Varanasi, India},
	series = {ICDCN '18}
}