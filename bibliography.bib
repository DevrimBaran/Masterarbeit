@article{herlihy1991wait,
	author = {Herlihy, Maurice},
	title = {Wait-free synchronization},
	year = {1991},
	issue_date = {Jan. 1991},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {13},
	number = {1},
	issn = {0164-0925},
	url = {https://doi.org/10.1145/114005.102808},
	doi = {10.1145/114005.102808},
	abstract = {A wait-free implementation of a concurrent data object is one that guarantees that any process can complete any operation in a finite number of steps, regardless of the execution speeds of the other processes. The problem of constructing a wait-free implementation of one data object from another lies at the heart of much recent work in concurrent algorithms, concurrent data structures, and multiprocessor architectures. First, we introduce a simple and general technique, based on reduction to a concensus protocol, for proving statements of the form, “there is no wait-free implementation of X by Y.” We derive a hierarchy of objects such that no object at one level has a wait-free implementation in terms of objects at lower levels. In particular, we show that atomic read/write registers, which have been the focus of much recent attention, are at the bottom of the hierarchy: thay cannot be used to construct wait-free implementations of many simple and familiar data types. Moreover, classical synchronization primitives such astest&set and fetch&add, while more powerful than read and write, are also computationally weak, as are the standard message-passing primitives. Second, nevertheless, we show that there do exist simple universal objects from which one can construct a wait-free implementation of any sequential object.},
	journal = {ACM Trans. Program. Lang. Syst.},
	month = jan,
	pages = {124–149},
	numpages = {26},
	keywords = {wait-free synchronization, linearization},
}

@article{kogan2012methodology,
	author = {Kogan, Alex and Petrank, Erez},
	title = {A methodology for creating fast wait-free data structures},
	year = {2012},
	issue_date = {August 2012},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {47},
	number = {8},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/2370036.2145835},
	doi = {10.1145/2370036.2145835},
	abstract = {Lock-freedom is a progress guarantee that ensures overall program progress. Wait-freedom is a stronger progress guarantee that ensures the progress of each thread in the program. While many practical lock-free algorithms exist, wait-free algorithms are typically inefficient and hardly used in practice. In this paper, we propose a methodology called fast-path-slow-path for creating efficient wait-free algorithms. The idea is to execute the efficient lock-free version most of the time and revert to the wait-free version only when things go wrong. The generality and effectiveness of this methodology is demonstrated by two examples. In this paper, we apply this idea to a recent construction of a wait-free queue, bringing the wait-free implementation to perform in practice as efficient as the lock-free implementation. In another work, the fast-path-slow-path methodology has been used for (dramatically) improving the performance of a wait-free linked-list.},
	journal = {SIGPLAN Not.},
	month = feb,
	pages = {141–150},
	numpages = {10},
	keywords = {wait-free queues, non-blocking synchronization, lock-free algorithms, concurrent data structures}
}

@article{timnat2014practical,
	author = {Timnat, Shahar and Petrank, Erez},
	title = {A practical wait-free simulation for lock-free data structures},
	year = {2014},
	issue_date = {August 2014},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {49},
	number = {8},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/2692916.2555261},
	doi = {10.1145/2692916.2555261},
	abstract = {Lock-free data structures guarantee overall system progress, whereas wait-free data structures guarantee the progress of each and every thread, providing the desirable non-starvation guarantee for concurrent data structures. While practical lock-free implementations are known for various data structures, wait-free data structure designs are rare. Wait-free implementations have been notoriously hard to design and often inefficient. In this work we present a transformation of lock-free algorithms to wait-free ones allowing even a non-expert to transform a lock-free data-structure into a practical wait-free one. The transformation requires that the lock-free data structure is given in a normalized form defined in this work. Using the new method, we have designed and implemented wait-free linked-list, skiplist, and tree and we measured their performance. It turns out that for all these data structures the wait-free implementations are only a few percent slower than their lock-free counterparts, while still guaranteeing non-starvation.},
	journal = {SIGPLAN Not.},
	month = feb,
	pages = {357–368},
	numpages = {12},
	keywords = {lock-freedom, wait-freedom}
}

@inproceedings{michael1996simple,
	author = {Michael, Maged M. and Scott, Michael L.},
	title = {Simple, fast, and practical non-blocking and blocking concurrent queue algorithms},
	year = {1996},
	isbn = {0897918002},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/248052.248106},
	doi = {10.1145/248052.248106},
	booktitle = {Proceedings of the Fifteenth Annual ACM Symposium on Principles of Distributed Computing},
	pages = {267–275},
	numpages = {9},
	keywords = {compare_and_swap, concurrent queue, lock-free, multiprogramming, non-blocking},
	location = {Philadelphia, Pennsylvania, USA},
	series = {PODC '96}
}

@InProceedings{xu2023rust,
	author = {Xu, Baowen and Chu, Bei and Fan, Hongcheng and Feng, Yang},
	title = {An Analysis of the Rust Programming Practice for Memory Safety Assurance},
	year = {2023},
	isbn = {978-981-99-6221-1},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
	url = {https://doi.org/10.1007/978-981-99-6222-8_37},
	doi = {10.1007/978-981-99-6222-8_37},
	abstract = {Memory safety is a critical concern in software development, as related issues often lead to program crashes, vulnerabilities, and security breaches, leading to severe consequences for applications and systems. This paper provides a detailed analysis of how Rust effectively addresses memory safety concerns. The paper first introduces the concepts of ownership, reference and lifetime in Rust, highlighting how they contribute to ensuring memory safety. It then delves into an examination of common memory safety issues and how they manifest in popular programming languages. Rust’s solutions to these issues are compared to those of other languages, emphasizing the benefits of using Rust for enhanced memory safety. In conclusion, this paper offers a comprehensive exploration of prevalent memory safety issues in programming and demonstrates how Rust effectively addresses them. With its encompassing mechanisms and strict rules, Rust proves to be a reliable choice for developers aiming to achieve enhanced memory safety in their programming endeavors.},
	booktitle = {Web Information Systems and Applications: 20th International Conference, WISA 2023,  Chengdu, China, September 15–17, 2023,  Proceedings},
	pages = {440–451},
	numpages = {12},
	keywords = {Memory safety, Rust, Ownership, Reference},
	location = {Chengdu, China}
}

@misc{sharma2024rustembeddedsystemscurrent,
      title={Rust for Embedded Systems: Current State, Challenges and Open Problems (Extended Report)}, 
      author={Ayushi Sharma and Shashank Sharma and Santiago Torres-Arias and Aravind Machiry},
      year={2024},
      eprint={2311.05063},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2311.05063}, 
}

@misc{brandenburg2019multiprocessorrealtimelockingprotocols,
      title={Multiprocessor Real-Time Locking Protocols: A Systematic Review}, 
      author={Björn B. Brandenburg},
      year={2019},
      eprint={1909.09600},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/1909.09600}, 
}

@misc{kode2024analysisSynchronization,
      title={Analysis of Synchronization Mechanisms in Operating Systems}, 
      author={Oluwatoyin Kode and Temitope Oyemade},
      year={2024},
      eprint={2409.11271},
      archivePrefix={arXiv},
      primaryClass={cs.OS},
      url={https://arxiv.org/abs/2409.11271}, 
}

@inproceedings {huang2002improvingWaitFree,
	author = {Hai Huang and Padmanabhan Pillai and Kang G. Shin},
	title = {Improving {Wait-Free} Algorithms for Interprocess Communication in Embedded {Real-Time} Systems},
	booktitle = {2002 USENIX Annual Technical Conference (USENIX ATC 02)},
	year = {2002},
	address = {Monterey, CA},
	url = {https://www.usenix.org/conference/2002-usenix-annual-technical-conference/improving-wait-free-algorithms-interprocess},
	publisher = {USENIX Association},
	month = jun
}

@misc{pellegrini2020relevancewaitfreecoordinationalgorithms,
      title={On the Relevance of Wait-free Coordination Algorithms in Shared-Memory HPC:The Global Virtual Time Case}, 
      author={Alessandro Pellegrini and Francesco Quaglia},
      year={2020},
      eprint={2004.10033},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2004.10033}, 
}

@inproceedings{dolz2016SPSC,
	author = {Dolz, Manuel F. and del Rio Astorga, David and Fern\'{a}ndez, Javier and Garc\'{\i}a, J. Daniel and Garc\'{\i}a-Carballeira, F\'{e}lix and Danelutto, Marco and Torquati, Massimo},
	title = {Embedding Semantics of the Single-Producer/Single-Consumer Lock-Free Queue into a Race Detection Tool},
	year = {2016},
	isbn = {9781450341967},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2883404.2883406},
	doi = {10.1145/2883404.2883406},
	abstract = {The rapid progress of multi-/many-core architectures has caused data-intensive parallel applications not yet be fully suited for getting the maximum performance. The advent of parallel programming frameworks offering structured patterns has alleviated developers' burden adapting such applications to parallel platforms. For example, the use of synchronization mechanisms in multithreaded applications is essential on shared-cache multi-core architectures. However, ensuring an appropriate use of their interfaces can be challenging, since different memory models plus instruction reordering at compiler/processor levels may influence the occurrence of data races. The benefits of race detectors are formidable in this sense, nevertheless if lock-free data structures with no high-level atomics are used, they may emit false positives. In this paper, we extend the ThreadSanitizer race detection tool in order to support semantics of the general Single-Producer/Single-Consumer (SPSC) lock-free parallel queue and to detect benign data races where it was correctly used. To perform our analysis, we leverage the FastFlow SPSC bounded lock-free queue implementation to test our extensions over a set of μ-benchmarks and real applications on a dual-socket Intel Xeon CPU E5-2695 platform. We demonstrate that this approach can reduce, on average, 30\% the number of data race warning messages.},
	booktitle = {Proceedings of the 7th International Workshop on Programming Models and Applications for Multicores and Manycores},
	pages = {20–29},
	numpages = {10},
	keywords = {Wait-/lock-free parallel structures, Semantics, Parallel programming, Data race detectors},
	location = {Barcelona, Spain},
	series = {PMAM'16}
}

@misc{torquati2010singleproducersingleconsumerqueuessharedcache,
      title={Single-Producer/Single-Consumer Queues on Shared Cache Multi-Core Systems}, 
      author={Massimo Torquati},
      year={2010},
      eprint={1012.1824},
      archivePrefix={arXiv},
      primaryClass={cs.DS},
      url={https://arxiv.org/abs/1012.1824}, 
}

@inproceedings{Gidenstam2010CacheAwareLockFreeQueues,
	author = {Gidenstam, Anders and Sundell, H\r{a}kan and Tsigas, Philippas},
	title = {Cache-aware lock-free queues for multiple producers/consumers and weak memory consistency},
	year = {2010},
	isbn = {3642176526},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
	abstract = {A lock-free FIFO queue data structure is presented in this paper. The algorithm supports multiple producers and multiple consumers and weak memory models. It has been designed to be cache-aware and work directly on weak memory models. It utilizes the cache behavior in concert with lazy updates of shared data, and a dynamic lock-free memory management scheme to decrease unnecessary synchronization and increase performance. Experiments on an 8- way multi-core platform show significantly better performance for the new algorithm compared to previous fast lock-free algorithms.},
	booktitle = {Proceedings of the 14th International Conference on Principles of Distributed Systems},
	pages = {302–317},
	numpages = {16},
	location = {Tozeur, Tunisia},
	series = {OPODIS'10}
}

@article{Brandenburg2010SpinBasedReaderWriterSynchronization,
	author = {Brandenburg, Bj\"{o}rn B. and Anderson, James H.},
	title = {Spin-based reader-writer synchronization for multiprocessor real-time systems},
	year = {2010},
	issue_date = {September 2010},
	publisher = {Kluwer Academic Publishers},
	address = {USA},
	volume = {46},
	number = {1},
	issn = {0922-6443},
	url = {https://doi.org/10.1007/s11241-010-9097-2},
	doi = {10.1007/s11241-010-9097-2},
	abstract = {Reader preference, writer preference, and task-fair reader-writer locks are shown to cause undue blocking in multiprocessor real-time systems. Phase-fair reader writer locks, a new class of reader-writer locks, are proposed as an alternative. Three local-spin phase-fair lock algorithms, one with constant remote-memory-reference complexity, are presented and demonstrated to be efficiently implementable on common hardware platforms. Both task- and phase-fair locks are evaluated and contrasted to mutex locks in terms of hard and soft real-time schedulability--each under both global and partitioned scheduling--under consideration of runtime overheads on a multicore Sun "Niagara" UltraSPARC T1 processor. Formal bounds on worst-case blocking are derived for all considered lock types.},
	journal = {Real-Time Syst.},
	month = sep,
	pages = {25–87},
	numpages = {63},
	keywords = {Real-time, Reader-writer synchronization, Multiprocessor}
}

@article{stankovic1996real,
	author = {Stankovic, John A.},
	title = {Real-time and embedded systems},
	year = {1996},
	issue_date = {March 1996},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {28},
	number = {1},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/234313.234400},
	doi = {10.1145/234313.234400},
	journal = {ACM Comput. Surv.},
	month = mar,
	pages = {205–208},
	numpages = {4}
}

@Misc{real-time,
	title        = {Introduction to Real-time Systems},
	author       = {Kay, Jackie},
	howpublished = {\url{https://design.ros2.org/articles/realtime_background.html}},
	year = {2016}
}

@Misc{jitter,
	title        = {Determinism and Jitter in a Real-Time System},
	howpublished = {\url{https://www.ni.com/docs/de-DE/bundle/labview-nxg-rt-module-programming-with-rt-target/page/determinism-real-time.html}},
	year = {2016}
}

@Misc{IPC,
	title        = {Inter Process Communication (IPC)},
	howpublished = {\url{https://www.geeksforgeeks.org/inter-process-communication-ipc/}},
	year = {2025}
}

@Misc{OracleIPC,
	title        = {Interprocess Communication},
	howpublished = {\url{https://docs.oracle.com/cd/E19455-01/806-4750/6jdqdfltn/index.html}},
}

@Misc{IPCMethods,
	title        = {Methods in Inter process Communication},
	howpublished = {\url{https://www.geeksforgeeks.org/methods-in-interprocess-communication/}},
}

@Misc{Race-Condition,
	title        = {Race Condition, Synchronization, atomic operations and Volatile keyword.},
	author       = {Aniket Thakur},
	howpublished = {\url{https://opensourceforgeeks.blogspot.com/2014/01/race-condition-synchronization-atomic.html}},
	year = {2014},
}

@Misc{DiffProcessThread,
	title        = {Difference between Process and Thread},
	howpublished = {\url{https://opensourceforgeeks.blogspot.com/2014/01/race-condition-synchronization-atomic.html}},
	year = {2025},
}

@Misc{Deadlock,
	title        = {Introduction of Deadlock in Operating System},
	howpublished = {\url{https://www.geeksforgeeks.org/introduction-of-deadlock-in-operating-system/}},
	year = {2025},
}

@Misc{MutualExclusion,
	title        = {Managing Mutual Exclusion Mechanism for Real-Time Applications },
	howpublished = {\url{https://realtimepartner.com/articles/mutual-exclusion.html}},
	year = {2016},
}

@inbook{HardSoftRealTime,
	author = {Kavi, Krishna and Akl, Robert and Hurson, Ali},
	year = {2009},
	month = {03},
	pages = {},
	title = {Real‐Time Systems: An Introduction and the State‐of‐the‐Art},
	isbn = {9780470050118},
	doi = {10.1002/9780470050118.ecse344}
}

@article{BrakeByWire,
	author = {Hua, Xuehui and Zeng, Jinbin and Li, Haoxin and Huang, Jingkai and Luo, Maolin and Feng, Xiaoming and Xiong, Huiyuan and Wu, Weibin},
	year = {2023},
	month = {03},
	pages = {994},
	title = {A Review of Automobile Brake-by-Wire Control Technology},
	volume = {11},
	journal = {Processes},
	doi = {10.3390/pr11040994}
}

@inproceedings{IPCMechanisms,
	title={Evaluation of Inter-Process Communication Mechanisms},
	author={Aditya Venkataraman and Kishore Kumar Jagadeesha},
	year={2015},
	url={https://api.semanticscholar.org/CorpusID:6899525}
}

@Inbook{SharedMemory,
	author="Wang, K. C.",
	title="Process Management in Embedded Systems",
	bookTitle="Embedded and Real-Time Operating Systems",
	year="2023",
	publisher="Springer International Publishing",
	address="Cham",
	pages="115--168",
	abstract="This chapter covers process management. It introduces the concept of processes and demonstrates the basic technique of multitasking by context switching. It shows how to create processes dynamically and discusses the goals, policy, and algorithms of process scheduling. It covers process synchronization and shows how to implement the various kinds of process synchronization mechanisms, which include sleep/wakeup, mutexes, and semaphores. It shows how to use the process synchronization mechanisms to implement event-driven embedded systems. It discusses interprocess communication (IPC) schemes, which include shared memory, pipes, and message passing. It shows how to integrate these concepts to implement a uniprocessor (UP) kernel for process management, and it shows the programming techniques for both non-preemptive and preemptive process scheduling. The UP kernel serves as the foundation for developing complete operating systems (OS) in later chapters.",
	isbn="978-3-031-28701-5",
	doi="10.1007/978-3-031-28701-5_5",
	url="https://doi.org/10.1007/978-3-031-28701-5_5"
}

@article{SharedMemoryMessagePassing,
	author = {Sarvesh Mogare and Abhijeet Mahamune and Dipak Sathe and Harshal Bhangare and Minal Deshmukh and Anup Ingale},
	journal = {International Research Journal of Modernization in Engineering Technology and Science (IRJMETS)},
	title = {Message Passing VS Shared Memory-a Survey Of Trade Off In IPC},
	volume = {06},
	month = {11},
	year = {2024}
}

@inproceedings{criticalSectionMutex,
	author = {Suleman, M. Aater and Mutlu, Onur and Qureshi, Moinuddin K. and Patt, Yale N.},
	title = {Accelerating critical section execution with asymmetric multi-core architectures},
	year = {2009},
	isbn = {9781605584065},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1508244.1508274},
	doi = {10.1145/1508244.1508274},
	abstract = {To improve the performance of a single application on Chip Multiprocessors (CMPs), the application must be split into threads which execute concurrently on multiple cores. In multi-threaded applications, critical sections are used to ensure that only one thread accesses shared data at any given time. Critical sections can serialize the execution of threads, which significantly reduces performance and scalability.This paper proposes Accelerated Critical Sections (ACS), a technique that leverages the high-performance core(s) of an Asymmetric Chip Multiprocessor (ACMP) to accelerate the execution of critical sections. In ACS, selected critical sections are executed by a high-performance core, which can execute the critical section faster than the other, smaller cores. As a result, ACS reduces serialization: it lowers the likelihood of threads waiting for a critical section to finish. Our evaluation on a set of 12 critical-section-intensive workloads shows that ACS reduces the average execution time by 34\% compared to an equal-area 32T-core symmetric CMP and by 23\% compared to an equal-area ACMP. Moreover, for 7 out of the 12 workloads, ACS improves scalability by increasing the number of threads at which performance saturates.},
	booktitle = {Proceedings of the 14th International Conference on Architectural Support for Programming Languages and Operating Systems},
	pages = {253–264},
	numpages = {12},
	keywords = {cmp, critical sections, heterogeneous cores, locks, multi-core, parallel programming},
	location = {Washington, DC, USA},
	series = {ASPLOS XIV}
}

@INPROCEEDINGS{Semaphore,
	author={Gong, Yi and Chen, Minjie and Song, Lihua and Guo, Yanfei},
	booktitle={2022 IEEE 2nd International Conference on Power, Electronics and Computer Applications (ICPECA)}, 
	title={Study on the classification model of lock mechanism in operating system}, 
	year={2022},
	volume={},
	number={},
	pages={857-861},
	keywords={Operating systems;Computational modeling;Conferences;Software algorithms;Computer architecture;System recovery;Software;lock model;spinlock;semaphore;lock chain structure},
	doi={10.1109/ICPECA53709.2022.9718877}
}

@INPROCEEDINGS{MutexSemaphoreIPC,
	author={Raghunathan, Sriram},
	booktitle={2008 14th IEEE International Conference on Parallel and Distributed Systems}, 
	title={Extending Inter-process Synchronization with Robust Mutex and Variants in Condition Wait}, 
	year={2008},
	volume={},
	number={},
	pages={121-128},
	keywords={Robustness;Yarn;Libraries;Linux;Application software;Kernel;Content addressable storage;Context-aware services;Protection;Event detection;Condition Wait;Mutexes;FIFO Waiters;Semaphores;Signaling and Synchronization},
	doi={10.1109/ICPADS.2008.98}
}

@article{chahar2013deadlock,
	title={Deadlock resolution techniques: an overview},
	author={Chahar, Pooja and Dalal, Surjeet},
	journal={International Journal of Scientific and Research Publications},
	volume={3},
	number={7},
	pages={1--5},
	year={2013},
	publisher={Citeseer}
}

@Inbook{Starvation,
	author="Buhr, Peter A.",
	title="Concurrency Errors",
	bookTitle="Understanding Control Flow: Concurrent Programming Using $\mu$C++",
	year="2016",
	publisher="Springer International Publishing",
	address="Cham",
	pages="395--423",
	abstract="The introduction of threads and locks into a programming language introduces new kinds of errors not present in sequential programming; several of these errors have been mentioned in previous chapters. When writing concurrent programs, it is important to understand the new kinds of errors so they can be avoided or debugged when they occur. Therefore, it is appropriate to take a short diversion from the discussion of specifying concurrency to explain these new programming problems.",
	isbn="978-3-319-25703-7",
	doi="10.1007/978-3-319-25703-7_8",
	url="https://doi.org/10.1007/978-3-319-25703-7_8"
}

@ARTICLE{priorityInversion,
	author={Yun Wang and Anceaume, E. and Brasileiro, F. and Greve, F. and Hurfin, M.},
	journal={IEEE Transactions on Computers}, 
	title={Solving the group priority inversion problem in a timed asynchronous system}, 
	year={2002},
	volume={51},
	number={8},
	pages={900-915},
	keywords={Protocols;Fault tolerant systems;Real time systems;Delay;Synchronization;Computer Society;Processor scheduling;Mechanical factors;Predictive models;Detectors},
	doi={10.1109/TC.2002.1024738}
}

@article{MichaelScottQueue,
	author = {Michael, Maged and Scott, Michael},
	year = {1996},
	month = {03},
	pages = {},
	title = {Simple, Fast, and Practical Non-Blocking and Blocking Concurrent Queue Algorithms},
	journal = {Proceedings of the Annual ACM Symposium on Principles of Distributed Computing},
	doi = {10.1145/248052.248106}
}

@article{culic2022lowRust,
	title={A low-latency optimization of a rust-based secure operating system for embedded devices},
	author={Culic, Ioana and Vochescu, Alexandru and Radovici, Alexandru},
	journal={Sensors},
	volume={22},
	number={22},
	pages={8700},
	year={2022},
	publisher={MDPI}
}

@inproceedings{Fuchs2014EvaluationOT,
	title={Evaluation of Task Scheduling Algorithms and Wait-Free Data Structures for Embedded Multi-Core Systems},
	author={Tobias Fuchs and Haruki Murakami},
	year={2014},
	url={https://api.semanticscholar.org/CorpusID:218073330}
}

@article{jiffy,
	author       = {Dolev Adas and
							Roy Friedman},
	title        = {Jiffy: {A} Fast, Memory Efficient, Wait-Free Multi-Producers Single-Consumer
							Queue},
	journal      = {CoRR},
	volume       = {abs/2010.14189},
	year         = {2020},
	url          = {https://arxiv.org/abs/2010.14189},
	eprinttype    = {arXiv},
	eprint       = {2010.14189},
	timestamp    = {Mon, 02 Nov 2020 18:17:09 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2010-14189.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Lamport1983SPSCCircularBuffer,
	author = {Lamport, Leslie},
	title = {Specifying Concurrent Program Modules},
	year = {1983},
	issue_date = {April 1983},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {5},
	number = {2},
	issn = {0164-0925},
	url = {https://doi.org/10.1145/69624.357207},
	doi = {10.1145/69624.357207},
	journal = {ACM Trans. Program. Lang. Syst.},
	month = apr,
	pages = {190–222},
	numpages = {33}
}

@article{Lamport1977ConcurrentReading,
	author = {Lamport, Leslie},
	title = {Concurrent reading and writing},
	year = {1977},
	issue_date = {Nov. 1977},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {20},
	number = {11},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/359863.359878},
	doi = {10.1145/359863.359878},
	abstract = {The problem of sharing data among asynchronous processes is considered. It is assumed that only one process at a time can modify the data, but concurrent reading and writing is permitted. Two general theorems are proved, and some algorithms are presented to illustrate their use. These include a solution to the general problem in which a read is repeated if it might have obtained an incorrect result, and two techniques for transmitting messages between processes. These solutions do not assume any synchronizing mechanism other than data which can be written by one process and read by other processes.},
	journal = {Commun. ACM},
	month = nov,
	pages = {806–811},
	numpages = {6},
	keywords = {asynchronous multiprocessing, multiprocess synchronization, readers/writers problem, shared data}
}

@article{HerlihyLinearizability,
	author = {Herlihy, Maurice P. and Wing, Jeannette M.},
	title = {Linearizability: a correctness condition for concurrent objects},
	year = {1990},
	issue_date = {July 1990},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {12},
	number = {3},
	issn = {0164-0925},
	url = {https://doi.org/10.1145/78969.78972},
	doi = {10.1145/78969.78972},
	abstract = {A concurrent object is a data object shared by concurrent processes. Linearizability is a correctness condition for concurrent objects that exploits the semantics of abstract data types. It permits a high degree of concurrency, yet it permits programmers to specify and reason about concurrent objects using known techniques from the sequential domain. Linearizability provides the illusion that each operation applied by concurrent processes takes effect instantaneously at some point between its invocation and its response, implying that the meaning of a concurrent object's operations can be given by pre- and post-conditions. This paper defines linearizability, compares it to other correctness conditions, presents and demonstrates a method for proving the correctness of implementations, and shows how to reason about concurrent objects, given they are linearizable.},
	journal = {ACM Trans. Program. Lang. Syst.},
	month = jul,
	pages = {463–492},
	numpages = {30}
}

@misc{naderibeni2023waitfreequeuepolylogarithmicstep,
	title={A Wait-free Queue with Polylogarithmic Step Complexity}, 
	author={Hossein Naderibeni and Eric Ruppert},
	year={2023},
	eprint={2305.07229},
	archivePrefix={arXiv},
	primaryClass={cs.DC},
	url={https://arxiv.org/abs/2305.07229}, 
}

@inproceedings{wCQWaitFreeQueue,
	author = {Nikolaev, Ruslan and Ravindran, Binoy},
	title = {wCQ: a fast wait-free queue with bounded memory usage},
	year = {2022},
	isbn = {9781450392044},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3503221.3508440},
	doi = {10.1145/3503221.3508440},
	abstract = {The concurrency literature presents a number of approaches for building non-blocking, FIFO, multiple-producer and multiple-consumer (MPMC) queues. However, existing wait-free queues are either not very scalable or suffer from potentially unbounded memory usage. We present a wait-free queue, wCQ, which uses its own variation of the fast-path-slow-path methodology to attain wait-freedom and bound memory usage. wCQ is memory efficient and its performance is often on par with the best known concurrent queue designs.},
	booktitle = {Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
	pages = {461–462},
	numpages = {2},
	keywords = {FIFO queue, ring buffer, wait-free},
	location = {Seoul, Republic of Korea},
	series = {PPoPP '22}
}

@article{10.1145/3155284.3019022,
author = {Ramalhete, Pedro and Correia, Andreia},
title = {POSTER: A Wait-Free Queue with Wait-Free Memory Reclamation},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019022},
doi = {10.1145/3155284.3019022},
abstract = {Queues are a widely deployed data structure. They are used extensively in many multi threaded applications, or as a communication mechanism between threads or processes. We propose a new linearizable multi-producer-multi-consumer queue we named Turn queue, with wait-free progress bounded by the number of threads, and with wait-free bounded memory reclamation. Its main characteristics are: a simple algorithm that does no memory allocation apart from creating the node that is placed in the queue, a new wait-free consensus algorithm using only the atomic instruction compare-and-swap (CAS), and is easy to plugin with other algorithms for either enqueue or dequeue methods.},
journal = {SIGPLAN Not.},
month = jan,
pages = {453–454},
numpages = {2},
keywords = {low latency, non-blocking queue, wait-free}
}

@inproceedings{WaitFreeQueueWithWaitFreeMemoryReclamation,
	author = {Ramalhete, Pedro and Correia, Andreia},
	title = {POSTER: A Wait-Free Queue with Wait-Free Memory Reclamation},
	year = {2017},
	isbn = {9781450344937},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3018743.3019022},
	doi = {10.1145/3018743.3019022},
	abstract = {Queues are a widely deployed data structure. They are used extensively in many multi threaded applications, or as a communication mechanism between threads or processes. We propose a new linearizable multi-producer-multi-consumer queue we named Turn queue, with wait-free progress bounded by the number of threads, and with wait-free bounded memory reclamation. Its main characteristics are: a simple algorithm that does no memory allocation apart from creating the node that is placed in the queue, a new wait-free consensus algorithm using only the atomic instruction compare-and-swap (CAS), and is easy to plugin with other algorithms for either enqueue or dequeue methods.},
	booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
	pages = {453–454},
	numpages = {2},
	keywords = {low latency, non-blocking queue, wait-free},
	location = {Austin, Texas, USA},
	series = {PPoPP '17}
}

@inproceedings{10.1145/1941553.1941585,
author = {Kogan, Alex and Petrank, Erez},
title = {Wait-free queues with multiple enqueuers and dequeuers},
year = {2011},
isbn = {9781450301190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1941553.1941585},
doi = {10.1145/1941553.1941585},
abstract = {The queue data structure is fundamental and ubiquitous. Lock-free versions of the queue are well known. However, an important open question is whether practical wait-free queues exist. Until now, only versions with limited concurrency were proposed. In this paper we provide a design for a practical wait-free queue. Our construction is based on the highly efficient lock-free queue of Michael and Scott. To achieve wait-freedom, we employ a priority-based helping scheme in which faster threads help the slower peers to complete their pending operations. We have implemented our scheme on multicore machines and present performance measurements comparing our implementation with that of Michael and Scott in several system configurations.},
booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
pages = {223–234},
numpages = {12},
keywords = {wait-free algorithms, concurrent queues},
location = {San Antonio, TX, USA},
series = {PPoPP '11}
}

@article{Kogan2011WaitFreeQueues,
	author = {Kogan, Alex and Petrank, Erez},
	title = {Wait-free queues with multiple enqueuers and dequeuers},
	year = {2011},
	issue_date = {August 2011},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {46},
	number = {8},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/2038037.1941585},
	doi = {10.1145/2038037.1941585},
	abstract = {The queue data structure is fundamental and ubiquitous. Lock-free versions of the queue are well known. However, an important open question is whether practical wait-free queues exist. Until now, only versions with limited concurrency were proposed. In this paper we provide a design for a practical wait-free queue. Our construction is based on the highly efficient lock-free queue of Michael and Scott. To achieve wait-freedom, we employ a priority-based helping scheme in which faster threads help the slower peers to complete their pending operations. We have implemented our scheme on multicore machines and present performance measurements comparing our implementation with that of Michael and Scott in several system configurations.},
	journal = {SIGPLAN Not.},
	month = feb,
	pages = {223–234},
	numpages = {12},
	keywords = {wait-free algorithms, concurrent queues}
}

@inproceedings{10.1145/2851141.2851168,
author = {Yang, Chaoran and Mellor-Crummey, John},
title = {A wait-free queue as fast as fetch-and-add},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851168},
doi = {10.1145/2851141.2851168},
abstract = {Concurrent data structures that have fast and predictable performance are of critical importance for harnessing the power of multicore processors, which are now ubiquitous. Although wait-free objects, whose operations complete in a bounded number of steps, were devised more than two decades ago, wait-free objects that can deliver scalable high performance are still rare.In this paper, we present the first wait-free FIFO queue based on fetch-and-add (FAA). While compare-and-swap (CAS) based non-blocking algorithms may perform poorly due to work wasted by CAS failures, algorithms that coordinate using FAA, which is guaranteed to succeed, can in principle perform better under high contention. Along with FAA, our queue uses a custom epoch-based scheme to reclaim memory; on x86 architectures, it requires no extra memory fences on our algorithm's typical execution path. An empirical study of our new FAA-based wait-free FIFO queue under high contention on four different architectures with many hardware threads shows that it outperforms prior queue designs that lack a wait-free progress guarantee. Surprisingly, at the highest level of contention, the throughput of our queue is often as high as that of a microbenchmark that only performs FAA. As a result, our fast wait-free queue implementation is useful in practice on most multi-core systems today. We believe that our design can serve as an example of how to construct other fast wait-free objects.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {16},
numpages = {13},
keywords = {fast-path-slow-path, non-blocking queue, wait-free},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{FastFetchAndAddWaitFreeQueue,
	author = {Yang, Chaoran and Mellor-Crummey, John},
	title = {A wait-free queue as fast as fetch-and-add},
	year = {2016},
	issue_date = {August 2016},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {51},
	number = {8},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/3016078.2851168},
	doi = {10.1145/3016078.2851168},
	abstract = {Concurrent data structures that have fast and predictable performance are of critical importance for harnessing the power of multicore processors, which are now ubiquitous. Although wait-free objects, whose operations complete in a bounded number of steps, were devised more than two decades ago, wait-free objects that can deliver scalable high performance are still rare.In this paper, we present the first wait-free FIFO queue based on fetch-and-add (FAA). While compare-and-swap (CAS) based non-blocking algorithms may perform poorly due to work wasted by CAS failures, algorithms that coordinate using FAA, which is guaranteed to succeed, can in principle perform better under high contention. Along with FAA, our queue uses a custom epoch-based scheme to reclaim memory; on x86 architectures, it requires no extra memory fences on our algorithm's typical execution path. An empirical study of our new FAA-based wait-free FIFO queue under high contention on four different architectures with many hardware threads shows that it outperforms prior queue designs that lack a wait-free progress guarantee. Surprisingly, at the highest level of contention, the throughput of our queue is often as high as that of a microbenchmark that only performs FAA. As a result, our fast wait-free queue implementation is useful in practice on most multi-core systems today. We believe that our design can serve as an example of how to construct other fast wait-free objects.},
	journal = {SIGPLAN Not.},
	month = feb,
	articleno = {16},
	numpages = {13},
	keywords = {fast-path-slow-path, non-blocking queue, wait-free}
}

